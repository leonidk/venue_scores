{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import csv\n",
    "import scipy.sparse\n",
    "Xauth = None\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('useful_venue_list.pkl.gz','rb') as fp:\n",
    "    all_venues = pickle.load(fp)\n",
    "with gzip.open('useful_authors_list.pkl.gz','rb') as fp:\n",
    "    all_authors = pickle.load(fp)\n",
    "with gzip.open('useful_papers.pkl.gz','rb') as fp:\n",
    "    all_papers = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_idx = {v:i for i,v in enumerate(all_venues)}\n",
    "name_idx = {v:i for i,v in enumerate(all_authors)}\n",
    "n_confs = len(all_venues)\n",
    "n_auths = len(all_authors)\n",
    "r1_confs = pickle.load(open('old_version/r1_confs.pkl','rb'))\n",
    "r1_confs_dict = {_:1 for _ in r1_confs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_affil = pd.read_csv('faculty-affiliations.csv')\n",
    "ranks = pd.read_csv('ranks.csv')\n",
    "def csv2dict_str_str(fname):\n",
    "    with open(fname, mode='r') as infile:\n",
    "        rdr = csv.reader(infile)\n",
    "        d = {rows[0].strip(): rows[1].strip() for rows in rdr}\n",
    "    return d\n",
    "aliasdict = csv2dict_str_str('dblp-aliases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year = all_papers[0][6]\n",
    "max_year = all_papers[-1][6]\n",
    "span_years = max_year - min_year + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_years = np.ones((n_auths,2)) * np.array([3000,1000]) \n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    for a in authors:\n",
    "        i = name_idx[a]\n",
    "        auth_years[i,0] = min(auth_years[i,0],year)\n",
    "        auth_years[i,1] = max(auth_years[i,1],year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_files = ['weights_faculty_above6_log_1_40_25_0.pkl',\n",
    "             'weights_salary_above6_linear_1_0_25_0.pkl',\n",
    "              'weights_nsfmarginal_above6_log_1_0_25_0.pkl']\n",
    "clf_files2 = ['weights_salary_above6_linear_1_0_25_1.pkl',\n",
    "            'weights_nsfmarginal_above6_log_1_0_25_1.pkl',\n",
    "            'weights_faculty_above6_log_1_40_25_7.pkl']\n",
    "clf_files = ['weights_salary_above6_linear_50_0_25_0.pkl',\n",
    "            'weights_nsfmarginal_above6_log_50_0_25_0.pkl',\n",
    "            'weights_faculty_above6_log_50_40_25_0.pkl']\n",
    "\n",
    "clfs = [pickle.load(open(c,'rb')) for c in clf_files]\n",
    "clfs2 = [pickle.load(open(c,'rb')) for c in clf_files2]\n",
    "\n",
    "span_years = 50 \n",
    "repeat_needed = span_years//(clfs[0].shape[0]//n_confs)\n",
    "if repeat_needed > 1:\n",
    "    clfs = [np.repeat(_,repeat_needed) for _ in clfs]\n",
    "clfs = clfs2\n",
    "clfs = [np.squeeze(_) for _ in clfs]\n",
    "\n",
    "print([_.shape for _ in clfs])\n",
    "clp =  18 #7\n",
    "clfs2 = []\n",
    "if True:\n",
    "    for result_clf in clfs:\n",
    "        result_clf = result_clf.reshape((-1,span_years))\n",
    "\n",
    "        #plt.plot(result_clf.sum(0)/result_clf.sum(0).sum(),label='sum')\n",
    "        print(abs(result_clf.mean(0)).mean(),abs(result_clf.std(0)).mean())\n",
    "        #result_clf = np.minimum(30,np.maximum(result_clf,-30))\n",
    "        #result_clf = (result_clf)/result_clf.std(0)\n",
    "        result_clf = (result_clf-result_clf.mean(0))/result_clf.std(0)\n",
    "        result_clf = result_clf.reshape((-1))\n",
    "        clfs2.append(result_clf)\n",
    "else:\n",
    "    clfs2 = [(c-c.mean(0))/c.std(0) for c in clfs]\n",
    "clfs = clfs2\n",
    "clfs = [np.minimum(clp,np.maximum(-clp,c)) for c in clfs]\n",
    "clfs = np.vstack(clfs)\n",
    "clf = np.mean(clfs,0)\n",
    "\n",
    "\n",
    "clf_gold = np.copy(clf)\n",
    "#clf_gold = np.ones_like(clf)\n",
    "#clf = clf_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_ord = np.argsort(np.squeeze(clf))\n",
    "#'Scientometrics','JCDL','NIPS',\n",
    "conf_choice = ['ICRA','ICML','SIGGRAPH','HRI','ECCV','Comput. Graph. Forum','Shape Modeling International','Symposium on Geometry Processing','Computer Aided Geometric Design','I. J. Robotics Res.','CVPR','International Journal of Computer Vision','Robotics: Science and Systems','ICRA','WACV','ICML','AISTATS','CoRR','SIGGRAPH Asia','ECCV','ICCV','ISER','Humanoids','3DV','IROS','CoRL','Canadian Conference on AI','ACCV','Graphics Interface','CRV','BMVC']\n",
    "ri_confs = np.zeros(clf_gold.shape[0])\n",
    "ms = clf_gold.mean()\n",
    "ss = clf_gold.std()\n",
    "np.set_printoptions(precision=1)\n",
    "seen = {}\n",
    "for i in range(clf_gold.shape[0]):\n",
    "    idx = conf_ord[-(i+1)]\n",
    "    conf_name = all_venues[idx//span_years]\n",
    "    conf_score = clf[idx]\n",
    "    if conf_name in conf_choice:\n",
    "        ri_confs[idx] = 1\n",
    "    if conf_name in conf_choice and conf_name not in seen:\n",
    "        vec = clfs[:,idx]\n",
    "        print('{:20s}{}\\t{:.1f}\\t{}'.format(conf_name[:20],str(min_year + (idx % span_years)),(conf_score-ms)/ss,vec))\n",
    "        seen[conf_name] =1\n",
    "ri_confs.shape,ri_confs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 50\n",
    "i = -1\n",
    "j = 0\n",
    "seen = {}\n",
    "while j < top_k:\n",
    "    i += 1\n",
    "    idx = conf_ord[-(i+1)]\n",
    "    conf_name = all_venues[idx//span_years]\n",
    "    if conf_name in seen:\n",
    "        continue\n",
    "    j+=1\n",
    "    conf_score = clf[idx]\n",
    "    seen[conf_name] = 1\n",
    "    print('{:20s}\\t{}\\t\\t{:.3f}\\t{:.2f}'.format(conf_name[:18],min_year + (idx % span_years),100*conf_score,(conf_score-ms)/ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for clf in [clf_gold]:\n",
    "    plt.style.use('fivethirtyeight')\n",
    "\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "    conf_choice2 = ['SIGGRAPH','AAAI','NIPS','CVPR','ICRA','ICML','ICCV','ECCV',\n",
    "                   'International Journal of Computer Vision','Robotics: Science and Systems']\n",
    "    #conf_choice2 = ['CVPR','ECCV','ICCV','International Journal of Computer Vision','Robotics: Science and Systems']\n",
    "    #conf_choice2 = ['SIGMOD Conference','VLDB','ICDT','KDD','ACM Trans. Inf. Syst.','PODS']\n",
    "    #conf_choice2 = ['ACL','EMNLP','EACL']\n",
    "\n",
    "    conf_choices = [conf_choice2, \n",
    "                    ['STOC','FOCS','SODA','EC','WINE'],\n",
    "                    ['UAI','AAAI','IJCAI','ICML','NIPS'],\n",
    "                    ['ECCV','ICCV','CVPR','BMVC','International Journal of Computer Vision','IEEE Trans. Pattern Anal. Mach. Intell.'],\n",
    "                    ['Robotics: Science and Systems','IROS','ICRA','WAFR','ISER'],\n",
    "                    ['SIGGRAPH','SIGGRAPH Asia','ACM Trans. Graph.','Graphics Interface'],\n",
    "                    ['SIGIR','JCDL','CIKM','KDD','WWW','SIGMOD Conference']\n",
    "                   ]\n",
    "    labels = ['All','Theory','AI','Vision','Robotics','Graphics','IR']\n",
    "    #conf_choices = [['Robotics: Science and Systems','IROS','ICRA','CoRL','WAFR','HRI','ISER']]\n",
    "    for conf_choice2,l in zip(conf_choices,labels):\n",
    "        plt.figure()\n",
    "        #conf_choice2 = \n",
    "        conf_choice3 = []\n",
    "        vs = clf.std()\n",
    "        for conf in conf_choice2:\n",
    "            idx = conf_idx[conf]\n",
    "            s = clf[span_years*idx+1999-1970]#max(clf[span_years*idx:span_years*(idx+1)]) ##s = clf[span_years*idx+2015-1970]#\n",
    "            conf_choice3.append((s,conf))\n",
    "        plt.figure(figsize=(12,8))\n",
    "        for s,conf in sorted(conf_choice3,reverse=True):\n",
    "            idx = conf_idx[conf]\n",
    "            _ = plt.plot(np.arange(min_year,max_year+1)[:-2],(clf[span_years*idx:span_years*(idx+1)]/vs)[:-2],label=conf,lw=5)\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('year')\n",
    "        plt.ylabel('value')\n",
    "        #plt.title('WITH Temporal Adjustment')\n",
    "        plt.title(l)\n",
    "        plt.legend()\n",
    "        #plt.ylim(0,25)\n",
    "        plt.xlim(1970,2020)\n",
    "        plt.xticks(np.arange(1970,2020,10),[str(_) for _ in np.arange(1970,2020,10)])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(l + '_t.pdf',facecolor='w',edgecolor='w')\n",
    "        \n",
    "        #plt.show()\n",
    "\n",
    "    #plt.show()\n",
    "clf = clf_gold\n",
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for style in []:##plt.style.available:#:\n",
    "    plt.style.use(style)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for s,conf in sorted(conf_choice3,reverse=True):\n",
    "        idx = conf_idx[conf]\n",
    "        weights = [result_clf[years_per_conf*idx + yr]/vs for yr in offset_years]\n",
    "        _ = plt.plot(np.arange(min_year,max_year+1),weights,label=conf,lw=5)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('year')\n",
    "    plt.ylabel('value')\n",
    "    plt.title(style)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Xauth is None:\n",
    "    count_vecs = {}\n",
    "    paper_vecs = []\n",
    "    for paper in all_papers:\n",
    "        tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "        n = len(authors)\n",
    "        j = 50*conf_idx[venue] + (year-min_year)\n",
    "\n",
    "        if n not in count_vecs:\n",
    "            author_scores = np.ones(n) #1/(np.arange(n)+1) \n",
    "            #author_scores[-1] = author_scores[0]\n",
    "            author_score_sum = author_scores.sum()\n",
    "            #author_scores /= author_score_sum\n",
    "            count_vecs[n] = author_scores #/ author_score_sum\n",
    "        else:\n",
    "            author_scores = count_vecs[n]\n",
    "            paper_vecs.append([(name_idx[a],j,v) for a,v in zip(authors,author_scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Xauth is None or Xauth.shape[1] != 50*n_confs:\n",
    "    import scipy.sparse\n",
    "    Xauth = scipy.sparse.dok_matrix((n_auths,50*n_confs))\n",
    "    xdict = {}\n",
    "    if False:\n",
    "        for paper in all_papers:\n",
    "            tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "            n = len(authors)\n",
    "            j = years_per_conf*conf_idx[venue] + (year-min_year)//YEAR_BLOCKS\n",
    "            for ai in range(n):#zip(count_vecs[n],authors):\n",
    "                i = name_idx[authors[ai]]\n",
    "                #xdict[(i,j)] = 1/n + xdict.get((i,j),0)\n",
    "                xdict[(i,j)] = count_vecs[n][ai] + xdict.get((i,j),0)\n",
    "\n",
    "    else:\n",
    "        for paper_vec in paper_vecs:\n",
    "            for i,j,v in paper_vec:\n",
    "                xdict[(i,j)] = v + xdict.get((i,j),0)\n",
    "\n",
    "    Xauth._update(xdict)\n",
    "            \n",
    "    Xauth = scipy.sparse.csr_matrix(Xauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_counts = np.zeros(n_auths)\n",
    "paper_counts_c = np.zeros(n_confs)\n",
    "paper_counts_year = {}#np.zeros(n_auths)\n",
    "paper_counts_year_counter = np.zeros((max_year-min_year+1))\n",
    "\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    paper_counts_c[conf_idx[venue]] += 1\n",
    "    paper_counts_year_counter[year-min_year] += 1\n",
    "    for a in authors:\n",
    "        i = name_idx[a]\n",
    "        paper_counts[i] += 1\n",
    "        paper_counts_year[(a,year)] = 1+ paper_counts_year.get((a,year),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = Xauth.dot(clf_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_rank_people = pickle.load(open('new_pagerank_people.pkl','rb'))\n",
    "page_rank_confs = pickle.load(open('new_pagerank_conf.pkl','rb'))\n",
    "page_rank_clf = np.repeat(page_rank_confs,50)\n",
    "page_rank_scores = Xauth.dot(page_rank_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.read_csv('correlation_cleaned.csv')\n",
    "df_corr = df_corr.drop(columns=[_ for _ in df_corr.columns if 'Unnamed' in _])\n",
    "df_corr = df_corr.drop(columns=['pms','n_papers'])\n",
    "df_corr = df_corr.rename(columns={'totals': 'venue_score', 'csrp': 'csr_pubs','csrpn': 'csr_adj','gcite': 'influence'})\n",
    "df_corr = df_corr[['name','papers', 'citations', 'h-index',\n",
    "       'i10','csr_pubs', 'csr_adj','venue_score','influence']]\n",
    "df_corr = df_corr.dropna('index')\n",
    "df_corr.index = df_corr.name\n",
    "df_corr['pagerankA'] = df_corr.venue_score\n",
    "df_corr['pagerankC'] = df_corr.venue_score\n",
    "\n",
    "ts = total_scores.std()\n",
    "tm = total_scores.mean()\n",
    "for name in df_corr.name:\n",
    "    if name in name_idx:\n",
    "        idx = name_idx[name]\n",
    "        df_corr.loc[name,'venue_score'] = total_scores[idx]\n",
    "        df_corr.loc[name,'pagerankA'] = page_rank_people[idx]\n",
    "        df_corr.loc[name,'pagerankC'] = page_rank_scores[idx]\n",
    "        #print(name,df_corr.loc[name,'papers'],paper_counts[idx])\n",
    "        df_corr.loc[name,'papers'] = paper_counts[idx]\n",
    "\n",
    "\n",
    "print(df_corr.corr('spearman').loc['influence','venue_score'],df_corr.corr('kendall').loc['influence','venue_score'],df_corr.corr('spearman').loc['h-index','venue_score'])\n",
    "#if clfn == clfs_test.shape[-1]:\n",
    "df_corr[['papers','citations','h-index','i10','csr_pubs','venue_score','pagerankA','pagerankC','influence']].corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv('faculty_affil_scholar.csv')\n",
    "fuzzythresh=0.9 #0.9\n",
    "df_merged.loc[df_merged.fuzzyscore > fuzzythresh,'dblpname'] = df_merged[df_merged.fuzzyscore > fuzzythresh].fuzzyname\n",
    "df_merged.loc[df_merged.fuzzyscore > fuzzythresh,'dblpexists'] = 1\n",
    "df_merged = df_merged[df_merged.dblpexists == 1]\n",
    "df_merged['venue_score'] = np.ones_like(df_merged.dblpexists)\n",
    "df_merged['pagerankA'] = np.ones_like(df_merged.dblpexists)\n",
    "df_merged['pagerankC'] = np.ones_like(df_merged.dblpexists)\n",
    "df_merged['papers'] = np.ones_like(df_merged.dblpexists)\n",
    "\n",
    "ts = total_scores.std()\n",
    "tm = total_scores.mean()\n",
    "df_merged = df_merged.set_index(df_merged.dblpname)\n",
    "seen_map = {}\n",
    "for name in df_merged.index:\n",
    "    if name in name_idx:\n",
    "        idx = name_idx[name]\n",
    "        seen_map[name] = 1\n",
    "        df_merged.loc[name,'venue_score'] = total_scores[idx]\n",
    "        df_merged.loc[name,'pagerankA'] = page_rank_people[idx]\n",
    "        df_merged.loc[name,'pagerankC'] = page_rank_scores[idx]\n",
    "        df_merged.loc[name,'papers'] = paper_counts[idx]\n",
    "df_merged = df_merged.drop(columns=['Unnamed: 0','First Name','Last Name','Sholar link','Rank (Full, Associate, Assistant, Other)','Full Name','University_y','University_x','Unnamed: 11',\"ID\",'fuzzyname','dblpexists','fuzzyscore','UniversityID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_datas = []\n",
    "for row in faculty_affil.itertuples():\n",
    "    if row[1] in name_idx and row[1] not in seen_map:\n",
    "        idx = name_idx[row[1]]\n",
    "        seen_map[row[1]] = 1\n",
    "        new_data = {}\n",
    "        new_data['dblpname'] = row[1]\n",
    "        #new_data['index'] = row[1]\n",
    "        new_data['school'] = row[2]\n",
    "        new_data['venue_score'] = total_scores[idx]\n",
    "        new_data['pagerankA'] = page_rank_people[idx]\n",
    "        new_data['pagerankC'] = page_rank_scores[idx]\n",
    "        new_data['papers'] = paper_counts[idx]\n",
    "        \n",
    "        new_datas.append(new_data)\n",
    "        #df_merged = df_merged.append([row[1],np.nan,np.nan,np.nan,row[2],row[1],(total_scores[idx]-tm)/ts])\n",
    "df_csr_to_add = pd.DataFrame(new_datas)\n",
    "#df_csr_to_add = df_csr_to_add.set_index('dblpname')\n",
    "df_csr_to_add = df_csr_to_add.set_index('dblpname')\n",
    "#df_merged = pd.concat([df_merged,df_csr_to_add])\n",
    "print(df_merged.shape)\n",
    "#print(faculty_affil.shape)\n",
    "df_t10 = df_merged[df_merged['t10-index'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_vals = df_t10.groupby('school').aggregate('sum').sort_values('venue_score',0,False)\n",
    "t10_schools = school_vals.sort_values('t10-index',0,False)\n",
    "school_vals.corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msar = pd.read_csv('traditional_conf_scores.csv')\n",
    "offset_years = [i for i in range(span_years)]\n",
    "years_per_conf = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msar_filt = df_msar[df_msar.dblp_name != 'NotAConf'].copy()#.sort_values('H',ascending=False)\n",
    "scores = []\n",
    "paperss = []\n",
    "pagerankc = []\n",
    "for row in df_msar_filt.itertuples():\n",
    "    conf = row[-1]\n",
    "    idx = conf_idx[conf]\n",
    "    weights = [result_clf[years_per_conf*idx + yr] for yr in offset_years[1984-min_year:2014-min_year]]\n",
    "    #scores.append(result_clf[years_per_conf*idx + offset_years[2014-min_year]])\n",
    "\n",
    "    scores.append(np.max(np.array([w for w in weights])))\n",
    "    pagerankc.append(page_rank_confs[idx])\n",
    "    paperss.append(paper_counts_c[idx])\n",
    "\n",
    "    #print(scores[-1],weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msar_filt['venue_scores'] = scores\n",
    "df_msar_filt['pagerankc'] = pagerankc\n",
    "df_msar_filt['papers'] = paperss\n",
    "\n",
    "df_msar_filt = df_msar_filt[~df_msar_filt.dblp_name.duplicated()].copy()\n",
    "df_msar_filt = df_msar_filt[~df_msar_filt.venue_scores.duplicated()].copy()\n",
    "#df_msar_filt['h-approx'] = 0.54*np.sqrt(df_msar_filt.citations)\n",
    "\n",
    "df_msar_filt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0\n",
    "df_msar_filt = df_msar_filt[['papers','citations','H','pagerankc','venue_scores']]\n",
    "print(df_msar_filt[df_msar_filt.H > thresh].shape)\n",
    "df_msar_filt[df_msar_filt.H > thresh].corr('spearman')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp2 = pd.read_csv('uni_rank_bp.csv')\n",
    "times = pd.read_csv('uni_rank_times.csv')\n",
    "\n",
    "srf2 = pd.read_csv('uni_rank_mergedscholar.csv')\n",
    "st2 = pd.read_csv('uni_rank_st.csv')\n",
    "qt2 = pd.read_csv('uni_rank_qt.csv')\n",
    "sr2 = pd.read_csv('uni_rank_sr.csv')\n",
    "pr2 = pd.read_csv('uni_rank_pr.csv')\n",
    "cm2 = pd.read_csv('uni_rank_cs.csv')\n",
    "usn2 = pd.read_csv('uni_rank_usn.csv')\n",
    "df_csr = pd.read_csv('ranks.csv')\n",
    "\n",
    "pr2.USN2010 = pr2.USN2010.map(lambda x: int(x) if x.isnumeric() else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    [(row[2],row[1]) for row in st2.itertuples()],\n",
    "    [(row[2],row[1]) for row in times.itertuples()],\n",
    "    [(row[2],row[1]) for row in qt2.itertuples()],\n",
    "    [(row[6],row[1]) for row in pr2.itertuples()],\n",
    "    [(row[2],row[1]) for row in cm2.itertuples()],\n",
    "    [(row[2],row[1]) for row in sr2.itertuples()],\n",
    "    [(row[-1],row[2]) for row in srf2.itertuples()],\n",
    "\n",
    "    [(row[0],idx+1) for idx,row in enumerate(t10_schools.itertuples())],\n",
    "    [(row[2],row[1]) for row in df_csr.itertuples()],\n",
    "    [(row[0],idx+1) for idx,row in enumerate(school_vals.itertuples())],\n",
    "    [(row[0],idx+1) for idx,row in enumerate(school_vals.sort_values('pagerankA',0,False).itertuples())],\n",
    "    [(row[0],idx+1) for idx,row in enumerate(school_vals.sort_values('pagerankC',0,False).itertuples())],\n",
    "    [(row[0],idx+1) for idx,row in enumerate(school_vals.sort_values('Citations',0,False).itertuples())],\n",
    "    [(row[0],idx+1) for idx,row in enumerate(school_vals.sort_values('papers',0,False).itertuples())],\n",
    "\n",
    "    [(row[2],row[1]) for row in bp2.itertuples()],\n",
    "    [(row[6],row[4]) for row in pr2.sort_values('NRC95',ascending=True).itertuples() ],\n",
    "    [(row[6],row[3]) for row in pr2.sort_values('USN2010',ascending=True).itertuples() if np.isfinite(row[3]) ],\n",
    "    [(row[2],row[1]) for row in usn2.itertuples()]\n",
    "]\n",
    "dataset_names = ['Shanghai','Times','QS','Prestige','CSMetrics',\n",
    "                 'ScholarRank','ScholarRankFull','t10Sum','CSRankings','Mine','pageranka','pagerankc','citations','papers','BestPaper','NRC95',\"USN10\",'USN18']\n",
    "n_datasets = len(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.zeros((n_datasets,n_datasets))\n",
    "count_matrix = np.zeros((n_datasets,n_datasets))\n",
    "name_datasets = [ [v[0] for v in d] for d in datasets ]\n",
    "all_names = sorted(list(set(sum(name_datasets,[]))))\n",
    "all_vec = [sum([name in d for d in name_datasets])>=(len(datasets)-4) for name in all_names]\n",
    "subset_names = [name for name,vec in zip(all_names,all_vec) if vec]\n",
    "subset_names,len(subset_names)\n",
    "import scipy.stats as stats\n",
    "for i in range(n_datasets):\n",
    "    inames = [u[0] for u in datasets[i]]\n",
    "    for j in range(i,n_datasets):\n",
    "        jnames = [u[0] for u in datasets[j]]\n",
    "\n",
    "        #exist_1 = [((ni in subset_names) and (ni in jnames)) for ni in inames]\n",
    "        #exist_2 = [((nj in subset_names) and (nj in inames))for nj in jnames]\n",
    "        exist_1 = [((True) and (ni in jnames)) for ni in inames]\n",
    "        exist_2 = [((True) and (nj in inames))for nj in jnames]\n",
    "        \n",
    "        d1 = np.array(datasets[i])[exist_1]\n",
    "        d2 = np.array(datasets[j])[exist_2]\n",
    "        v1 = d1[:,1].astype(np.float)\n",
    "        v2 = np.array([d2[np.where(d2[:,0] == name)[0][0],1] for name in d1[:,0]]).astype(np.float)\n",
    "        c = stats.kendalltau(v1,v2)[0]\n",
    "        corr_matrix[i][j] = c\n",
    "        corr_matrix[j][i] = c\n",
    "        count_matrix[i][j] = len(v1)\n",
    "        count_matrix[j][i] = len(v2)\n",
    "        #print(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "print(corr_matrix)\n",
    "print('mean best')\n",
    "for s,n in sorted([(s,n) for n,s in zip(dataset_names,corr_matrix.mean(1))],reverse=True):\n",
    "    print('{:30s}\\t{:.3f}'.format(n,s))\n",
    "print('\\n usnews best')\n",
    "for s,n in sorted([(s,n) for n,s in zip(dataset_names,corr_matrix[-1])],reverse=True):\n",
    "    print('{:30s}\\t{:.3f}'.format(n,s))\n",
    "print('\\n names')\n",
    "\n",
    "for n in dataset_names:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict\n",
    "def di():\n",
    "    return defaultdict(float)\n",
    " \n",
    "author_by_year = defaultdict(di)\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    for a in authors:\n",
    "        author_by_year[a][year] += result_clf[years_per_conf*conf_idx[venue] + offset_years[year-min_year]]/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import matplotlib.gridspec as gridspec\n",
    "gs = gridspec.GridSpec(4, 4)\n",
    "plt.figure(figsize=(12,12))\n",
    "example_names = ['Takeo Kanade','Michael I. Jordan','Leonidas J. Guibas','Daniela Rus','Jon M. Kleinberg', 'Judea Pearl']\n",
    "\n",
    "plt.subplot(gs[2:4, 1:3])\n",
    "\n",
    "for example_name in example_names:\n",
    "    example_value = np.zeros(max_year+1-min_year)\n",
    "    years = author_by_year[example_name]\n",
    "    yrs = [_ for _ in years.keys() if _  > 0]\n",
    "    start_year = min(yrs)\n",
    "    end_year = max(yrs)\n",
    "    span = end_year - start_year\n",
    "    start_year,end_year,span\n",
    "    for y,v in years.items():\n",
    "        example_value[y-min_year] += v\n",
    "            \n",
    "    plt.plot(np.arange(min_year,max_year+1)[:-1],gaussian_filter1d(example_value[:-1], sigma=4.5),label=example_name,lw=4)\n",
    "    #plt.plot(gaussian_filter1d(example_value[:span], sigma=2),label=example_name )\n",
    "\n",
    "#plt.plot((val_by_year/v_count),label='average author')\n",
    "plt.ylabel('annual value (4.5 sigma smoothing)')\n",
    "#plt.xlabel('year since first publication')\n",
    "plt.xlabel('year')\n",
    "plt.title('venue scores')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.subplot(gs[:2, :2])\n",
    "for example_name in example_names:\n",
    "    example_value = np.zeros(max_year+1-min_year)\n",
    "    for yr in range(min_year,max_year+1):\n",
    "        if (example_name,yr) in paper_counts_year:\n",
    "            example_value[yr-min_year] += paper_counts_year[example_name,yr]\n",
    "    plt.plot(np.arange(min_year,max_year+1)[:-1],gaussian_filter1d(example_value[:-1], sigma=4.5),label=example_name,lw=4)\n",
    "    #plt.plot(gaussian_filter1d(example_value[:span], sigma=2),label=example_name )\n",
    "\n",
    "#plt.plot((val_by_year/v_count),label='average author')\n",
    "plt.ylabel('annual value (4.5 sigma smoothing)')\n",
    "#plt.xlabel('year since first publication')\n",
    "plt.xlabel('year')\n",
    "plt.title('paper count')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.subplot(gs[:2, 2:])\n",
    "\n",
    "for example_name in example_names:\n",
    "    example_value = np.zeros(max_year+1-min_year)\n",
    "    for yr in range(min_year,max_year+1):\n",
    "        if (example_name,yr) in paper_counts_year:\n",
    "            example_value[yr-min_year] += paper_counts_year[example_name,yr]\n",
    "    plt.plot(np.arange(min_year,max_year+1)[:-1],gaussian_filter1d((example_value/paper_counts_year_counter)[:-1], sigma=4.5),label=example_name,lw=4)\n",
    "    #plt.plot(gaussian_filter1d(example_value[:span], sigma=2),label=example_name )\n",
    "\n",
    "#plt.plot((val_by_year/v_count),label='average author')\n",
    "plt.ylabel('annual value (4.5 sigma smoothing)')\n",
    "#plt.xlabel('year since first publication')\n",
    "plt.xlabel('year')\n",
    "plt.title('paper count (norm by annual paper totals)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.savefig('fame3.pdf',facecolor='w',edgecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri_names = list(set([aliasdict.get(row[1],row[1]) for row in pd.read_csv('cmu_faculty.csv').itertuples() if row[2] == 'RI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_names = np.array(['Artur Dubrawski','Artur W. Dubrawski'])\n",
    "uni_names = list(uni_names)\n",
    "cmu_scores = []\n",
    "for name in uni_names:\n",
    "    if name in name_idx:\n",
    "        score = total_scores[name_idx[name]]\n",
    "        cmu_scores.append(((score-tm)/ts,name))\n",
    "for s,p in sorted(cmu_scores,reverse=True):\n",
    "    print('{:30s}\\t\\t{:.3f}'.format(p,s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_uni = pd.read_csv('cmu_faculty.csv')\n",
    "cmu_uni = cmu_uni.fillna('Other')\n",
    "cmu_uni = cmu_uni[cmu_uni.dept == 'RI']\n",
    "#print(list(cmu_uni.name))\n",
    "uni_names = list(cmu_uni.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_counts = {}\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    if year < 2004:\n",
    "        continue\n",
    "    for a in authors:\n",
    "        if a in uni_names:\n",
    "            conf_counts[venue] = 1 + conf_counts.get(venue,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v,k in sorted([(v,k) for k,v in conf_counts.items() ],reverse=True):\n",
    "    print('{:30s}\\t{}'.format(k[:30],round(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conf_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_conf_counts = {}\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    if year < 2004:\n",
    "        continue\n",
    "    for a in authors:\n",
    "        if a in uni_names:\n",
    "            prof_conf_counts[venue] = [a] + prof_conf_counts.get(venue,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs_of_interest = ['CVPR','ICCV','ECCV']\n",
    "for conf_of_interest in confs_of_interest:\n",
    "    unique_set = {k: 0 for k in set(prof_conf_counts[conf_of_interest])}\n",
    "    for k in prof_conf_counts[conf_of_interest]:\n",
    "        unique_set[k] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v,k in sorted([(v,k) for k,v in unique_set.items() ],reverse=True):\n",
    "    print('{:30s}\\t{}'.format(k[:30],round(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
