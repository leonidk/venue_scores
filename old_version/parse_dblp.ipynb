{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "xmltodict.parse(StringIO(u\"<A>香</A>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree\n",
    "from xml.parsers import expat\n",
    "f = gzip.GzipFile('./dblp-2019-01-01.xml.gz')\n",
    "parser = expat.ParserCreate()\n",
    "parser.ParseFile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle(path,item):\n",
    "    if 'author' in item:\n",
    "        if 'Angela Dai' in item['author']:\n",
    "            print(item)\n",
    "        #print(item['author'])\n",
    "    return True\n",
    "xmltodict.parse(gzip.GzipFile('./dblp-2019-01-01.xml.gz'),item_depth=2, item_callback=handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "main_log2 = json.load(open('article.json','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(main_log2),len(main_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_log = pickle.load(open('articles.pkl','rb'))\n",
    "#tree = etree.parse(gzip.GzipFile('../CSrankings/dblp-2019-01-01.xml.gz','rb'))\n",
    "#root = tree.getroot()\n",
    "#with gzip.GzipFile('../CSrankings/dblp-2019-01-01.xml.gz','rb') as fp:\n",
    "#    for event, elem in etree.iterparse(fp, events=('start', 'end', 'start-ns', 'end-ns')):\n",
    "#        print(event, elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in main_log:\n",
    "    #if 'authors' in paper:\n",
    "    print(paper.keys())\n",
    "        #if 'Angela Dai' in paper['author']:\n",
    "        #    print(paper)\n",
    "    #for a in paper['author']:\n",
    "    #    if 'Matthias Nießner' in a:\n",
    "    #        print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "# MIT LICENSE from Isaac Changhau (c) 2018\n",
    "# https://github.com/IsaacChanghau/DBLPParser\n",
    "from lxml import etree\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import codecs\n",
    "import ujson\n",
    "import re\n",
    "\n",
    "# all of the feature types in dblp\n",
    "all_features = {\"address\", \"author\", \"booktitle\", \"cdrom\", \"chapter\", \"cite\", \"crossref\", \"editor\", \"ee\", \"isbn\",\n",
    "                \"journal\", \"month\", \"note\", \"number\", \"pages\", \"publisher\", \"school\", \"series\", \"title\", \"url\",\n",
    "                \"volume\", \"year\"}\n",
    "\n",
    "def log_msg(message):\n",
    "    \"\"\"Produce a log with current time\"\"\"\n",
    "    print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), message)\n",
    "\n",
    "def context_iter(dblp_path):\n",
    "    \"\"\"Create a dblp data iterator of (event, element) pairs for processing\"\"\"\n",
    "    return etree.iterparse(source=dblp_path, dtd_validation=True, load_dtd=True)  # required dtd\n",
    "\n",
    "def clear_element(element):\n",
    "    \"\"\"Free up memory for temporary element tree after processing the element\"\"\"\n",
    "    element.clear()\n",
    "    while element.getprevious() is not None:\n",
    "        del element.getparent()[0]\n",
    "\n",
    "def count_pages(pages):\n",
    "    \"\"\"Borrowed from: https://github.com/billjh/dblp-iter-parser/blob/master/iter_parser.py\n",
    "    Parse pages string and count number of pages. There might be multiple pages separated by commas.\n",
    "    VALID FORMATS:\n",
    "        51         -> Single number\n",
    "        23-43      -> Range by two numbers\n",
    "    NON-DIGITS ARE ALLOWED BUT IGNORED:\n",
    "        AG83-AG120\n",
    "        90210H     -> Containing alphabets\n",
    "        8e:1-8e:4\n",
    "        11:12-21   -> Containing colons\n",
    "        P1.35      -> Containing dots\n",
    "        S2/109     -> Containing slashes\n",
    "        2-3&4      -> Containing ampersands and more...\n",
    "    INVALID FORMATS:\n",
    "        I-XXI      -> Roman numerals are not recognized\n",
    "        0-         -> Incomplete range\n",
    "        91A-91A-3  -> More than one dash\n",
    "        f          -> No digits\n",
    "    ALGORITHM:\n",
    "        1) Split the string by comma evaluated each part with (2).\n",
    "        2) Split the part to subparts by dash. If more than two subparts, evaluate to zero. If have two subparts,\n",
    "           evaluate by (3). If have one subpart, evaluate by (4).\n",
    "        3) For both subparts, convert to number by (4). If not successful in either subpart, return zero. Subtract first\n",
    "           to second, if negative, return zero; else return (second - first + 1) as page count.\n",
    "        4) Search for number consist of digits. Only take the last one (P17.23 -> 23). Return page count as 1 for (2)\n",
    "           if find; 0 for (2) if not find. Return the number for (3) if find; -1 for (3) if not find.\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for part in re.compile(r\",\").split(pages):\n",
    "        subparts = re.compile(r\"-\").split(part)\n",
    "        if len(subparts) > 2:\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                re_digits = re.compile(r\"[\\d]+\")\n",
    "                subparts = [int(re_digits.findall(sub)[-1]) for sub in subparts]\n",
    "            except IndexError:\n",
    "                continue\n",
    "            cnt += 1 if len(subparts) == 1 else subparts[1] - subparts[0] + 1\n",
    "    return \"\" if cnt == 0 else str(cnt)\n",
    "\n",
    "\n",
    "def extract_feature(elem, features, include_key=False):\n",
    "    \"\"\"Extract the value of each feature\"\"\"\n",
    "    if include_key:\n",
    "        attribs = {'key': [elem.attrib['key']]}\n",
    "    else:\n",
    "        attribs = {}\n",
    "    for feature in features:\n",
    "        attribs[feature] = []\n",
    "    for sub in elem:\n",
    "        if sub.tag not in features:\n",
    "            continue\n",
    "        if sub.tag == 'title':\n",
    "            text = re.sub(\"<.*?>\", \"\", etree.tostring(sub).decode('utf-8')) if sub.text is None else sub.text\n",
    "        elif sub.tag == 'pages':\n",
    "            text = count_pages(sub.text)\n",
    "        else:\n",
    "            text = sub.text\n",
    "        if text is not None and len(text) > 0:\n",
    "            attribs[sub.tag] = attribs.get(sub.tag) + [text]\n",
    "    return attribs\n",
    "\n",
    "\n",
    "def parse_all(dblp_path, save_path, include_key=False):\n",
    "    log_msg(\"PROCESS: Start parsing...\")\n",
    "    f = open(save_path, 'w', encoding='utf8')\n",
    "    for _, elem in context_iter(dblp_path):\n",
    "        if elem.tag in all_elements:\n",
    "            attrib_values = extract_feature(elem, all_features, include_key)\n",
    "            f.write(str(attrib_values) + '\\n')\n",
    "        clear_element(elem)\n",
    "    f.close()\n",
    "    log_msg(\"FINISHED...\")  # load the saved results line by line using json\n",
    "\n",
    "\n",
    "def parse_entity(dblp_path, save_path, type_name, features=None, save_to_csv=False, include_key=False):\n",
    "    \"\"\"Parse specific elements according to the given type name and features\"\"\"\n",
    "    log_msg(\"PROCESS: Start parsing for {}...\".format(str(type_name)))\n",
    "    assert features is not None, \"features must be assigned before parsing the dblp dataset\"\n",
    "    results = []\n",
    "    attrib_count, full_entity, part_entity = {}, 0, 0\n",
    "    for _, elem in context_iter(dblp_path):\n",
    "        if elem.tag in type_name:\n",
    "            attrib_values = extract_feature(elem, features, include_key)  # extract required features\n",
    "            results.append(attrib_values)  # add record to results array\n",
    "            for key, value in attrib_values.items():\n",
    "                attrib_count[key] = attrib_count.get(key, 0) + len(value)\n",
    "            cnt = sum([1 if len(x) > 0 else 0 for x in list(attrib_values.values())])\n",
    "            if cnt == len(features):\n",
    "                full_entity += 1\n",
    "            else:\n",
    "                part_entity += 1\n",
    "        elif elem.tag not in all_elements:\n",
    "            continue\n",
    "        clear_element(elem)\n",
    "    if save_to_csv:\n",
    "        f = open(save_path, 'w', newline='', encoding='utf8')\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(features)  # write title\n",
    "        for record in results:\n",
    "            # some features contain multiple values (e.g.: author), concatenate with `::`\n",
    "            row = ['::'.join(v) for v in list(record.values())]\n",
    "            writer.writerow(row)\n",
    "        f.close()\n",
    "    else:  # default save to json file\n",
    "        with codecs.open(save_path, mode='w', encoding='utf8', errors='ignore') as f:\n",
    "            ujson.dump(results, f)\n",
    "    return full_entity, part_entity, attrib_count\n",
    "\n",
    "\n",
    "def parse_author(dblp_path, save_path, save_to_csv=False):\n",
    "    type_name = ['article', 'book', 'incollection', 'inproceedings']\n",
    "    log_msg(\"PROCESS: Start parsing for {}...\".format(str(type_name)))\n",
    "    authors = set()\n",
    "    for _, elem in context_iter(dblp_path):\n",
    "        if elem.tag in type_name:\n",
    "            authors.update(a.text for a in elem.findall('author'))\n",
    "        elif elem.tag not in all_elements:\n",
    "            continue\n",
    "        clear_element(elem)\n",
    "    if save_to_csv:\n",
    "        f = open(save_path, 'w', newline='', encoding='utf8')\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerows([a] for a in sorted(authors))\n",
    "        f.close()\n",
    "    else:\n",
    "        with open(save_path, 'w', encoding='utf8') as f:\n",
    "            f.write('\\n'.join(sorted(authors)))\n",
    "    log_msg(\"FINISHED...\")\n",
    "\n",
    "\n",
    "def parse_article(dblp_path, save_path, save_to_csv=False, include_key=False):\n",
    "    type_name = ['article']\n",
    "    features = ['title', 'author', 'year', 'journal', 'pages']\n",
    "    info = parse_entity(dblp_path, save_path, type_name, features, save_to_csv=save_to_csv, include_key=include_key)\n",
    "    log_msg('Total articles found: {}, articles contain all features: {}, articles contain part of features: {}'\n",
    "            .format(info[0] + info[1], info[0], info[1]))\n",
    "    log_msg(\"Features information: {}\".format(str(info[2])))\n",
    "\n",
    "\n",
    "def parse_proceedings(dblp_path, save_path, save_to_csv=False, include_key=False):\n",
    "    type_name = [\"inproceedings\", \"proceedings\"]\n",
    "    features = ['title', 'author', 'year', 'pages', 'booktitle']\n",
    "    info = parse_entity(dblp_path, save_path, type_name, features, save_to_csv=save_to_csv, include_key=include_key)\n",
    "    log_msg('Total proceedings found: {}, proceedings contain all features: {}, proceedings contain part of '\n",
    "            'features: {}'.format(info[0] + info[1], info[0], info[1]))\n",
    "    log_msg(\"Features information: {}\".format(str(info[2])))\n",
    "\n",
    "\n",
    "def parse_book(dblp_path, save_path, save_to_csv=False, include_key=False):\n",
    "    type_name = [\"book\"]\n",
    "    features = ['title', 'author', 'publisher', 'isbn', 'year', 'pages']\n",
    "    info = parse_entity(dblp_path, save_path, type_name, features, save_to_csv=save_to_csv, include_key=include_key)\n",
    "    log_msg('Total books found: {}, books contain all features: {}, books contain part of features: {}'\n",
    "            .format(info[0] + info[1], info[0], info[1]))\n",
    "    log_msg(\"Features information: {}\".format(str(info[2])))\n",
    "\n",
    "\n",
    "def parse_publications(dblp_path, save_path, save_to_csv=False, include_key=False):\n",
    "    type_name = ['article', 'book', 'incollection', 'inproceedings']\n",
    "    features = ['title', 'year', 'pages']\n",
    "    info = parse_entity(dblp_path, save_path, type_name, features, save_to_csv=save_to_csv, include_key=include_key)\n",
    "    log_msg('Total publications found: {}, publications contain all features: {}, publications contain part of '\n",
    "            'features: {}'.format(info[0] + info[1], info[0], info[1]))\n",
    "    log_msg(\"Features information: {}\".format(str(info[2])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtd = etree.DTD(open('../CSrankings/dblp-2017-03-29.dtd','rb'))\n",
    "parser = etree.XMLParser(dtd_validation=True)\n",
    "#with gzip.GzipFile('../CSrankings/dblp-2019-01-01.xml.gz','rb') as fp:\n",
    "#    root = etree.XML(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserTarget:\n",
    "    events = []\n",
    "    close_count = 0\n",
    "    def start(self, tag, attrib):\n",
    "        self.events.append((\"start\", tag, attrib))\n",
    "    def close(self):\n",
    "        events, self.events = self.events, []\n",
    "        self.close_count += 1\n",
    "        return events\n",
    "parser_target = ParserTarget()\n",
    "parser = etree.XMLParser(encoding='ascii',dtd_validation='True',load_dtd=dtd,target=parser_target)\n",
    "#with gzip.GzipFile('../CSrankings/dblp-2019-01-01.xml.gz','rb') as fp:\n",
    "events = etree.parse('../CSrankings/dblp-2019-01-01.xml.gz',parser=parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtd = etree.DTD(open('../CSrankings/dblp-2017-03-29.dtd','rb'))\n",
    "all_features = {\"address\", \"author\", \"booktitle\", \"cdrom\", \"chapter\", \"cite\", \"crossref\", \"editor\", \"ee\", \"isbn\",\n",
    "                \"journal\", \"month\", \"note\", \"number\", \"pages\", \"publisher\", \"school\", \"series\", \"title\", \"url\",\n",
    "\"volume\", \"year\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titer =  etree.iterparse('../CSrankings/dblp-2019-01-01.xml.gz',dtd_validation='True',load_dtd=dtd)\n",
    "\n",
    "for _, elem in titer:\n",
    "    if elem.tag in all_elements:\n",
    "        attrib_values = extract_feature(elem, all_features, include_key)\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.GzipFile('../CSrankings/dblp-2019-01-01.xml.gz','rb') as fp:\n",
    "    for event, elem in etree.iterparse('../CSrankings/dblp-2019-01-01.xml.gz',dtd ):\n",
    "        print(elem.tag,elem.text)\n",
    "        ##   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtd.elements()\n",
    "elem.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
