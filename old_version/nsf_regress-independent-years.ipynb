{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fnmatch\n",
    "import zipfile\n",
    "import xmltodict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import csv\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsf data\n",
    "df2 = pd.read_pickle('nsf2.pkl')\n",
    "df = pd.read_pickle('nsf.pkl')\n",
    "Xauth = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_confs = pickle.load(open('r1_confs.pkl','rb'))\n",
    "r1_confs_dict = {_:1 for _ in r1_confs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the big paper thing\n",
    "papers = pd.read_hdf('papers.h5','table')\n",
    "unique_names = pickle.load(open('big_names.pkl','rb'))\n",
    "unique_confs = pickle.load(open('confs.pkl','rb'))\n",
    "\n",
    "faculty_affil = pd.read_csv('faculty-affiliations.csv')\n",
    "ranks = pd.read_csv('ranks.csv')\n",
    "def csv2dict_str_str(fname):\n",
    "    with open(fname, mode='r') as infile:\n",
    "        rdr = csv.reader(infile)\n",
    "        d = {rows[0].strip(): rows[1].strip() for rows in rdr}\n",
    "    return d\n",
    "aliasdict = csv2dict_str_str('dblp-aliases.csv')\n",
    "conf_idx = pickle.load(open('conf_idx.pkl','rb'))\n",
    "name_idx = pickle.load(open('name_idx.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areadict = {\n",
    "    'icse' : ['ICSE', 'ICSE (1)'],\n",
    "    'fse'  : ['SIGSOFT FSE', 'ESEC/SIGSOFT FSE'],\n",
    "    'usenixatc' : ['USENIX Annual Technical Conference', 'USENIX Annual Technical Conference, General Track'], # next tier\n",
    "    'imc': ['IMC', 'Internet Measurement Conference'],\n",
    "    'sigmetrics': ['SIGMETRICS', 'SIGMETRICS/Performance', 'POMACS'],\n",
    "    'mobicom' : ['MobiCom', 'MOBICOM'],\n",
    "    'rtas' : ['RTAS', 'IEEE Real-Time and Embedded Technology and Applications Symposium'],\n",
    "    'ccs': ['CCS', 'ACM Conference on Computer and Communications Security'],\n",
    "    'oakland' : ['IEEE Symposium on Security and Privacy'],\n",
    "    'usenixsec' : ['USENIX Security Symposium', 'USENIX Security'],\n",
    "    'pets' : ['PoPETs', 'Privacy Enhancing Technologies'],\n",
    "    'cav': ['CAV', 'CAV (1)', 'CAV (2)'],\n",
    "    'lics' : ['LICS', 'CSL-LICS'],\n",
    "    'nips': ['NIPS', 'NeurIPS'],\n",
    "    'icml': ['ICML', 'ICML (1)', 'ICML (2)', 'ICML (3)'],\n",
    "    'aaai': ['AAAI', 'AAAI/IAAI'],\n",
    "    'ubicomp' : ['UbiComp', 'Ubicomp', 'IMWUT', 'Pervasive'],\n",
    "    'emnlp': ['EMNLP', 'EMNLP-CoNLL', 'HLT/EMNLP'],\n",
    "    'acl' : ['ACL', 'ACL (1)', 'ACL (2)', 'ACL/IJCNLP', 'COLING-ACL'],\n",
    "    'naacl' : ['NAACL', 'HLT-NAACL', 'NAACL-HLT'],\n",
    "    'cvpr': ['CVPR', 'CVPR (1)', 'CVPR (2)'],\n",
    "    'eccv': ['ECCV', 'ECCV (1)', 'ECCV (2)', 'ECCV (3)', 'ECCV (4)', 'ECCV (5)', 'ECCV (6)', 'ECCV (7)', 'ECCV (8)', 'ECCV (9)', 'ECCV (10)', 'ECCV (11)', 'ECCV (12)', 'ECCV (13)', 'ECCV (14)', 'ECCV (15)', 'ECCV (16)'],\n",
    "    'icra': ['ICRA', 'ICRA (1)', 'ICRA (2)'],\n",
    "    'rss': ['Robotics: Science and Systems'],\n",
    "    'crypto': ['CRYPTO', 'CRYPTO (1)', 'CRYPTO (2)', 'CRYPTO (3)'],\n",
    "    'eurocrypt': ['EUROCRYPT', 'EUROCRYPT (1)', 'EUROCRYPT (2)', 'EUROCRYPT (3)'],\n",
    "}\n",
    "inverse_area_dict = {}\n",
    "for k,v in areadict.items():\n",
    "    n = len(v)\n",
    "    for i in range(1,n):\n",
    "        inverse_area_dict[v[i]] = v[0]\n",
    "for k,v in inverse_area_dict.items():\n",
    "    if k in conf_idx and v in conf_idx:\n",
    "        conf_idx[k] = conf_idx[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# munge the years\n",
    "min_year = papers.year.min()\n",
    "max_year = papers.year.max()\n",
    "span_years = max_year - min_year +1\n",
    "\n",
    "print(span_years,min_year,max_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsf_paper_n, _ = df2.shape\n",
    "nsf_paper_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "from unidecode import unidecode\n",
    "\n",
    "from collections import Counter,defaultdict\n",
    "# create or load some author data\n",
    "def dd():\n",
    "    return defaultdict(list)\n",
    "if False:\n",
    "    papers_per_year = {} \n",
    "    author_papers = defaultdict(dd)\n",
    "\n",
    "    for row in papers.itertuples():\n",
    "        paper_year = row[10]\n",
    "        conf = row[2]\n",
    "        n = row[4]\n",
    "        authors = row[3]\n",
    "        yc = papers_per_year.get(conf,np.zeros(2020-1970))\n",
    "        yc[paper_year-1970] += 1\n",
    "        papers_per_year[conf] = yc\n",
    "        for a in authors:\n",
    "            a = unidecode(ftfy.fix_encoding(a))\n",
    "            split_name = a.split(' ')\n",
    "            if not split_name[-1].isalpha() and len(split_name) > 2:\n",
    "                first_last = split_name[0] +' ' + split_name[-2]\n",
    "            else: \n",
    "                first_last = split_name[0] +' ' + split_name[-1]\n",
    "            author_papers[first_last.lower()][paper_year].append((conf,n))\n",
    "    import pickle\n",
    "    with open('nsf_auth.pkl','wb') as fp:\n",
    "        pickle.dump(author_papers,fp)\n",
    "else:\n",
    "    pass\n",
    "    with open('nsf_auth.pkl','rb') as fp:\n",
    "        author_papers = pickle.load(fp)\n",
    "    with open('papers_per_year.pkl','rb') as fp:\n",
    "        papers_per_year = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('papers_per_year.pkl','wb') as fp:\n",
    "    pickle.dump(papers_per_year,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddn():\n",
    "    return defaultdict(int)\n",
    "author_amounts = defaultdict(ddn)\n",
    "for i,row in enumerate(df2.itertuples()):\n",
    "    authors, year, amount = row[3],row[4],row[5]\n",
    "    # some infinite amounts exist! bad!\n",
    "    if not np.isfinite(amount):\n",
    "        continue\n",
    "\n",
    "    amount = amount# min(amount,1e7)\n",
    "    for a in authors:\n",
    "        split_name = a.split(' ')\n",
    "        first_last = split_name[0] +' ' + split_name[-1]\n",
    "        for yr in range(int(year),2020):\n",
    "            author_amounts[first_last.lower()][yr] += amount/len(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2.infaward > 1e9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(max(v.values()),k) for k,v in author_amounts.items() if k ],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "sigma = 2\n",
    "weights = []\n",
    "for i in range(span_years):\n",
    "    a = np.array([scipy.stats.norm.pdf( (j-i)/sigma) for j in range(span_years)])\n",
    "    a[a < 0.05] = 0\n",
    "    weights.append(a/np.linalg.norm(a))\n",
    "_ = plt.plot(np.arange(span_years)+min_year,weights[2000-min_year])\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#pairs_of_years = itertools.product(range(span_years),range(span_years))\n",
    "\n",
    "wdict = {}\n",
    "for i,j,k in itertools.product(range(unique_confs.shape[0]),range(span_years),range(span_years)):\n",
    "    wdict[i*span_years+j,i*span_years+k] = weights[j][k]\n",
    "wsa = scipy.sparse.dok_matrix((span_years*unique_confs.shape[0],span_years*unique_confs.shape[0]))\n",
    "wsa._update(wdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design mattrix\n",
    "X = scipy.sparse.dok_matrix((nsf_paper_n,span_years*unique_confs.shape[0]))\n",
    "xdict = {}\n",
    "duplicate_authors = {}\n",
    "y = np.zeros(nsf_paper_n,dtype=np.float32)\n",
    "for i,row in enumerate(df2.itertuples()):\n",
    "    authors, year, amount = row[3],row[4],row[5]\n",
    "\n",
    "    for a in authors:\n",
    "        split_name = a.split(' ')\n",
    "        if not split_name[-1].isalpha() and len(split_name) > 2:\n",
    "            first_last = split_name[0] +' ' + split_name[-2]\n",
    "        else: \n",
    "            first_last = split_name[0] +' ' + split_name[-1]\n",
    "        for year_a,conf_list in author_papers[first_last.lower()].items():\n",
    "            if year_a <= year:\n",
    "                for paper in conf_list:\n",
    "                    j = span_years*conf_idx[paper[0]] + year_a-min_year\n",
    "                    xdict[(i,j)] = 1/paper[1]\n",
    "X._update(xdict)\n",
    "print(X.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scipy.sparse.csr_matrix(X)\n",
    "wsa = scipy.sparse.csr_matrix(wsa)\n",
    "X = X @ wsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_amounts = np.zeros(span_years,dtype=np.float32)\n",
    "y = np.zeros(nsf_paper_n,dtype=np.float32)\n",
    "\n",
    "if False:\n",
    "    for i,row in enumerate(df2.itertuples()):\n",
    "        authors, year, amount = row[3],row[4],row[5]\n",
    "        # some infinite amounts exist! bad!\n",
    "\n",
    "        if not np.isfinite(amount):\n",
    "            continue\n",
    "        if amount <= 20000: #what is that even for?\n",
    "            continue\n",
    "        # maybe the old years are misleading!?\n",
    "        #if year < 2002:\n",
    "        #    continue\n",
    "        # small grants are misleading? 150000\n",
    "        #if amount < 1e7:\n",
    "        #    continue\n",
    "        # giant grants are msileading?\n",
    "        #if amount >= 4e5:\n",
    "        #    amount = 4e5 + np.log((amount-4e5)+1)*4e3\n",
    "        if amount >= 1e7:\n",
    "            amount = 1e7 + np.log((amount-1e7)+1)*1e5\n",
    "        #print(len(authors),sum([(a in author_papers) for a in authors]))\n",
    "        #print(a)\n",
    "        #print(len(authors),sum([(a in author_papers) for a in authors]))\n",
    "        #print(a)\n",
    "        total_authors = len(authors)\n",
    "        needed_authors = 0.5 * total_authors # half of all authors\n",
    "        found_authors = sum([(a.lower() in author_papers) for a in authors])\n",
    "        if needed_authors > 0 and needed_authors <= found_authors:\n",
    "            y[i] = amount* (found_authors/total_authors)\n",
    "            year_amounts[year-1970] += amount\n",
    "if True: # get cumulative amount\n",
    "    for i,row in enumerate(df2.itertuples()):\n",
    "        authors, year, amount = row[3],row[4],row[5]\n",
    "        a2 = []\n",
    "        for a in authors:\n",
    "            split_name = a.split(' ')\n",
    "            if not split_name[-1].isalpha() and len(split_name) > 2:\n",
    "                first_last = split_name[0] +' ' + split_name[-2]\n",
    "            else: \n",
    "                first_last = split_name[0] +' ' + split_name[-1]\n",
    "            a2.append(first_last)\n",
    "        authors = a2\n",
    "        \n",
    "        # some infinite amounts exist! bad!\n",
    "        if not np.isfinite(amount):\n",
    "            continue\n",
    "\n",
    "        if amount < 1000: #50000\n",
    "            continue\n",
    "        total_authors = len(authors)\n",
    "        needed_authors = 0.5 * total_authors # half of all authors\n",
    "        found_authors = sum([(a.lower() in author_papers) for a in authors])\n",
    "        if needed_authors > 0 and needed_authors <= found_authors:\n",
    "            y[i] = sum([author_amounts[first_last.lower()][year] for first_last in authors])\n",
    "            year_amounts[year-1970] += sum([author_amounts[first_last.lower()][year] for first_last in authors])\n",
    "nonarxiv = np.ones(span_years*len(unique_confs))\n",
    "nonarxiv[span_years*conf_idx['CoRR']:span_years*(conf_idx['CoRR']+1)] = 0\n",
    "skipped_conf = scipy.sparse.diags(nonarxiv)\n",
    "skipped_data = scipy.sparse.diags((y != 0).astype(float))\n",
    "print(X.shape,skipped_conf.shape,skipped_data.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "y_orig = np.copy(y)\n",
    "_ = plt.hist(y,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_orig = np.copy(y)\n",
    "print(y_orig.min(),y_orig.max())\n",
    "print((y_orig > 0).sum())\n",
    "if False: # do log\n",
    "    y = np.copy(np.log(1+y_orig))\n",
    "    y[y == np.log(1)] = y[y != np.log(1)].mean()\n",
    "else:\n",
    "    y = np.copy(y_orig)\n",
    "    y[y == 0] = y[y != 0].mean()\n",
    "\n",
    "from matplotlib.pyplot import figure,hist\n",
    "hist((y-y.mean())/y.std(),100)\n",
    "figure()\n",
    "_ = hist(y,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X = scipy.sparse.csr_matrix(X)\n",
    "clf = SGDRegressor('huber',tol=1e-9,max_iter=100,verbose=0,penalty='l2',alpha=1e-3,epsilon=0.01,average=True)\n",
    "#clf = SGDRegressor('huber',tol=1e-9,max_iter=100,verbose=1,penalty='l1',alpha=1e-7)\n",
    "\n",
    "clf.fit(skipped_data @X@ skipped_conf ,(y-y.mean())/y.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_ord = np.argsort(np.squeeze(clf.coef_))\n",
    "conf_choice = ['SIGGRAPH','HRI','ECCV','Comput. Graph. Forum','Shape Modeling International','Symposium on Geometry Processing','Computer Aided Geometric Design','I. J. Robotics Res.','CVPR','International Journal of Computer Vision','Robotics: Science and Systems','ICRA','WACV','ICML','AISTATS','CoRR','SIGGRAPH Asia','ECCV','ICCV','ISER','Humanoids','3DV','IROS','CoRL','Canadian Conference on AI','ACCV','Graphics Interface','CRV','BMVC']\n",
    "ri_confs = np.zeros(len(unique_confs)*span_years)\n",
    "print(clf.intercept_)\n",
    "ms = clf.coef_.mean()\n",
    "ss = clf.coef_.std()\n",
    "seen = {}\n",
    "for i in range(len(unique_confs)*span_years):\n",
    "    idx = conf_ord[-(i+1)]\n",
    "    conf_name = unique_confs[idx//span_years]\n",
    "    conf_score = clf.coef_[idx]\n",
    "    if conf_name in conf_choice:\n",
    "        ri_confs[idx] = 1\n",
    "    if conf_name in conf_choice and conf_name not in seen:\n",
    "        print('{:20s}{}\\t{:.1f}'.format(conf_name[:20],str(min_year + (idx % span_years)),(conf_score-ms)/ss))\n",
    "        seen[conf_name] =1\n",
    "ri_confs.shape,ri_confs.sum(),X.shape\n",
    "\n",
    "conf_choice2 = ['SIGGRAPH','BMVC','AAAI','NIPS','CVPR','ICRA','ICML','ICCV','ECCV','IROS',\n",
    "               'International Journal of Computer Vision','Robotics: Science and Systems']\n",
    "conf_choice3 = []\n",
    "vs = clf.coef_.std()\n",
    "for conf in conf_choice2:\n",
    "    idx = conf_idx[conf]\n",
    "    s = max(clf.coef_[span_years*idx:span_years*(idx+1)])\n",
    "    conf_choice3.append((s,conf))\n",
    "plt.figure(figsize=(12,8))\n",
    "for s,conf in sorted(conf_choice3,reverse=True):\n",
    "    idx = conf_idx[conf]\n",
    "    _ = plt.plot(np.arange(min_year,max_year+1),(clf.coef_[span_years*idx:span_years*(idx+1)]/vs),label=conf)\n",
    "plt.grid()\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('value')\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig('nsf-fixed-total-2008-nonlog-names.pdf')\n",
    "figure()\n",
    "plt.plot(np.arange(min_year,max_year+1),year_amounts)\n",
    "figure(figsize=(12,8))\n",
    "for s,conf in sorted(conf_choice3,reverse=True):\n",
    "    idx = conf_idx[conf]\n",
    "    _ = plt.plot(np.arange(min_year,max_year+1),papers_per_year[conf],label=conf)\n",
    "pickle.dump(clf.coef_,open('nsf_fixed_total-nonlog-names.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 50\n",
    "i = -1\n",
    "j = 0\n",
    "seen = {}\n",
    "while j < top_k:\n",
    "    i += 1\n",
    "    idx = conf_ord[-(i+1)]\n",
    "    conf_name = unique_confs[idx//span_years]\n",
    "    if conf_name in seen:\n",
    "        continue\n",
    "    j+=1\n",
    "    conf_score = clf.coef_[idx]\n",
    "    seen[conf_name] = 1\n",
    "    print('{:20s}\\t{}\\t\\t{:.3f}\\t{:.2f}'.format(conf_name[:18],min_year + (idx % span_years),100*conf_score,(conf_score-ms)/ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = hist(clf.coef_,70)\n",
    "pickle.dump(clf.coef_,open('nsf_indep2.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Xauth is None or (Xauth.shape[1] != span_years*unique_confs.shape[0]):  \n",
    "    Xauth = scipy.sparse.dok_matrix((len(unique_names),span_years*unique_confs.shape[0]))\n",
    "    xdict = {}\n",
    "    auth_years = np.ones((len(unique_names),2)) * np.array([3000,1000]) \n",
    "    for row in papers.itertuples():\n",
    "        paper_year = row[10]\n",
    "        #if row['year'] < 2005:\n",
    "        #    continue\n",
    "        #print(row)\n",
    "        #if row['conf'] == 'CoRR':\n",
    "        #    continue\n",
    "        conf = row[2]\n",
    "        n = row[4]\n",
    "        authors = row[3]\n",
    "        j = span_years*conf_idx[conf] + (paper_year-min_year)\n",
    "        for a in authors:\n",
    "            i = name_idx[a]\n",
    "            xdict[(i,j)] = 1/n + xdict.get((i,j),0)\n",
    "            auth_years[i,0] = min(auth_years[i,0],paper_year)\n",
    "            auth_years[i,1] = max(auth_years[i,1],paper_year)\n",
    "    Xauth._update(xdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = clf.predict(Xauth) - np.squeeze(clf.intercept_)\n",
    "years_working = (1+auth_years[:,1]-auth_years[:,0])\n",
    "value_scores = scores\n",
    "norm_scores = (value_scores)/years_working\n",
    "ri_filter_mat = scipy.sparse.diags(ri_confs)\n",
    "ri_scores = clf.predict(Xauth.dot(ri_filter_mat))-np.squeeze(clf.intercept_)\n",
    "ri_norm_scores = ri_scores/years_working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_cand = ['Pulkit Agrawal',\n",
    " 'Joydeep Biswas',\n",
    " 'Katherine L. Bouman',\n",
    " 'David Braun',\n",
    " 'Jia Deng',\n",
    " 'Naomi T. Fitter',\n",
    " 'David F. Fouhey',\n",
    " 'Saurabh Gupta',\n",
    " 'Judy Hoffman',\n",
    " 'Hanbyul Joo',\n",
    " 'Honglak Lee',\n",
    " 'Changliu Liu',\n",
    " 'Petter Nilsson',\n",
    " \"Matthew O'Toole\",\n",
    " 'Alessandro Roncone',\n",
    " 'Alanson P. Sample',\n",
    " 'Manolis Savva',\n",
    " 'Adriana Schulz',\n",
    " 'Amy Tabb',\n",
    " 'Fatma Zeynep Temel',\n",
    " 'Long Wang',\n",
    " 'Cathy Wu',\n",
    " 'Ling-Qi Yan']\n",
    "print('{:20s}\\t{:4s}\\t{:4s}\\t{:4s}\\t{}'.format('name','rate','total','ri','years'))\n",
    "for ns, name in sorted([(value_scores[name_idx[ni]],ni) for ni in prev_cand],reverse=True):\n",
    "    ni = name_idx[name]\n",
    "    print('{:20s}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.0f}'.format(name,100*norm_scores[ni],100*value_scores[ni],100*ri_scores[ni],years_working[ni]))\n",
    "print('')\n",
    "curious_names = ['Xiaolong Wang 0004','Judy Hoffman','Paris Siminelakis',\n",
    "                 'Nicholas Rhinehart',\n",
    "                 'Humphrey Hu',\n",
    "                 'David F. Fouhey',\n",
    "                 'Lerrel Pinto',\n",
    "                 'Justin Johnson',\n",
    "                 'Amir Roshan Zamir',\n",
    "                 'Brian Okorn','David Held']\n",
    "print('{:20s}\\t{:4s}\\t{:4s}\\t{:4s}\\t{}'.format('name','rate','total','ri','years'))\n",
    "for _,name in sorted([(value_scores[name_idx[_]],_) for _ in curious_names],reverse=True):\n",
    "    ni = name_idx[name]\n",
    "    print('{:20s}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.0f}'.format(name,100*norm_scores[ni],100*value_scores[ni],100*ri_scores[ni],years_working[ni]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n best overall \\n')\n",
    "cmu_scores = []\n",
    "\n",
    "best_scores = np.argsort(value_scores)[::-1]\n",
    "#print(best_scores.shape,unique_names[best_scores[0]])\n",
    "fa_list = list(faculty_affil.name)\n",
    "fa_a_list = list(faculty_affil.affiliation)\n",
    "uni_names = [unique_names[i] for i in best_scores[:38000]]\n",
    "for name in set([aliasdict.get(n, n) for n in uni_names]):\n",
    "    if name in name_idx:\n",
    "        uni = 'unknown'\n",
    "        if name in fa_list:\n",
    "            uni = fa_a_list[fa_list.index(name)]\n",
    "        if name not in []:#['Jacob Walker','Justin Johnson','Pieter Abbeel','Martial Hebert','Jessica K. Hodgins','Abhinav Gupta','Christopher G. Atkeson','Tom M. Mitchell','Matthew T. Mason']:\n",
    "            if years_working[name_idx[name]] < 3:\n",
    "                continue\n",
    "            if years_working[name_idx[name]] > 8:\n",
    "                continue\n",
    "            if ri_scores[name_idx[name]] < 0.008:\n",
    "                continue\n",
    "            if auth_years[name_idx[name],1] < 2017:\n",
    "                continue\n",
    "        #if (np.array(X[name_idx[name],:].todense()) * ri_confs).sum() == 0:\n",
    "        #    continue\n",
    "        #print(name,auth_years[name_idx[name]])\n",
    "        score = norm_scores[name_idx[name]]\n",
    "        ri_vscore = ri_norm_scores[name_idx[name]]\n",
    "        vscore = value_scores[name_idx[name]]\n",
    "        cmu_scores.append((vscore,ri_scores[name_idx[name]],score,uni,name,auth_years[name_idx[name]],ri_vscore))\n",
    "    else:\n",
    "        pass\n",
    "        #print(name)\n",
    "        ri_norm_scores\n",
    "print('{:22s}\\t{:15s}\\t{:5s}\\t{:3s}\\t{:4s}\\t{:4s}\\t{} {}'.format('name','uni','rate','RI-t','total','RI-r','start','end'))\n",
    "for vs,ris,s,u,p,yrs,rir in sorted(cmu_scores,reverse=True):\n",
    "    print('{:22s}\\t{:15s}\\t{:.3f}\\t{:.1f}\\t{:.2f}\\t{:.2f}\\t{} {}'.format(p[:22],u[:15],s*100,ris*100,vs*100,rir*100,int(yrs[0]),int(yrs[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_faculty = faculty_affil[faculty_affil.affiliation == 'Carnegie Mellon University'] #Carnegie Mellon University\n",
    "uni_names = np.array(uni_faculty.name)\n",
    "uni_names = list(uni_names) + ['Nicholas Rhinehart','Jacob Walker','Lerrel Pinto','Brian Okorn','Leonid Keselman','Siddharth Ancha','Humphrey Hu']\n",
    "cmu_scores = []\n",
    "#uni_names = [unique_names[i] for i in (np.argsort(scores)[::-1])[:150]]\n",
    "for name in set([aliasdict.get(n, n) for n in uni_names]):\n",
    "    if name in name_idx:\n",
    "        #if ri_scores[name_idx[name]] < 2.5:\n",
    "        #    continue\n",
    "        score = scores[name_idx[name]]\n",
    "        cmu_scores.append((score,name))\n",
    "    else:\n",
    "        pass\n",
    "        #print(name)\n",
    "for s,p in sorted(cmu_scores,reverse=True):\n",
    "    print('{:30s}\\t\\t{:.3f}'.format(p,s*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du -h *.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Xauth,open('xauth.pkl','wb'))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
