{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import csv\n",
    "import scipy.sparse\n",
    "Xauth = None\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('useful_venue_list.pkl.gz','rb') as fp:\n",
    "    all_venues = pickle.load(fp)\n",
    "with gzip.open('useful_authors_list.pkl.gz','rb') as fp:\n",
    "    all_authors = pickle.load(fp)\n",
    "with gzip.open('useful_papers.pkl.gz','rb') as fp:\n",
    "    all_papers = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_idx = {v:i for i,v in enumerate(all_venues)}\n",
    "name_idx = {v:i for i,v in enumerate(all_authors)}\n",
    "n_confs = len(all_venues)\n",
    "n_auths = len(all_authors)\n",
    "print(n_confs,n_auths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_affil = pd.read_csv('faculty-affiliations.csv')\n",
    "ranks = pd.read_csv('other_ranks/ranks.csv')\n",
    "def csv2dict_str_str(fname):\n",
    "    with open(fname, mode='r') as infile:\n",
    "        rdr = csv.reader(infile)\n",
    "        d = {rows[0].strip(): rows[1].strip() for rows in rdr}\n",
    "    return d\n",
    "aliasdict = csv2dict_str_str('dblp-aliases-expanded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year = all_papers[0][6]\n",
    "max_year = all_papers[-1][6]\n",
    "span_years = max_year - min_year + 1\n",
    "print(min_year,max_year,span_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_years = np.ones((n_auths,2)) * np.array([3000,1000]) \n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    for a in authors:\n",
    "        i = name_idx[a]\n",
    "        auth_years[i,0] = min(auth_years[i,0],year)\n",
    "        auth_years[i,1] = max(auth_years[i,1],year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import gmean\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from scipy.stats.mstats import trimmed_mean, trimmed_std\n",
    "\n",
    "clf_files2 = ['weights_faculty_above6_linear_2_75_25_0.pkl',\n",
    "              'weights_nsfmarginal_above6_log_2_0_25_0.pkl',\n",
    "              'weights_salary_above6_linear_2_0_25_0.pkl']\n",
    "# without salary data\n",
    "clf_files3 = ['weights_faculty_above6_linear_2_75_25_0.pkl',\n",
    "              'weights_nsfmarginal_above6_log_2_0_25_0.pkl']\n",
    "\n",
    "#clf_files2 = [\n",
    "#              'weights_salary_above6_linear_2_0_25_0.pkl']\n",
    "clfs2 = [pickle.load(open(c,'rb')) for c in clf_files2]\n",
    "\n",
    "years_per_conf = clfs2[0].shape[0]//n_confs\n",
    "\n",
    "#span_years = 50 \n",
    "YEAR_BLOCKS = span_years//years_per_conf\n",
    "\n",
    "print(span_years,years_per_conf)\n",
    "#repeat_needed = span_years//(clfs[0].shape[0]//n_confs)\n",
    "#if repeat_needed > 1:\n",
    "#    clfs = [np.repeat(_,repeat_needed) for _ in clfs2]\n",
    "clfs = clfs2\n",
    "clfs = [np.squeeze(_) for _ in clfs]\n",
    "\n",
    "\n",
    "no_conf_weights = abs(np.vstack(clfs)).sum(0)\n",
    "no_conf_weights = (no_conf_weights != 0).astype(np.float)\n",
    "\n",
    "print([_.shape for _ in clfs])\n",
    "clp =  int(os.environ.get('COMBINE_CLIP',14)) #7\n",
    "clfs2 = []\n",
    "\n",
    "clfs = [(c-c.mean(0))/c.std(0) for c in clfs]\n",
    "clfs = [np.minimum(clp*2,np.maximum(-clp*2,c)) for c in clfs]\n",
    "if False:\n",
    "    clfs2 = clfs\n",
    "    pass\n",
    "elif True:\n",
    "    for result_clf,w in zip(clfs,[1,1,1]):\n",
    "        result_clf = result_clf.reshape((-1,years_per_conf))\n",
    "\n",
    "        #plt.plot(result_clf.sum(0)/result_clf.sum(0).sum(),label='sum')\n",
    "        print(abs(result_clf.mean(0)).mean(),abs(result_clf.std(0)).mean())\n",
    "        #result_clf = np.minimum(30,np.maximum(result_clf,-30))\n",
    "        #result_clf = (result_clf)/result_clf.std(0)\n",
    "        # nearest reflect mirror\n",
    "        meanv = gaussian_filter1d(result_clf.mean(0),0.9,mode='nearest')#result_clf.mean(0)#trimmed_mean(result_clf,0,axis=0)#gaussian_filter1d(result_clf.mean(0),0.9,mode='nearest')\n",
    "        stdv =  gaussian_filter1d(np.sqrt(np.mean((result_clf-meanv)**2,0)),0.9,mode='nearest')#trimmed_std(result_clf,0.0001,axis=0)#gaussian_filter1d(result_clf.std(0),0.9,mode='nearest')\n",
    "        #plt.plot(stdv)\n",
    "\n",
    "        result_clf = w*(result_clf-meanv)/stdv\n",
    "        result_clf = result_clf.reshape((-1))\n",
    "        clfs2.append(result_clf)\n",
    "else:\n",
    "    clfs2 = [(c-c.mean(0))/c.std(0) for c in clfs]\n",
    "clfs = clfs2\n",
    "\n",
    "clfs = [np.minimum(clp,np.maximum(-clp,c)) for c in clfs]\n",
    "clfs = np.vstack(clfs)\n",
    "\n",
    "if True: # weights\n",
    "    if False: # per conf\n",
    "        w = np.exp(-(clfs-np.median(clfs,0))**2/1e-3)\n",
    "        w /= w.sum(0)\n",
    "        print(w.sum(1))\n",
    "    else: # per regression\n",
    "        w = np.exp(-abs(clfs-np.median(clfs,0)).mean(1)/0.05)\n",
    "        w /= w.sum()\n",
    "        w = w.reshape((-1,1))\n",
    "        print(w)\n",
    "else:\n",
    "    w = np.ones(len(clfs2))\n",
    "#print(w)\n",
    "\n",
    "clf = np.mean(np.vstack([np.median(clfs,0),w*clfs]),0)*no_conf_weights\n",
    "#clf = gmean(clfs-clfs.min(0)+1,0)*no_conf_weights\n",
    "#clf = clf# - clf.min()\n",
    "\n",
    "clf_gold = np.copy(clf)\n",
    "np.save('clf_gold.pkl',clf_gold)\n",
    "#clf_gold = np.ones_like(clf)\n",
    "#clf = clf_gold\n",
    "clf_yearsize = np.copy(clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[_ for _ in all_venues if 'Nature'.lower() in _.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_conf_weights[conf_idx['SIGGRAPH']*years_per_conf+(2008-1970)//YEAR_BLOCKS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for paper in all_papers:\n",
    "#    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "#    n = len(authors)\n",
    "#    j = years_per_conf*conf_idx[venue] + (year-min_year)//YEAR_BLOCKS\n",
    "#    clfs[0][conf_idx['SIGGRAPH']*years_per_conf:conf_idx['SIGGRAPH']*years_per_conf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_none = np.copy(clf)\n",
    "#clf_size = np.copy(clf)\n",
    "#clf_year = np.copy(clf)\n",
    "#clf_yearsize = np.copy(clf)\n",
    "clf_none = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR_BLOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf_none is not None:\n",
    "    i = 0\n",
    "    plt.figure(figsize=(12,12))\n",
    "    for name,clf in zip(['None','Size','Year','Year + Size'],[clf_none,clf_size,clf_year,clf_yearsize]):\n",
    "        plt.subplot(2,2,i+1)\n",
    "        i+=1\n",
    "        vs = clf.std()\n",
    "        conf_vector = ['NIPS','AAAI','ICML','IJCAI','AISTATS','UAI']\n",
    "        #conf_vector = ['SIGIR','JCDL','CIKM','KDD','WWW','SIGMOD Conference']\n",
    "        #conf_vector = ['STOC','FOCS','SODA','EC','WINE']\n",
    "        for conf in conf_vector:\n",
    "            idx = conf_idx[conf]\n",
    "            _ = plt.plot(np.arange(min_year,max_year+1),(clf[years_per_conf*idx:years_per_conf*(idx+1)]/vs),label=conf,lw=5)\n",
    "            plt.grid(True)\n",
    "            plt.xlabel('year')\n",
    "            plt.ylabel('venue scores (in standard deviations)')\n",
    "        plt.title(name)\n",
    "        plt.legend(loc=2,fancybox=True, framealpha=0.8, borderpad=1,frameon=True)\n",
    "        #plt.ylim(0,25)\n",
    "        plt.xlim(1970,2020)\n",
    "        plt.xticks(np.arange(1970,2020,10),[str(_) for _ in np.arange(1970,2020,10)])\n",
    "        plt.tight_layout()\n",
    "    plt.suptitle('Normalization Methods',size=36,fontweight='demibold')\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.savefig('size_year_norm2.pdf',facecolor='w',edgecolor='w')\n",
    "    clf = clf_yearsize\n",
    "    clf_gold = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for clf_tmp,clf_name in zip(clfs2,clf_files2):\n",
    "    clf_tmp = clf_tmp.reshape((-1,years_per_conf))[:,-1]\n",
    "    plt.hist(clf_tmp,100,label=clf_name.split('_')[1],alpha=0.3)\n",
    "    print(clf_tmp.mean(),clf_tmp.std())\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(clf_files2) == 3:\n",
    "    df_clfs2 = pd.DataFrame(np.vstack(clfs2).T,columns=['Faculty','NSF','Salary'])\n",
    "    print(df_clfs2.corr('spearman').to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_ord = np.argsort(np.squeeze(clf))\n",
    "#'Scientometrics','JCDL','NIPS',\n",
    "conf_choice = ['ICLR','ICPR','Foundations and Trends in Electronic Design Automation',\n",
    "               'FOCS','STOC','ICRA','ICML','SIGGRAPH','HRI','ECCV','Comput. Graph. Forum',\n",
    "               'Shape Modeling International','Symposium on Geometry Processing',\n",
    "               'Computer Aided Geometric Design','I. J. Robotics Res.','CVPR',\n",
    "               'International Journal of Computer Vision','Robotics: Science and Systems',\n",
    "               'ICRA','WACV','ICML','AISTATS','CoRR','SIGGRAPH Asia','ECCV','ICCV','ISER','MICCAI',\n",
    "               'Humanoids','3DV','IROS','CoRL','Canadian Conference on AI','ACCV','Graphics Interface',\n",
    "               'CRV','BMVC','Theory of Computing','SODA','SIAM J. Comput.','J. ACM','NIPS','Allerton',\n",
    "               'ICC','GLOBECOM','IEEE Trans. Vehicular Technology','IEEE Trans. Wireless Communications','Underwater Networks']\n",
    "ri_confs = np.zeros(clf_gold.shape[0])\n",
    "ms = clf_gold.mean()\n",
    "ss = clf_gold.std()\n",
    "np.set_printoptions(precision=1)\n",
    "seen = {}\n",
    "for i in range(clf_gold.shape[0]):\n",
    "    idx = conf_ord[-(i+1)]\n",
    "    conf_name = all_venues[idx//years_per_conf]\n",
    "    conf_score = clf[idx]\n",
    "    if conf_name in conf_choice:\n",
    "        ri_confs[idx] = 1\n",
    "    if conf_name in conf_choice and conf_name not in seen:\n",
    "        vec = clfs[:,idx]\n",
    "        print('{:20s}{}\\t{:.1f}\\t{}'.format(conf_name[:20],str(min_year + YEAR_BLOCKS*(idx % years_per_conf)),(conf_score-ms)/ss,vec))\n",
    "        seen[conf_name] =1\n",
    "ri_confs.shape,ri_confs.sum()\n",
    "seen = {}\n",
    "print('\\n\\n')\n",
    "conf_ord2 = np.argsort(clf_gold.reshape((-1,years_per_conf))[:,-1])[::-1]\n",
    "for idx in conf_ord2[:100]:\n",
    "    conf_name = all_venues[idx]\n",
    "    conf_score = clf[(idx+1)*years_per_conf-1]\n",
    "    if True or conf_name in conf_choice:\n",
    "        vec = clfs[:,(idx+1)*years_per_conf-1]\n",
    "        print('{:20s}{}\\t{:.1f}\\t{}'.format(conf_name[:20],2018,(conf_score-ms)/ss,vec))\n",
    "        seen[conf_name] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(clf_files2) == 3:\n",
    "    clf_fnsf = np.mean(clfs[0:1],0)*no_conf_weights\n",
    "    clf_fnsf /= clf_fnsf.std()\n",
    "    clf_s = np.mean(clfs[1:3],0)*no_conf_weights\n",
    "    clf_s /= clf_s.std()\n",
    "    ratio = np.nan_to_num(clf_s/clf_fnsf) * (clf_s > 5).astype(np.float)\n",
    "    ratio_sort = np.argsort(ratio)[::-1]\n",
    "    top_k = 100\n",
    "    seen = {}\n",
    "    i = -1\n",
    "    while len(seen) < top_k and i < (ratio_sort.shape[0]-1) :\n",
    "        i+=1\n",
    "\n",
    "        idx = ratio_sort[i]\n",
    "        conf_name = all_venues[idx//years_per_conf]\n",
    "        if conf_name in seen: \n",
    "            continue\n",
    "        year = min_year + (idx % years_per_conf)*YEAR_BLOCKS\n",
    "        #if len(seen) > 25:\n",
    "        #print(conf_name,clf_s[idx],clf_fnsf[idx])\n",
    "        print(\"'\"+conf_name+\"'\",end=',')#,year#,clf_s[idx],clf_fnsf[idx])\n",
    "        seen[conf_name] = year\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 100\n",
    "i = -1\n",
    "j = 0\n",
    "seen = {}\n",
    "while j < top_k:\n",
    "    i += 1\n",
    "    idx = conf_ord[-(i+1)]\n",
    "    conf_name = all_venues[idx//years_per_conf]\n",
    "    if conf_name in seen: #or (idx%span_years) < 40 or (idx%span_years) > 45:\n",
    "        continue\n",
    "    j+=1\n",
    "    conf_score = clf[idx]\n",
    "    seen[conf_name] = 1\n",
    "    print('{:40s}\\t{}\\t\\t{:.3f}\\t{:.2f}'.format(conf_name[:38],min_year + (idx % years_per_conf),100*conf_score,(conf_score-ms)/ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[_ for _ in all_venues if 'Nano to Macro' in _] #SC INFOCOM CCCG SIGCSE HotOS WWOS \n",
    "#XP7.52 Workshop on Database Theory      \t1978\t\t885.024\t10.96\n",
    "#ACM SIGMOD Workshop on Research Issues  \t1985\t\t869.130\t10.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[_ for _ in all_venues if 'nature'.lower() in _.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "conf_fix = csv2dict_str_str('name_alias.csv')\n",
    "conf_fix.update({'International Journal of Computer Vision': 'IJCV',\n",
    "             'Conference on Designing Interactive Systems':'DIS',\n",
    "            'IEEE Trans. Pattern Anal. Mach. Intell.' : 'PAMI',\n",
    "            'Robotics: Science and Systems':'RSS',\n",
    "            'SIGMOD Conference': 'SIGMOD',\n",
    "            'I. J. Robotics Res.': 'IJRR',\n",
    "            'Symposium on Computer Animation':'SCA',\n",
    "            'Comput. Graph. Forum':'CGF',\n",
    "            'ACM Trans. Comput.-Hum. Interact.': 'TOCHI',\n",
    "            'Theory of Computing': 'ToC',\n",
    "                 'Conference on Computational Complexity':'CCC',\n",
    "            'Comput. Graph. Forum': 'CGF',\n",
    "            'APPROX-RANDOM':'APPROX-RANDOM',\n",
    "                 'IEEE Trans. Med. Imaging': 'IEEE T-MI'\n",
    "           })\n",
    "\n",
    "for clf in [clf_gold]:\n",
    "    plt.style.use('fivethirtyeight')\n",
    "\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "    conf_choice2 = ['SIGGRAPH','AAAI','NIPS','CVPR','ICRA','ICML','ICCV','ECCV', 'I. J. Robotics Res.',\n",
    "                    'WACV','CHI','3DV','HRI',  'AAMAS','IJCAI',\n",
    "                   'ISER','Robotics: Science and Systems','IROS','CoRL','ICLR']\n",
    "    #conf_choice2 = ['CVPR','ECCV','ICCV','International Journal of Computer Vision','Robotics: Science and Systems']\n",
    "    #conf_choice2 = ['SIGMOD Conference','VLDB','ICDT','KDD','ACM Trans. Inf. Syst.','PODS']\n",
    "    #conf_choice2 = ['ACL','EMNLP','EACL']\n",
    "\n",
    "    conf_choices = [conf_choice2, \n",
    "                    ['STOC','FOCS','SODA','Conference on Computational Complexity','COLT','APPROX-RANDOM'],\n",
    "                    ['UAI','AAAI','IJCAI','ICML','NIPS','AISTATS'],\n",
    "                    ['ECCV','ICCV','CVPR','WACV','International Journal of Computer Vision','IEEE Trans. Pattern Anal. Mach. Intell.'],\n",
    "                    ['Robotics: Science and Systems','IROS','ICRA','WAFR','HRI','I. J. Robotics Res.'],\n",
    "                    ['SIGGRAPH','SIGGRAPH Asia','I3D','ACM Trans. Graph.','Symposium on Computer Animation','Comput. Graph. Forum'],\n",
    "                    ['SIGIR','JCDL','CIKM','KDD','WWW','SIGMOD Conference'],\n",
    "                    ['CHI','Conference on Designing Interactive Systems','CSCW','UbiComp','UIST','ICWSM'],\n",
    "                    ['IEEE Trans. Med. Imaging','MICCAI','IPMI','ISBI','NeuroImage','IPCAI'],\n",
    "                    ['EC','WINE','SIGecom Exchanges','ACM Trans. Economics and Comput.','SAGT'],\n",
    "                    ['ACCV','BMVC','FGR','WACV','CRV']\n",
    "                   ]\n",
    "    cm = plt.cm.get_cmap('tab20')\n",
    "\n",
    "    labels = ['All','Theory','AI/ML','Vision','Robotics','Graphics','IR','HCI','Medical','Economics','Vision #2']\n",
    "    #conf_choices = [['Robotics: Science and Systems','IROS','ICRA','CoRL','WAFR','HRI','ISER']]\n",
    "    for conf_choice2,l in zip(conf_choices,labels):\n",
    "\n",
    "\n",
    "        #conf_choice2 = \n",
    "        conf_choice3 = []\n",
    "        vs = clf.std()\n",
    "        for conf in conf_choice2:\n",
    "            idx = conf_idx[conf]\n",
    "            s = max(clf[years_per_conf*idx+(2014-1970)//YEAR_BLOCKS:years_per_conf*idx+(2019-1970)//YEAR_BLOCKS])#max(clf[span_years*idx:span_years*(idx+1)]) ##s = clf[span_years*idx+2015-1970]#\n",
    "            conf_choice3.append((s,conf))\n",
    "        if l == 'All':\n",
    "            plt.figure(figsize=(18,10))\n",
    "        else:\n",
    "            plt.figure(figsize=(3*2.7,2*2.7))\n",
    "        index = 0\n",
    "        for s,conf in sorted(conf_choice3,reverse=True):\n",
    "            idx = conf_idx[conf]\n",
    "            if conf in conf_fix:\n",
    "                conf = conf_fix[conf]\n",
    "            if l == 'All':\n",
    "                line = (clf[years_per_conf*idx:years_per_conf*(idx+1)]/vs)[:-1]\n",
    "                line = [float('nan') if x==0 else x for x in line]\n",
    "\n",
    "                _ = plt.plot((np.arange(years_per_conf-1)*YEAR_BLOCKS +1970 ),line,marker='.',markersize=15,label='{:9s} ({:.1f})'.format(conf,s/vs),lw=7,alpha=0.9,c=cm(index))\n",
    "            else:\n",
    "                line = (np.maximum(0,clf[years_per_conf*idx:years_per_conf*(idx+1)]/vs))[:-1]\n",
    "                line = [float('nan') if x==0 else x for x in line]\n",
    "\n",
    "                _ = plt.plot((np.arange(years_per_conf-1)*YEAR_BLOCKS +1970 ),line,alpha=0.9,marker='.',markersize=15,label='{} ({:.1f})'.format(conf,s/vs),lw=5)\n",
    "            index += 1\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('year',fontsize=26)\n",
    "        plt.ylabel('venue scores',fontsize=26)\n",
    "        #plt.title('WITH Temporal Adjustment')\n",
    "        plt.title(l,fontsize=32) #+ ' (with size normalization)'\n",
    "        #plt.title(l+ ' (with size normalization)',fontsize=32) #'\n",
    "\n",
    "        plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        if l == 'All':\n",
    "            plt.legend(loc=3,fancybox=True, framealpha=0.8, borderpad=1,frameon=True,markerfirst=True,prop={'stretch':0,'size':18,'family': 'monospace','weight':500})\n",
    "        else:\n",
    "            plt.legend(loc=3,fancybox=True, framealpha=0.8, borderpad=1,frameon=True,markerfirst=True,fontsize=18)\n",
    "              #plt.ylim(0,16.)\n",
    "        plt.ylim(bottom=0.05) # hide the 0 label, effectively. can also pad x axis below\n",
    "        plt.xlim(1970,2020)\n",
    "        plt.xticks(np.arange(1970,2020,10),[str(_) for _ in np.arange(1970,2020,10)],fontsize=22)\n",
    "        plt.yticks(fontsize=22)\n",
    "        #plt.gca().tick_params('x',pad=10)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(l.split('/')[0] + '_t_year_size.pdf',facecolor='w',edgecolor='w')\n",
    "\n",
    "\n",
    "        #plt.show()\n",
    "\n",
    "    #plt.show()\n",
    "clf = clf_gold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for style in []:##plt.style.available:#:\n",
    "    plt.style.use(style)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for s,conf in sorted(conf_choice3,reverse=True):\n",
    "        idx = conf_idx[conf]\n",
    "        weights = [clf_gold[years_per_conf*idx + yr]/vs for yr in offset_years]\n",
    "        _ = plt.plot(np.arange(min_year,max_year+1),weights,label=conf,lw=5)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('year')\n",
    "    plt.ylabel('value')\n",
    "    plt.title(style)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Xauth is None:\n",
    "    count_vecs = {}\n",
    "    paper_vecs = []\n",
    "    for paper in all_papers:\n",
    "        tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "        n = len(authors)\n",
    "        j = years_per_conf*conf_idx[venue] + (year-min_year)//YEAR_BLOCKS\n",
    "\n",
    "        if n not in count_vecs:\n",
    "            author_scores = np.ones(n) #1/(np.arange(n)+1) \n",
    "            #author_scores[-1] = author_scores[0]\n",
    "            author_score_sum = author_scores.sum()\n",
    "            #author_scores /= author_score_sum\n",
    "            count_vecs[n] = author_scores #/ author_score_sum\n",
    "        else:\n",
    "            author_scores = count_vecs[n]\n",
    "            paper_vecs.append([(name_idx[a],j,v) for a,v in zip(authors,author_scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Xauth is None or Xauth.shape[1] != years_per_conf*n_confs:\n",
    "    import scipy.sparse\n",
    "    Xauth = scipy.sparse.dok_matrix((n_auths,years_per_conf*n_confs))\n",
    "    xdict = {}\n",
    "    if False:\n",
    "        for paper in all_papers:\n",
    "            tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "            n = len(authors)\n",
    "            j = years_per_conf*conf_idx[venue] + (year-min_year)//YEAR_BLOCKS\n",
    "            for ai in range(n):#zip(count_vecs[n],authors):\n",
    "                i = name_idx[authors[ai]]\n",
    "                #xdict[(i,j)] = 1/n + xdict.get((i,j),0)\n",
    "                xdict[(i,j)] = count_vecs[n][ai] + xdict.get((i,j),0)\n",
    "\n",
    "    else:\n",
    "        for paper_vec in paper_vecs:\n",
    "            for i,j,v in paper_vec:\n",
    "                xdict[(i,j)] = v + xdict.get((i,j),0)\n",
    "\n",
    "    Xauth._update(xdict)\n",
    "            \n",
    "    Xauth = scipy.sparse.csr_matrix(Xauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_counts = np.zeros(n_auths)\n",
    "paper_counts_c = np.zeros(n_confs)\n",
    "paper_counts_year = {}#np.zeros(n_auths)\n",
    "paper_counts_year_counter = np.zeros((max_year-min_year+1))\n",
    "\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    paper_counts_c[conf_idx[venue]] += 1\n",
    "    paper_counts_year_counter[year-min_year] += 1\n",
    "    for a in authors:\n",
    "        i = name_idx[a]\n",
    "        paper_counts[i] += 1\n",
    "        paper_counts_year[(a,year)] = 1+ paper_counts_year.get((a,year),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_uni = pd.read_csv('other_ranks/cmu_faculty.csv')\n",
    "cmu_uni = cmu_uni.fillna('Other')\n",
    "cmu_uni = cmu_uni[cmu_uni.dept == 'RI']\n",
    "#print(list(cmu_uni.name))\n",
    "uni_names = list(cmu_uni.name)#['Xuemin Shen','H. Vincent Poor','Kang G. Shin','Mohamed-Slim Alouini','Lajos Hanzo']#list(cmu_uni.name)\n",
    "#uni_names = list(faculty_affil[faculty_affil.affiliation == 'Johns Hopkins University'].name)\n",
    "print(len(uni_names))\n",
    "conf_counts = {}\n",
    "conf_counts_value = {}\n",
    "\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    if year < 2000:\n",
    "        continue\n",
    "    for a in authors:\n",
    "        if a in uni_names:\n",
    "            conf_counts[venue] = 1/n + conf_counts.get(venue,0)\n",
    "            conf_counts_value[venue] = clf_gold[years_per_conf*(conf_idx[venue]) + (year-min_year)//YEAR_BLOCKS]/n + conf_counts_value.get(venue,0)\n",
    "conf_counts_value = {k: v/conf_counts[k] for k,v in conf_counts_value.items()}\n",
    "ri_fav_confs = [(conf_counts[_[1]]*conf_counts_value[_[1]],_[1],conf_counts[_[1]],conf_counts_value[_[1]]) for _ in sorted([(v,k) for k,v in conf_counts.items() if v > 0],reverse=True)]\n",
    "[_[1] for _ in sorted(ri_fav_confs,reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_filter = np.zeros_like(clf_gold).reshape((-1,years_per_conf))\n",
    "#year_filter[:-10] = 1\n",
    "confs_to_filter = ['ECCV','ICCV','CVPR','WACV','International Journal of Computer Vision',\n",
    "                   'IEEE Trans. Pattern Anal. Mach. Intell.',\n",
    "                   'Robotics: Science and Systems','IROS','ICRA','WAFR','HRI','I. J. Robotics Res.',\n",
    "                   '3DV','BMVC','ACCV','ISRR','ISER','NIPS','SIGGRAPH','AAAI','Humanoids','SIGGRAPH Asia']\n",
    "confs_to_filter = ri_fav_confs\n",
    "best_ri_confs = []\n",
    "for conf in confs_to_filter:\n",
    "    if conf[-2] >= 1.0:\n",
    "        best_ri_confs.append(conf)\n",
    "        #print(conf)\n",
    "#        year_filter[conf_idx[conf[1]],-7:] = 1\n",
    "#year_filter[:,-15:] = 1\n",
    "\n",
    "for thing in sorted([(_[-1],_[:-1]) for _ in best_ri_confs],reverse=True):\n",
    "    print(thing)\n",
    "year_filter = np.ones_like(clf_gold).reshape((-1,years_per_conf))\n",
    "\n",
    "total_scores = Xauth.dot(clf_gold * year_filter.reshape((-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_rank_people = pickle.load(open('new_pagerank_people.pkl','rb'))\n",
    "page_rank_confs = pickle.load(open('new_pagerank_conf.pkl','rb'))\n",
    "page_rank_clf = np.repeat(page_rank_confs,years_per_conf)\n",
    "page_rank_scores = Xauth.dot(page_rank_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.read_csv('other_ranks/correlation_cleaned.csv')\n",
    "df_corr = df_corr.drop(columns=[_ for _ in df_corr.columns if 'Unnamed' in _])\n",
    "df_corr = df_corr.drop(columns=['pms','n_papers'])\n",
    "df_corr = df_corr.rename(columns={'totals': 'venue_score', 'csrp': 'csr_pubs','csrpn': 'csr_adj','gcite': 'influence'})\n",
    "df_corr = df_corr[['name','papers', 'citations', 'h-index',\n",
    "       'i10','csr_pubs', 'csr_adj','venue_score','influence']]\n",
    "df_corr = df_corr.dropna('index')\n",
    "df_corr.index = df_corr.name\n",
    "df_corr['pagerankA'] = df_corr.venue_score\n",
    "df_corr['pagerankC'] = df_corr.venue_score\n",
    "\n",
    "ts = total_scores.std()\n",
    "tm = total_scores.mean()\n",
    "for name in df_corr.name:\n",
    "    if name in name_idx:\n",
    "        idx = name_idx[name]\n",
    "        df_corr.loc[name,'venue_score'] = total_scores[idx]\n",
    "        df_corr.loc[name,'pagerankA'] = page_rank_people[idx]\n",
    "        df_corr.loc[name,'pagerankC'] = page_rank_scores[idx]\n",
    "        #print(name,df_corr.loc[name,'papers'],paper_counts[idx])\n",
    "        df_corr.loc[name,'papers'] = paper_counts[idx]\n",
    "\n",
    "\n",
    "print(df_corr.corr('spearman').loc['influence','venue_score'],df_corr.corr('kendall').loc['influence','venue_score'],df_corr.corr('spearman').loc['h-index','venue_score'])\n",
    "#if clfn == clfs_test.shape[-1]:\n",
    "df_corr[['papers','citations','h-index','i10','csr_pubs','venue_score','pagerankA','pagerankC','influence']].corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv('other_ranks/faculty_affil_scholar.csv')\n",
    "fuzzythresh=0.9 #0.9\n",
    "df_merged.loc[df_merged.fuzzyscore > fuzzythresh,'dblpname'] = df_merged[df_merged.fuzzyscore > fuzzythresh].fuzzyname\n",
    "df_merged.loc[df_merged.fuzzyscore > fuzzythresh,'dblpexists'] = 1\n",
    "df_merged = df_merged[df_merged.dblpexists == 1]\n",
    "df_merged['venue_score'] = np.ones_like(df_merged.dblpexists)\n",
    "df_merged['pagerankA'] = np.ones_like(df_merged.dblpexists)\n",
    "df_merged['pagerankC'] = np.ones_like(df_merged.dblpexists)\n",
    "df_merged['papers'] = np.ones_like(df_merged.dblpexists)\n",
    "\n",
    "ts = total_scores.std()\n",
    "tm = total_scores.mean()\n",
    "df_merged = df_merged.set_index(df_merged.dblpname)\n",
    "seen_map = {}\n",
    "for name in df_merged.index:\n",
    "    if name in name_idx:\n",
    "        idx = name_idx[name]\n",
    "        seen_map[name] = 1\n",
    "        df_merged.loc[name,'venue_score'] = total_scores[idx]\n",
    "        df_merged.loc[name,'pagerankA'] = page_rank_people[idx]\n",
    "        df_merged.loc[name,'pagerankC'] = page_rank_scores[idx]\n",
    "        df_merged.loc[name,'papers'] = paper_counts[idx]\n",
    "df_merged = df_merged.drop(columns=['Unnamed: 0','First Name','Last Name','Sholar link','Rank (Full, Associate, Assistant, Other)','Full Name','University_y','University_x','Unnamed: 11',\"ID\",'fuzzyname','dblpexists','fuzzyscore','UniversityID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msar_filt = df_msar[df_msar.dblp_name != 'NotAConf'].copy()#.sort_values('H',ascending=False)\n",
    "scores = []\n",
    "paperss = []\n",
    "pagerankc = []\n",
    "for row in df_msar_filt.itertuples():\n",
    "    conf = row[-1]\n",
    "    if conf in conf_idx:\n",
    "        idx = conf_idx[conf]\n",
    "        weights = [clf_gold[years_per_conf*idx + yr] for yr in offset_years[1984-min_year:]]\n",
    "        #scores.append(result_clf[years_per_conf*idx + offset_years[2014-min_year]])\n",
    "    \n",
    "        scores.append(np.max(np.array([w for w in weights])))\n",
    "        pagerankc.append(page_rank_confs[idx])\n",
    "        paperss.append(paper_counts_c[idx])\n",
    "    else:\n",
    "        print(conf)\n",
    "        scores.append(-1)\n",
    "        pagerankc.append(-1)\n",
    "        paperss.append(-1)\n",
    "    #print(scores[-1],weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_datas = []\n",
    "for row in faculty_affil.itertuples():\n",
    "    if row[1] in name_idx: #and row[1] not in seen_map:\n",
    "        idx = name_idx[row[1]]\n",
    "        seen_map[row[1]] = 1\n",
    "        new_data = {}\n",
    "        new_data['dblpname'] = row[1]\n",
    "        #new_data['index'] = row[1]\n",
    "        new_data['school'] = row[2].strip().rstrip()\n",
    "        new_data['venue_score'] = total_scores[idx]\n",
    "        new_data['pagerankA'] = page_rank_people[idx]\n",
    "        new_data['pagerankC'] = page_rank_scores[idx]\n",
    "        new_data['papers'] = paper_counts[idx] if total_scores[idx] > 0 else 0\n",
    "        new_data['authors'] = 1 if total_scores[idx] > 0 else 0\n",
    "        \n",
    "        new_datas.append(new_data)\n",
    "        #df_merged = df_merged.append([row[1],np.nan,np.nan,np.nan,row[2],row[1],(total_scores[idx]-tm)/ts])\n",
    "df_csr_to_add = pd.DataFrame(new_datas)\n",
    "#df_csr_to_add = df_csr_to_add.set_index('dblpname')\n",
    "df_csr_to_add = df_csr_to_add.set_index('dblpname')\n",
    "#df_merged = pd.concat([df_merged,df_csr_to_add])\n",
    "print(df_merged.shape)\n",
    "#print(faculty_affil.shape)\n",
    "df_t10 = df_merged[df_merged['t10-index'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_csr_to_add.sort_values('venue_score',0,False).itertuples():\n",
    "    print('{},{},{:.0f},{:.0f}'.format(row[0],row[-2],row[-1],row[-3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_school_df = df_csr_to_add.groupby('school').sum()\n",
    "print_school_df['size_normed'] = np.array(print_school_df.venue_score)/np.maximum(1,np.log(print_school_df.authors+1))\n",
    "print_school_df.sort_values('size_normed',0,False).to_csv('schools.csv')\n",
    "#total_scores[name_idx['Aaron Clauset']],total_scores[name_idx['Martial Hebert']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_vals = df_t10.groupby('school').aggregate('sum').sort_values('venue_score',0,False)\n",
    "t10_schools = school_vals.sort_values('t10-index',0,False)\n",
    "school_vals.corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_msar = pd.read_csv('other_ranks/traditional_conf_scores.csv')\n",
    "except:\n",
    "    from fuzzywuzzy import process, fuzz\n",
    "    with open('other_ranks/msar.json') as fp:\n",
    "        msar = json.load(fp)\n",
    "    df_msar = pd.DataFrame(msar)\n",
    "    dblp_conf_name = []\n",
    "    matchable_names = [fuzz._process_and_sort(n,False) for n in all_venues]\n",
    "    for row in df_msar.itertuples():\n",
    "        #print(row[2],row[-1])\n",
    "        try:\n",
    "            if row[-1] in conf_idx:\n",
    "                dblp_conf_name.append(row[-1])\n",
    "            elif row[2] in conf_idx:\n",
    "                dblp_conf_name.append(row[2])\n",
    "            elif len(row[2].split('/')) > 1:\n",
    "                found = False\n",
    "                for subname in row[2].split('/'):\n",
    "                    if found == False and subname in conf_idx:\n",
    "                        dblp_conf_name.append(subname)\n",
    "                        found = True\n",
    "                if found == False:\n",
    "                    raise\n",
    "            elif len(row[2].split('(')) > 1:\n",
    "                substr = row[2].split('(')\n",
    "                found = False\n",
    "                for subname in [substr[0],substr[1][:-1]]:\n",
    "                    if found == False and subname in conf_idx:\n",
    "                        dblp_conf_name.append(subname)\n",
    "                        found = True\n",
    "                if found == False:\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "        except:\n",
    "            bestshort,bestlong = None,None\n",
    "            if row[2] != None:\n",
    "                matcher = fuzz.SequenceMatcher(None, fuzz._process_and_sort(row[2],False))\n",
    "                n3s = []\n",
    "                for n2 in matchable_names:\n",
    "                    matcher.set_seq2(n2)\n",
    "                    n3s.append(matcher.ratio())\n",
    "                v=np.argmax(n3s)\n",
    "                bestshort = (all_venues[v],n3s[v])\n",
    "                #print(bestshort[1:],end='\\t')\n",
    "            if row[-1] != None:\n",
    "                matcher = fuzz.SequenceMatcher(None, fuzz._process_and_sort(row[-1],False))\n",
    "                n3s = []\n",
    "                for n2 in matchable_names:\n",
    "                    matcher.set_seq2(n2)\n",
    "                    n3s.append(matcher.ratio())\n",
    "                v=np.argmax(n3s)\n",
    "                bestlong = (all_venues[v],n3s[v])\n",
    "                #print(bestlong[1:],end='\\t')\n",
    "            if bestlong and bestlong[-1] > 0.96:\n",
    "                dblp_conf_name.append(bestlong[0])\n",
    "                #print(bestlong,row)\n",
    "            elif bestshort and bestshort[-1] > 0.96:\n",
    "                dblp_conf_name.append(bestshort[0])\n",
    "                #print(bestshort,row)\n",
    "            else:\n",
    "                #print(bestlong,bestshort,row[2],row[-1])\n",
    "                dblp_conf_name.append('NotAConf')\n",
    "    df_msar['dblp_name'] = dblp_conf_name\n",
    "    df_msar.to_csv('other_ranks/traditional_conf_scores.csv')\n",
    "    #ILPS/ISLP/NACLP/SLP\n",
    "    #DISC(WDAG)\n",
    "\n",
    "\n",
    "offset_years = [i//YEAR_BLOCKS for i in range(span_years)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_counts_c.shape\n",
    "counter = 0\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    if venue == 'Internet Measurement Workshop':\n",
    "        counter += 1\n",
    "counter,paper_counts_c[conf_idx['IMC']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msar_filt['venue_scores'] = scores\n",
    "df_msar_filt['pagerankc'] = pagerankc\n",
    "df_msar_filt['papers'] = paperss\n",
    "df_msar_filt['cp'] = df_msar_filt.citations/df_msar_filt.papers\n",
    "\n",
    "\n",
    "df_msar_filt = df_msar_filt[~df_msar_filt.dblp_name.duplicated()].copy()\n",
    "df_msar_filt = df_msar_filt[~df_msar_filt.venue_scores.duplicated()].copy()\n",
    "#df_msar_filt['h-approx'] = 0.54*np.sqrt(df_msar_filt.citations)\n",
    "\n",
    "df_msar_filt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msar_filt[df_msar_filt.category == 'Computer Vision'].sort_values('cp',0,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 15\n",
    "df_msar_filt = df_msar_filt[['papers','citations','H','pagerankc','cp','venue_scores']]\n",
    "print(df_msar_filt[df_msar_filt.H > thresh].shape)\n",
    "df_msar_filt[df_msar_filt.H > thresh].corr('spearman')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp2 = pd.read_csv('other_ranks/uni_rank_bp.csv')\n",
    "times = pd.read_csv('other_ranks/uni_rank_times.csv')\n",
    "\n",
    "srf2 = pd.read_csv('other_ranks/uni_rank_mergedscholar.csv')\n",
    "st2 = pd.read_csv('other_ranks/uni_rank_st.csv')\n",
    "qt2 = pd.read_csv('other_ranks/uni_rank_qt.csv')\n",
    "sr2 = pd.read_csv('other_ranks/uni_rank_sr.csv')\n",
    "pr2 = pd.read_csv('other_ranks/uni_rank_pr.csv')\n",
    "cm2 = pd.read_csv('other_ranks/uni_rank_cs.csv')\n",
    "usn2 = pd.read_csv('other_ranks/uni_rank_usn.csv')\n",
    "df_csr = pd.read_csv('other_ranks/ranks.csv')\n",
    "\n",
    "pr2.USN2010 = pr2.USN2010.map(lambda x: int(x) if x.isnumeric() else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    [(row[2],row[1]) for row in st2.itertuples()],\n",
    "    [(row[2],row[1]) for row in times.itertuples()],\n",
    "    [(row[2],row[1]) for row in qt2.itertuples()],\n",
    "    [(row[6],row[1]) for row in pr2.itertuples()],\n",
    "    [(row[2],row[1]) for row in cm2.itertuples()],\n",
    "    [(row[2],row[1]) for row in sr2.itertuples()],\n",
    "    [(row[-1],row[2]) for row in srf2.itertuples()],\n",
    "\n",
    "    [(row[0],idx+1) for idx,row in enumerate(t10_schools.itertuples())],\n",
    "    [(row[2],row[1]) for row in df_csr.itertuples()],\n",
    "    [(row[0],idx+1) for idx,row in enumerate(school_vals.itertuples())],\n",
    "    [(row[0],idx+1) for idx,row in enumerate(school_vals.sort_values('pagerankA',0,False).itertuples())],\n",
    "    [(row[0],idx+1) for idx,row in enumerate(school_vals.sort_values('pagerankC',0,False).itertuples())],\n",
    "    [(row[0],idx+1) for idx,row in enumerate(school_vals.sort_values('Citations',0,False).itertuples())],\n",
    "    [(row[0],idx+1) for idx,row in enumerate(school_vals.sort_values('papers',0,False).itertuples())],\n",
    "\n",
    "    [(row[2],row[1]) for row in bp2.itertuples()],\n",
    "    [(row[6],row[4]) for row in pr2.sort_values('NRC95',ascending=True).itertuples() ],\n",
    "    [(row[6],row[3]) for row in pr2.sort_values('USN2010',ascending=True).itertuples() if np.isfinite(row[3]) ],\n",
    "    [(row[2],row[1]) for row in usn2.itertuples()]\n",
    "]\n",
    "dataset_names = ['Shanghai','Times','QS','Prestige','CSMetrics',\n",
    "                 'ScholarRank','ScholarRankFull','t10Sum','CSRankings','Mine','pageranka','pagerankc','citations','papers','BestPaper','NRC95',\"USN10\",'USN18']\n",
    "n_datasets = len(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#school_vals.to_csv('test3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.zeros((n_datasets,n_datasets))\n",
    "count_matrix = np.zeros((n_datasets,n_datasets))\n",
    "name_datasets = [ [v[0] for v in d] for d in datasets ]\n",
    "all_names = sorted(list(set(sum(name_datasets,[]))))\n",
    "all_vec = [sum([name in d for d in name_datasets])>=(len(datasets)-4) for name in all_names]\n",
    "subset_names = [name for name,vec in zip(all_names,all_vec) if vec]\n",
    "subset_names,len(subset_names)\n",
    "import scipy.stats as stats\n",
    "for i in range(n_datasets):\n",
    "    inames = [u[0] for u in datasets[i]]\n",
    "    for j in range(i,n_datasets):\n",
    "        try:\n",
    "            jnames = [u[0] for u in datasets[j]]\n",
    "\n",
    "            #exist_1 = [((ni in subset_names) and (ni in jnames)) for ni in inames]\n",
    "            #exist_2 = [((nj in subset_names) and (nj in inames))for nj in jnames]\n",
    "            exist_1 = [((True) and (ni in jnames)) for ni in inames]\n",
    "            exist_2 = [((True) and (nj in inames))for nj in jnames]\n",
    "\n",
    "            d1 = np.array(datasets[i])[exist_1]\n",
    "            d2 = np.array(datasets[j])[exist_2]\n",
    "            v1 = d1[:,1].astype(np.float)\n",
    "            v2 = np.array([d2[np.where(d2[:,0] == name)[0][0],1] for name in d1[:,0]]).astype(np.float)\n",
    "            c = stats.spearmanr(v1,v2)[0]\n",
    "            corr_matrix[i][j] = c\n",
    "            corr_matrix[j][i] = c\n",
    "            count_matrix[i][j] = len(v1)\n",
    "            count_matrix[j][i] = len(v2)\n",
    "        except:\n",
    "            print(len(v1),len(v2))\n",
    "        #print(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "print(corr_matrix)\n",
    "print('mean best')\n",
    "for s,n in sorted([(s,n) for n,s in zip(dataset_names,corr_matrix.mean(1))],reverse=True):\n",
    "    print('{:30s}\\t{:.3f}'.format(n,s))\n",
    "print('\\n usnews best')\n",
    "for s,n in sorted([(s,n) for n,s in zip(dataset_names,corr_matrix[-1])],reverse=True):\n",
    "    print('{:30s}\\t{:.3f}'.format(n,s))\n",
    "print('\\n names')\n",
    "\n",
    "for n in dataset_names:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict\n",
    "def di():\n",
    "    return defaultdict(float)\n",
    "author_by_year = defaultdict(di)\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    for a in authors:\n",
    "        author_by_year[a][year] += clf_gold[years_per_conf*conf_idx[venue] + offset_years[year-min_year]]/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import matplotlib.gridspec as gridspec\n",
    "gs = gridspec.GridSpec(4, 4)\n",
    "plt.figure(figsize=(12,12))\n",
    "#example_names = ['Takeo Kanade','Martial Hebert','Christopher G. Atkeson','Howie Choset','Deva Ramanan','Jessica K. Hodgins'] #,'Pieter Abbeel'\n",
    "#Pearl, Guibas, Karp, Kanade, Jordan, Woodruff\n",
    "example_names = ['Judea Pearl','Leonidas J. Guibas','Richard M. Karp','Takeo Kanade','Michael I. Jordan','Elisa Bertino']\n",
    "#example_names = ['Christopher Scaffidi','Humphrey Hu','Keenan Crane']\n",
    "#example_names = ['Takeo Kanade','Michael I. Jordan','Richard M. Karp','Leonidas J. Guibas','David P. Woodruff', 'Judea Pearl']\n",
    "#example_names = ['Christopher G. Atkeson','Oliver Kroemer','Jeannette Bohg','Mark R. Cutkosky','Doug L. James','Allison M. Okamura','Edward H. Adelson','Pulkit Agrawal','Phillip Isola','Alberto Rodriguez','Russ Tedrake']\n",
    "#example_names = ['Dorsa Sadigh','Chelsea Finn','Oliver Kroemer','Jeannette Bohg','Phillip Isola','Alberto Rodriguez']\n",
    "#example_names = ['Christopher G. Atkeson','Mark R. Cutkosky','Doug L. James','Allison M. Okamura','Edward H. Adelson','Russ Tedrake']\n",
    "#example_names = ['Martial Hebert','Jessica K. Hodgins','Christopher G. Atkeson','Katia P. Sycara','Deva Ramanan','Pieter Abbeel']\n",
    "ri_names = list(set([aliasdict.get(row[1],row[1]) for row in pd.read_csv('other_ranks/cmu_faculty.csv').itertuples() if row[2] != 'c']))\n",
    "#example_names = [_[1] for _ in sorted([(total_scores[name_idx[i]],i,year_span[name_idx[i]]) for i in ri_names if i in name_idx],reverse=True)[:6]]\n",
    "plt.subplot(gs[2:4, 1:3])\n",
    "\n",
    "for example_name in example_names:\n",
    "    example_value = np.zeros(max_year+1-min_year)\n",
    "    years = author_by_year[example_name]\n",
    "    yrs = [_ for _ in years.keys() if _  > 0]\n",
    "    start_year = min(yrs)\n",
    "    end_year = max(yrs)\n",
    "    span = end_year - start_year\n",
    "    start_year,end_year,span\n",
    "    for y,v in years.items():\n",
    "        example_value[y-min_year] += v\n",
    "            \n",
    "    plt.plot(np.arange(min_year,max_year+1)[:-1],gaussian_filter1d(example_value[:-1], sigma=4.5),label=example_name,lw=4)\n",
    "    #plt.plot(gaussian_filter1d(example_value[:span], sigma=2),label=example_name )\n",
    "\n",
    "#plt.plot((val_by_year/v_count),label='average author')\n",
    "plt.ylabel('annual value (4.5 sigma smoothing)')\n",
    "#plt.xlabel('year since first publication')\n",
    "plt.xlabel('year',fontsize=18)\n",
    "plt.title('venue scores')\n",
    "plt.legend(bbox_to_anchor=(0.5, 1.4),loc=9,fancybox=True, framealpha=0.8, borderpad=1,frameon=True,ncol=3,fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.subplot(gs[:2, :2])\n",
    "for example_name in example_names:\n",
    "    example_value = np.zeros(max_year+1-min_year)\n",
    "    for yr in range(min_year,max_year+1):\n",
    "        if (example_name,yr) in paper_counts_year:\n",
    "            example_value[yr-min_year] += paper_counts_year[example_name,yr]\n",
    "    plt.plot(np.arange(min_year,max_year+1)[:-1],gaussian_filter1d(example_value[:-1], sigma=4.5),label=example_name,lw=4)\n",
    "    #plt.plot(gaussian_filter1d(example_value[:span], sigma=2),label=example_name )\n",
    "\n",
    "#plt.plot((val_by_year/v_count),label='average author')\n",
    "plt.ylabel('annual value (4.5 sigma smoothing)')\n",
    "#plt.xlabel('year since first publication')\n",
    "plt.xlabel('year',fontsize=18)\n",
    "plt.title('paper count')\n",
    "#plt.legend(loc=4,fancybox=True, framealpha=0.8, borderpad=1,frameon=True,ncol=2,fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.subplot(gs[:2, 2:])\n",
    "\n",
    "for example_name in example_names:\n",
    "    example_value = np.zeros(max_year+1-min_year)\n",
    "    for yr in range(min_year,max_year+1):\n",
    "        if (example_name,yr) in paper_counts_year:\n",
    "            example_value[yr-min_year] += paper_counts_year[example_name,yr]\n",
    "    plt.plot(np.arange(min_year,max_year+1)[:-1],1e4*gaussian_filter1d((example_value/paper_counts_year_counter)[:-1], sigma=4.5),label=example_name,lw=4)\n",
    "    #plt.plot(gaussian_filter1d(example_value[:span], sigma=2),label=example_name )\n",
    "\n",
    "#plt.plot((val_by_year/v_count),label='average author')\n",
    "plt.ylabel('annual value (4.5 sigma smoothing)')\n",
    "#plt.xlabel('year since first publication')\n",
    "plt.xlabel('year',fontsize=18)\n",
    "plt.title('paper count (norm by annual paper totals)')\n",
    "#plt.legend(fancybox=True, framealpha=0.8, borderpad=1,frameon=True,ncol=2,fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.savefig('fame4.pdf',facecolor='w',edgecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_by_year = np.zeros(50)\n",
    "v_count = np.zeros(50)\n",
    "for a in author_by_year:\n",
    "    vector = author_by_year[a]\n",
    "    yrs = vector.keys()\n",
    "    start_year_a = min(yrs)\n",
    "    end_year_a = max(yrs)\n",
    "    span = end_year_a - start_year_a\n",
    "    if span < 85:\n",
    "        for y in vector:\n",
    "            v_count[y-start_year_a] += 1\n",
    "            val_by_year[y-start_year_a] += vector[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot((val_by_year/v_count)[:-2])\n",
    "plt.title('Aging Curve for Authors')\n",
    "plt.xlabel('years since first publication')\n",
    "plt.ylabel('average annual value generated')\n",
    "plt.xlim(0,50)\n",
    "#plt.ylim(0,2)\n",
    "plt.ylim(bottom=0)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('age_curve.pdf',facecolor='w',edgecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_values = {k: sum(v.values()) for k,v in author_by_year.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_value_list = sorted([(v,k) for k,v in total_values.items()],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_value_list[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curious_names = [\n",
    "    'Aditya Dhawale',\n",
    "    'Tesca Fitzgerald',\n",
    "    'Adam W. Harley',\n",
    "    \"Xiaolong Wang 0004\",\n",
    "    \"Judy Hoffman\",\n",
    "    \"Paris Siminelakis\",\n",
    "    \"Roie Levin\",\n",
    "    \"Leonid Keselman\",\n",
    "    \"Rick Goldstein\",\n",
    "    \"Nicholas Rhinehart\",\n",
    "    \"Vincent Sitzmann\",\n",
    "    \"Siddharth Ancha\",\n",
    "    \"Xingyu Lin\",\n",
    "    \"Humphrey Hu\",\n",
    "    \"David F. Fouhey\",\n",
    "    \"Chelsea Finn\",\n",
    "    \"Dinesh Jayaraman\",\n",
    "    \"Wen Sun 0002\",\n",
    "    \"Lerrel Pinto\",\n",
    "    \"Justin Johnson\",\n",
    "    \"Amir Roshan Zamir\",\n",
    "    \"Dominik Peters\",\n",
    "    \"Jonathan T. Barron\",\n",
    "    \"Dorsa Sadigh\",\n",
    "    \"Derek Hoiem\",\n",
    "    \"Vaggos Chatziafratis\",\n",
    "    \"Brian Okorn\",\n",
    "    \"David Held\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri_names = list(set([aliasdict.get(row[1],row[1]) for row in pd.read_csv('other_ranks/cmu_faculty.csv').itertuples() if row[2] == 'RI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_names = np.array(['Artur Dubrawski','Artur W. Dubrawski'])\n",
    "uni_names = list(uni_names)\n",
    "cmu_scores = []\n",
    "for name in uni_names:\n",
    "    if name in name_idx:\n",
    "        score = total_scores[name_idx[name]]\n",
    "        cmu_scores.append(((score-tm)/ts,name))\n",
    "for s,p in sorted(cmu_scores,reverse=True):\n",
    "    print('{:30s}\\t\\t{:.3f}'.format(p,s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v,k in sorted([(v,k) for k,v in conf_counts.items() ],reverse=True):\n",
    "    print('{:30s}\\t{}'.format(k[:30],round(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_conf_counts = {}\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    if year < 2004:\n",
    "        continue\n",
    "    for a in authors:\n",
    "        if a in uni_names:\n",
    "            prof_conf_counts[venue] = [a] + prof_conf_counts.get(venue,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs_of_interest =[]# ['CVPR','ICCV','ECCV']\n",
    "for conf_of_interest in confs_of_interest:\n",
    "    unique_set = {k: 0 for k in set(prof_conf_counts[conf_of_interest])}\n",
    "    for k in prof_conf_counts[conf_of_interest]:\n",
    "        unique_set[k] += 1\n",
    "    for v,k in sorted([(v,k) for k,v in unique_set.items() ],reverse=True):\n",
    "        print('{:30s}\\t{}'.format(k[:30],round(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_confs = [k for k,v in conf_counts.items() if v > 1]\n",
    "faculty_lookup = {_[1]:_[2] for _ in faculty_affil.itertuples()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_confs = np.zeros_like(clf_gold) \n",
    "for conf in good_confs:\n",
    "    filter_confs[conf_idx[conf]*years_per_conf:(conf_idx[conf]+1)*years_per_conf] = 1\n",
    "year_span = (auth_years[:,1] - auth_years[:,0]) + 1\n",
    "ri_scores_total = (Xauth.dot(clf_gold * filter_confs))\n",
    "ri_scores = ri_scores_total * (auth_years[:,0] >= 2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ri_confs = [_ for _ in np.argsort(clf_gold * filter_confs)[::-1] if filter_confs[_]]\n",
    "for idx in best_ri_confs:\n",
    "    year = idx%years_per_conf\n",
    "    if (year+1970) < 2016:\n",
    "        continue\n",
    "    print('{:45s}\\t{:.2f}\\t{}'.format(all_venues[idx//years_per_conf][:35],clf_gold[idx],year+1970))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ri = np.argsort(total_scores)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,idx in enumerate(best_ri[:500]):\n",
    "    name = all_authors[idx]\n",
    "    uni = faculty_lookup[name] if name in faculty_lookup else 'None'\n",
    "    if uni != 'None':\n",
    "        continue\n",
    "    print('{},{},{:.2f}'.format(all_authors[idx],uni,total_scores[idx]))\n",
    "    #print('{}\\t\\t{:30s}\\t{:.2f}\\t\\t{}\\t{}'.format(i,all_authors[idx],ri_scores[idx],uni,int(auth_years[idx,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "things2 = []\n",
    "for idx in np.argsort(total_scores)[::-1][:15000]:\n",
    "    name = all_authors[idx]\n",
    "    uni = faculty_lookup[name] if name in faculty_lookup else 'None'\n",
    "    #if uni == 'None':\n",
    "    #    continue\n",
    "    #if auth_years[idx,1] < 2010:\n",
    "    #    continue\n",
    "    i+=1\n",
    "    things2.append((all_authors[idx],total_scores[idx],uni,auth_years[idx,1]-auth_years[idx,0]))\n",
    "    print('{}\\t\\t{:30s}\\t{:.2f}\\t\\t{}\\t{}'.format(i,all_authors[idx],total_scores[idx],uni,int(auth_years[idx,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(things2,columns=['Name','Score','Uni','Years']).set_index('Name').to_csv('authors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(total_scores[name_idx[i]]/year_span[name_idx[i]],i,year_span[name_idx[i]]) for i in ri_names],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(total_scores[name_idx[i]],i,year_span[name_idx[i]]) for i in ri_names],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(total_scores[name_idx[i]],i,year_span[name_idx[i]]) for i in curious_names],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_idx[\"Joshua B. Tenenbaum\"],name_idx['Matthias Nießner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_paper_year = 2013\n",
    "min_pub_years = 2\n",
    "cand_total = total_scores * (auth_years[:,0] >= first_paper_year).astype(np.float)* (year_span >= min_pub_years).astype(np.float) \n",
    "\n",
    "cand_ri = ri_scores* (auth_years[:,0] >= first_paper_year).astype(np.float) * (year_span >= min_pub_years).astype(np.float) \n",
    "\n",
    "\n",
    "cand_total_ef = cand_total/year_span\n",
    "cand_ri_ef = cand_ri/year_span\n",
    "\n",
    "print('{}\\t{:30s}\\t{:s}\\t{:s}\\t{:s}\\t{:s}\\t{:s}\\t{:20s}'.format('Rank','Author',\n",
    "                                                      'RI','Total',\n",
    "                                                      'eRI','eTotal',\n",
    "                                                      'Since','Affiliation'))\n",
    "for num,idx in enumerate((np.argsort(cand_ri)[::-1])[:100]):\n",
    "    uni = faculty_lookup.get(all_authors[idx],'None')\n",
    "\n",
    "    print('{}\\t{:30s}\\t{:.1f}\\t{:.1f}\\t{:.1f}\\t{:.1f}\\t{:.0f}\\t{:20s}'.format(num+1,all_authors[idx],\n",
    "                                                          cand_ri[idx],cand_total[idx],\n",
    "                                                          cand_ri_ef[idx],cand_total_ef[idx],\n",
    "                                                          auth_years[idx,0],uni))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_papers = np.zeros(years_per_conf*n_confs)\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    yr = (year-min_year)//YEAR_BLOCKS\n",
    "    j = years_per_conf*conf_idx[venue] + yr\n",
    "    count_of_papers[j] += 1\n",
    "# safe divide\n",
    "count_of_papers = np.maximum(1,count_of_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_papers2 = count_of_papers.reshape((-1,years_per_conf))\n",
    "masked_count = (count_of_papers2 * (count_of_papers2 > 1))\n",
    "how_many_valid_years = (masked_count > 1).sum(1)\n",
    "conf_sizes = masked_count.sum(1)/how_many_valid_years\n",
    "valid_confs_for_vis = (np.isfinite(conf_sizes)) & (conf_sizes > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_confs_for_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_scores = clf_gold.reshape((-1,years_per_conf))[:,-6:-1].copy()\n",
    "ranks = np.nan_to_num(np.true_divide(tmp_scores.sum(1),(tmp_scores!=0).sum(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_names = {'Theory of Computing, Graduate Surveys' : 'Theory of Computing, GS',\n",
    "               'IEEE Symposium on Security and Privacy':'IEEE SSP',\n",
    "              'IEEE Conference on Computational Complexity': 'IEEE CCC',\n",
    "              'USENIX Annual Technical Conference': 'USENIX ATC',\n",
    "              'Journal of Machine Learning Research': 'JMLR',\n",
    "              'Computer Communication Review' : 'SIGCOMM CCR'}\n",
    "things = []\n",
    "for idx in np.argsort(ranks)[::-1]:#[:500]:\n",
    "    name = short_names[all_venues[idx]] if all_venues[idx] in short_names else all_venues[idx]\n",
    "    paper_count = conf_sizes[idx]\n",
    "    if not valid_confs_for_vis[idx]:\n",
    "        continue\n",
    "    if ranks[idx] < 0.1:\n",
    "        continue\n",
    "    things.append((name,ranks[idx],conf_sizes[idx]))\n",
    "    print('{:30s}\\t{:.2f}\\t{:.1f}'.format(name,ranks[idx],conf_sizes[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(things,columns=['Name','Score','Size']).set_index('Name').to_csv('confs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks[conf_idx['STOC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs cluster_new to be run. sorry\n",
    "labels = np.load('labels.npy')\n",
    "mask = np.load('mask.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs cluster_new to be run. sorry\n",
    "cluster_ranks = clf_gold.reshape((-1,years_per_conf))[mask,:]\n",
    "small_names = np.array([all_venues[i] for i in range(n_confs) if mask[i]])\n",
    "alias_dict = csv2dict_str_str('name_alias.csv')\n",
    "group_names = csv2dict_str_str('lookups.csv')\n",
    "group_names = {k:v for k,v in group_names.items() if v != ''}\n",
    "valid_confs_for_small_vis = valid_confs_for_vis[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = labels.max() + 1\n",
    "small_conf_counts = np.zeros(NUM_CLUSTERS)\n",
    "\n",
    "for i in range(32):\n",
    "    cluster_names = small_names[labels == i]\n",
    "    for conf in cluster_names:\n",
    "        small_conf_counts[i] += count_of_papers[years_per_conf*conf_idx[conf]:years_per_conf*(1+conf_idx[conf])].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(32,32))\n",
    "names_to_save = []\n",
    "\n",
    "for cnti, i in enumerate(np.argsort(small_conf_counts)[::-1]):\n",
    "    plt.subplot(8,4,cnti+1)\n",
    "    cluster_names = small_names[labels == i]\n",
    "    cluster_filtered = cluster_ranks[labels == i] * valid_confs_for_small_vis[labels == i].reshape((-1,1))\n",
    "    \n",
    "    cluster_scores = cluster_filtered[:,-10:].max(1)\n",
    "    valid_idxs = np.argsort(cluster_filtered[:,-4:-1].mean(1))[::-1]\n",
    "    for idx in valid_idxs[:6]:\n",
    "        name_for_now = cluster_names[idx]\n",
    "        if name_for_now in group_names:\n",
    "            plt.title(group_names[name_for_now])\n",
    "            #print(group_names[name_for_now],cnti,small_conf_counts[i])\n",
    "        name_for_now = alias_dict[name_for_now] if name_for_now in alias_dict else name_for_now\n",
    "        line = [np.nan if _ == 0 else _ for _ in cluster_filtered[idx]]\n",
    "        plt.plot(min_year+np.arange(years_per_conf-1)*YEAR_BLOCKS,line[:-1],label=name_for_now)\n",
    "        names_to_save.append(cluster_names[idx])\n",
    "        #print(name_for_now,i)\n",
    "    plt.legend(loc=3,fancybox=True, framealpha=0.8, borderpad=1,frameon=True)\n",
    "    plt.tight_layout()\n",
    "    #plt.ylim(0,13)\n",
    "    plt.ylim(bottom=0.05)\n",
    "    plt.xlim(1970,2020)\n",
    "plt.savefig('big_plot4.pdf',facecolor='w',edgecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    plt.figure(figsize=(24,20))\n",
    "    names_to_save = []\n",
    "\n",
    "    #cluster_labels = ['Networks','Systems','Crypto','Robotics','Graphics','Security','Networks','Software Eng','Control','Arch','Database','Computer Vision','Design Automation','Mobile','Data Mining','Embedded','Programming Languages','Real-Time','NLP','BioMed','Comp Bio','IR','Multimedia','Communication','Theory','HCI','HPC','Econ','Computational Geometry','Machine Learning']\n",
    "    for i in range(24):\n",
    "\n",
    "        plt.subplot(6,4,i+1)\n",
    "        cluster_names = conf_set[i]\n",
    "        filt = np.zeros(cluster_ranks.shape[0]).astype(np.bool)\n",
    "        for conf in conf_set[i]:\n",
    "            if conf in small_names:\n",
    "                filt[np.where(small_names == conf)[0][0]] = True\n",
    "        cluster_filtered = cluster_ranks[filt]\n",
    "        cluster_scores = cluster_filtered[:,-10:].max(1)\n",
    "        valid_idxs = np.argsort(cluster_filtered[:,-7:-2].mean(1))[::-1][:3]\n",
    "        print(cluster_names,len(valid_idxs),cluster_scores)\n",
    "        for idx in valid_idxs:\n",
    "            name_for_now = conf_set[i][idx]\n",
    "            print(name_for_now,cluster_scores[idx])\n",
    "            if name_for_now in group_names:\n",
    "                plt.title(group_names[name_for_now])\n",
    "            name_for_now = alias_dict[name_for_now] if name_for_now in alias_dict else name_for_now\n",
    "            plt.plot(min_year+np.arange(years_per_conf-1)*YEAR_BLOCKS,cluster_filtered[idx][:-1],label=name_for_now)\n",
    "            names_to_save.append(cluster_names[idx])\n",
    "\n",
    "        plt.legend(loc=2,fancybox=True, framealpha=0.8, borderpad=1,frameon=True)\n",
    "        plt.tight_layout()\n",
    "        plt.ylim(0,13)\n",
    "        plt.xlim(1970,2020)\n",
    "    plt.savefig('big_plot2.pdf',facecolor='w',edgecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in names_to_save:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
