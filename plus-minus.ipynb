{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import csv\n",
    "import scipy.sparse\n",
    "Xauth = None\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the update to work despite the broken scipy documentation\n",
    "try:\n",
    "    a = scipy.sparse.dok_matrix((10,10))\n",
    "    a.update({(0,0):1.0})\n",
    "    scipy.sparse.dok_matrix.my_update = scipy.sparse.dok_matrix.update\n",
    "except:\n",
    "    a = scipy.sparse.dok_matrix((10,10))\n",
    "    a._update({(0,0):1.0})\n",
    "    scipy.sparse.dok_matrix.my_update = scipy.sparse.dok_matrix._update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('useful_venue_list.pkl.gz','rb') as fp:\n",
    "    all_venues = pickle.load(fp)\n",
    "with gzip.open('useful_authors_list.pkl.gz','rb') as fp:\n",
    "    all_authors = pickle.load(fp)\n",
    "with gzip.open('useful_papers.pkl.gz','rb') as fp:\n",
    "    all_papers = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year = all_papers[0][6]\n",
    "max_year = all_papers[-1][6]\n",
    "span_years = max_year - min_year + 1\n",
    "print(min_year,max_year,span_years)\n",
    "conf_idx = {v:i for i,v in enumerate(all_venues)}\n",
    "name_idx = {v:i for i,v in enumerate(all_authors)}\n",
    "n_confs = len(all_venues)\n",
    "n_auths = len(all_authors)\n",
    "n_papers = len(all_papers)\n",
    "print(n_confs,n_auths,n_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  np.load('clf_gold.pkl.npy')\n",
    "years_per_conf = clf.shape[0]//n_confs\n",
    "YEAR_BLOCKS = span_years//years_per_conf\n",
    "clf[2323]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import gc\n",
    "if Xauth is None:\n",
    "    valid_ns = set()\n",
    "    for paper in all_papers:\n",
    "        tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "        n = len(authors)\n",
    "        valid_ns.add(n)\n",
    "    \n",
    "    per_author_val = {}\n",
    "    for n in valid_ns:\n",
    "        author_scores = 1/(np.arange(n)+1)\n",
    "        per_author_val[n] = author_scores/author_scores.sum()\n",
    "        \n",
    "    count_vecs = {}\n",
    "    paper_vecs = []\n",
    "    for paper in all_papers:\n",
    "        tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "        n = len(authors)\n",
    "        j = years_per_conf*conf_idx[venue] + (year-min_year)//YEAR_BLOCKS\n",
    "\n",
    "        author_scores = per_author_val[n]\n",
    "        paper_vecs.append([(name_idx[a],j,v) for a,v in zip(authors,author_scores)])\n",
    "        \n",
    "    Xauth = scipy.sparse.dok_matrix((n_auths,years_per_conf*n_confs))\n",
    "    xdict = {}\n",
    "  \n",
    "    for paper_vec in paper_vecs:\n",
    "        for i,j,v in paper_vec:\n",
    "            xdict[(i,j)] = v + xdict.get((i,j),0)\n",
    "\n",
    "    Xauth.my_update(xdict)\n",
    "            \n",
    "    Xauth = scipy.sparse.csr_matrix(Xauth)\n",
    "    paper_vec = []\n",
    "    xdict = {}\n",
    "    gc.collect()\n",
    "    auth_years = np.ones((n_auths,2)) * np.array([3000,1000]) \n",
    "    for paper in all_papers:\n",
    "        tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "        for a in authors:\n",
    "            i = name_idx[a]\n",
    "            auth_years[i,0] = min(auth_years[i,0],year)\n",
    "            auth_years[i,1] = max(auth_years[i,1],year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_uni = pd.read_csv('other_ranks/cmu_faculty.csv')\n",
    "cmu_uni = cmu_uni.fillna('Other')\n",
    "cmu_uni = cmu_uni[cmu_uni.dept == 'RI']\n",
    "#uni_names = ['Andrea Tagliasacchi','Paul G. Kry']#['Xuemin Shen','H. Vincent Poor','Kang G. Shin','Mohamed-Slim Alouini','Lajos Hanzo']#list(cmu_uni.name)\n",
    "#uni_names = list(faculty_affil[faculty_affil.affiliation == 'Johns Hopkins University'].name)\n",
    "uni_names = list(cmu_uni.name)\n",
    "print(len(uni_names))\n",
    "conf_counts = {}\n",
    "conf_counts_value = {}\n",
    "\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    if year < 2000:\n",
    "        continue\n",
    "    n = len(authors)\n",
    "    for a in authors:\n",
    "        if a in uni_names:\n",
    "            conf_counts[venue] = 1/n + conf_counts.get(venue,0)\n",
    "            conf_counts_value[venue] = clf[years_per_conf*(conf_idx[venue]) + (year-min_year)//YEAR_BLOCKS]/n + conf_counts_value.get(venue,0)\n",
    "conf_counts_value = {k: v/conf_counts[k] for k,v in conf_counts_value.items()}\n",
    "ri_fav_confs = [(conf_counts[_[1]]*conf_counts_value[_[1]],_[1],conf_counts[_[1]],conf_counts_value[_[1]]) for _ in sorted([(v,k) for k,v in conf_counts.items() if v > 0],reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs_to_filter = [_[1] for _ in sorted(ri_fav_confs,reverse=True) if _[-2] >= 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_after = 1970\n",
    "start_year_idx = max(0,only_after-min_year)//YEAR_BLOCKS\n",
    "\n",
    "year_filter = np.zeros_like(clf).reshape((-1,years_per_conf))\n",
    "\n",
    "if False: # filter confs\n",
    "    for conf in confs_to_filter:\n",
    "        year_filter[conf_idx[conf],start_year_idx:] = 1\n",
    "else:\n",
    "    year_filter[:,start_year_idx:] = 1\n",
    "total_scores = Xauth.dot(clf * year_filter.reshape((-1)))\n",
    "\n",
    "\n",
    "year_filter = np.zeros_like(clf).reshape((-1,years_per_conf))\n",
    "\n",
    "for conf in confs_to_filter:\n",
    "    year_filter[conf_idx[conf],start_year_idx:] = 1\n",
    "ri_scores = Xauth.dot(clf * year_filter.reshape((-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argsort(total_scores)[::-1]\n",
    "for k in range(10):\n",
    "    idx = best_idx[k]\n",
    "    print('{:30s}\\t{:.2f}'.format(all_authors[idx],total_scores[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the APM matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xauth = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xauth = scipy.sparse.dok_matrix((n_papers,n_auths))\n",
    "y = np.zeros(n_papers)\n",
    "xdict = {}\n",
    "\n",
    "for i,paper in enumerate(all_papers):\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    for a in authors:\n",
    "        j = name_idx[a]\n",
    "        xdict[(i,j)] = 1/n\n",
    "    ji = years_per_conf*conf_idx[venue] + (year-min_year)//YEAR_BLOCKS\n",
    "    y[i] = clf[ji]\n",
    "\n",
    "Xauth.my_update(xdict)\n",
    "\n",
    "Xauth = scipy.sparse.csr_matrix(Xauth)\n",
    "paper_vec = []\n",
    "xdict = {}\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(y,50)\n",
    "plt.figure()\n",
    "_ = plt.hist((y-y.mean())/y.std(),50)\n",
    "y.std(),y.mean()\n",
    "plt.figure()\n",
    "yt = np.copy(y)\n",
    "where = np.where(y > 0)\n",
    "yt2 = np.log(yt[where])\n",
    "yt[where] = (yt2-yt2.mean())/yt2.std()\n",
    "yt = np.clip(yt,-3.5,3.5)\n",
    "_ = plt.hist(yt,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FI = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "clf2 = SGDRegressor('huber',alpha=1e-3,penalty='l2',tol=1e-6,max_iter=1000,average=True,verbose=1,fit_intercept=FI)\n",
    "#clf2.fit(Xauth,yt)\n",
    "clf2.fit(Xauth,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apm_weights = clf2.coef_\n",
    "apm_weights = (apm_weights-apm_weights.mean())/apm_weights.std()\n",
    "_ = plt.hist(apm_weights,100,log=True)\n",
    "print((clf2.intercept_-apm_weights.mean())/apm_weights.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cumulative, pairwise apm\n",
    "Some questions I have\n",
    "* Should you get 1 and 0.5 as the coefficients?\n",
    "* Should you get per_paper or total credit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "pairwise_authors = {}\n",
    "\n",
    "for i,paper in enumerate(all_papers):\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    authors = sorted(authors)\n",
    "    for ap in itertools.combinations(authors, 2):\n",
    "        if not ap in pairwise_authors:\n",
    "            pairwise_authors[ap] = len(pairwise_authors)\n",
    "n_pairwise_authors = len(pairwise_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xauth = scipy.sparse.dok_matrix((n_auths+n_pairwise_authors,n_auths))\n",
    "y = np.zeros(n_auths+n_pairwise_authors)\n",
    "xdict = {}\n",
    "\n",
    "for i,paper in enumerate(all_papers):\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    ji = years_per_conf*conf_idx[venue] + (year-min_year)//YEAR_BLOCKS\n",
    "    per_person_credit = clf[ji]/n\n",
    "    for a in authors:\n",
    "        j = name_idx[a]\n",
    "        xdict[(j,j)] = 1\n",
    "        y[j] += per_person_credit\n",
    "    \n",
    "    authors_s = sorted(authors)\n",
    "    for ap in itertools.combinations(authors_s, 2):\n",
    "        j2 = pairwise_authors[ap] + n_auths\n",
    "        a1 = name_idx[ap[0]]\n",
    "        a2 = name_idx[ap[1]]\n",
    "        xdict[(j2,a1)] = 0.5\n",
    "        xdict[(j2,a2)] = 0.5\n",
    "        y[j2] += per_person_credit\n",
    "\n",
    "\n",
    "Xauth.my_update(xdict)\n",
    "\n",
    "Xauth = scipy.sparse.csr_matrix(Xauth)\n",
    "paper_vec = []\n",
    "xdict = {}\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "clf3 = SGDRegressor('huber',alpha=1e-3,penalty='l2',tol=1e-6,max_iter=1000,average=True,verbose=1,fit_intercept=FI)\n",
    "clf3.fit(Xauth,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_apm_weights = clf3.coef_\n",
    "pw_apm_weights = (pw_apm_weights-pw_apm_weights.mean())/pw_apm_weights.std()\n",
    "_ = plt.hist(pw_apm_weights,100,log=True)\n",
    "print((clf3.intercept_-pw_apm_weights.mean())/pw_apm_weights.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xauth = scipy.sparse.dok_matrix((n_auths+n_pairwise_authors,n_auths))\n",
    "y = np.zeros(n_auths+n_pairwise_authors)\n",
    "xdict = {}\n",
    "\n",
    "for i,paper in enumerate(all_papers):\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    ji = years_per_conf*conf_idx[venue] + (year-min_year)//YEAR_BLOCKS\n",
    "    per_person_credit = clf[ji]/n\n",
    "    for a in authors:\n",
    "        j = name_idx[a]\n",
    "        xdict[(j,j)] = 1 + xdict.get((j,j),0)\n",
    "        y[j] += per_person_credit\n",
    "    \n",
    "    authors_s = sorted(authors)\n",
    "    for ap in itertools.combinations(authors_s, 2):\n",
    "        j2 = pairwise_authors[ap] + n_auths\n",
    "        a1 = name_idx[ap[0]]\n",
    "        a2 = name_idx[ap[1]]\n",
    "        xdict[(j2,a1)] = 0.5 + xdict.get((j2,a1),0)\n",
    "        xdict[(j2,a2)] = 0.5 + xdict.get((j2,a2),0)\n",
    "        y[j2] += per_person_credit\n",
    "\n",
    "\n",
    "Xauth.my_update(xdict)\n",
    "\n",
    "Xauth = scipy.sparse.csr_matrix(Xauth)\n",
    "paper_vec = []\n",
    "xdict = {}\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "clf4 = SGDRegressor('huber',alpha=1e-3,penalty='l2',tol=1e-6,max_iter=1000,average=True,verbose=1,fit_intercept=FI)\n",
    "clf4.fit(Xauth,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pweff_apm_weights = clf4.coef_\n",
    "pweff_apm_weights = (pweff_apm_weights-pweff_apm_weights.mean())/pweff_apm_weights.std()\n",
    "_ = plt.hist(pweff_apm_weights,100,log=True)\n",
    "print((clf4.intercept_-pweff_apm_weights.mean())/pweff_apm_weights.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    Xauth = scipy.sparse.dok_matrix((n_auths+n_pairwise_authors,n_auths + 1))\n",
    "    y = np.zeros(n_auths+n_pairwise_authors)\n",
    "    xdict = {}\n",
    "\n",
    "    for i,paper in enumerate(all_papers):\n",
    "        tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "        n = len(authors)\n",
    "        ji = years_per_conf*conf_idx[venue] + (year-min_year)//YEAR_BLOCKS\n",
    "        per_person_credit = clf[ji]\n",
    "        for a in authors:\n",
    "            j = name_idx[a]\n",
    "            xdict[(j,j)] = 1 + xdict.get((j,j),0)\n",
    "            # an unknown author\n",
    "            xdict[(j,n_auths)] = (n-1) + xdict.get((j,n_auths),0)\n",
    "\n",
    "            y[j] += per_person_credit\n",
    "\n",
    "\n",
    "        authors_s = sorted(authors)\n",
    "        for ap in itertools.combinations(authors_s, 2):\n",
    "            j2 = pairwise_authors[ap] + n_auths + 1\n",
    "            a1 = name_idx[ap[0]]\n",
    "            a2 = name_idx[ap[1]]\n",
    "            xdict[(j2,a1)] = 1.0 + xdict.get((j2,a1),0)\n",
    "            xdict[(j2,a2)] = 1.0 + xdict.get((j2,a2),0)\n",
    "            y[j2] += per_person_credit\n",
    "            xdict[(j2,n_auths)] = (n-2) + xdict.get((j2,n_auths),0)\n",
    "\n",
    "\n",
    "\n",
    "    Xauth.my_update(xdict)\n",
    "\n",
    "    Xauth = scipy.sparse.csr_matrix(Xauth)\n",
    "    paper_vec = []\n",
    "    xdict = {}\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from sklearn.linear_model import SGDRegressor\n",
    "    clf5 = SGDRegressor('huber',alpha=1e-3,penalty='l2',tol=1e-6,max_iter=25,average=True,verbose=1,fit_intercept=FI)\n",
    "    clf5.fit(Xauth,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    pwunk_apm_weights = clf5.coef_[:n_auths]\n",
    "    pwunk_apm_weights = (pwunk_apm_weights-pwunk_apm_weights.mean())/pwunk_apm_weights.std()\n",
    "    _ = plt.hist(pwunk_apm_weights,100,log=True)\n",
    "    print((clf5.coef_[n_auths]-pwunk_apm_weights.mean())/pwunk_apm_weights.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get author year counts\n",
    "auth_pro_years = defaultdict(set)\n",
    "\n",
    "for i,paper in enumerate(all_papers):\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    for a in authors:\n",
    "        auth_pro_years[a].add(year)\n",
    "            \n",
    "    authors_s = sorted(authors)\n",
    "    for ap in itertools.combinations(authors_s, 2):\n",
    "        auth_pro_years[ap].add(year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_pro_years_count = {k: len(v) for k,v in auth_pro_years.items()}\n",
    "auth_pro_years = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_span2 = np.array([auth_pro_years_count[n] for n in all_authors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xauth = scipy.sparse.dok_matrix((n_auths+n_pairwise_authors,n_auths))\n",
    "y = np.zeros(n_auths+n_pairwise_authors)\n",
    "xdict = {}\n",
    "\n",
    "for i,paper in enumerate(all_papers):\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    ji = years_per_conf*conf_idx[venue] + (year-min_year)//YEAR_BLOCKS\n",
    "    credit = clf[ji]\n",
    "    author_scores = per_author_val[n]\n",
    "\n",
    "    for a,c in zip(authors,author_scores):\n",
    "        j = name_idx[a]\n",
    "        xdict[(j,j)] = c + xdict.get((j,j),0)\n",
    "\n",
    "        y[j] += c*credit\n",
    "\n",
    "    authors_s = sorted([(a,c) for a,c in zip(authors,author_scores)])\n",
    "    for ap in itertools.combinations(authors_s, 2):\n",
    "        a1,c1 = ap[0]\n",
    "        a2,c2 = ap[1] \n",
    "        j2 = pairwise_authors[(a1,a2)] + n_auths\n",
    "        a1 = name_idx[a1]\n",
    "        a2 = name_idx[a2]\n",
    "        xdict[(j2,a1)] = c1 + xdict.get((j2,a1),0)\n",
    "        xdict[(j2,a2)] = c2 + xdict.get((j2,a2),0)\n",
    "        y[j2] += (c1+c2)*credit\n",
    "\n",
    "\n",
    "Xauth.my_update(xdict) \n",
    "\n",
    "Xauth = scipy.sparse.csr_matrix(Xauth)\n",
    "paper_vec = []\n",
    "xdict = {}\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yn = np.zeros_like(y)\n",
    "for k,v in name_idx.items():\n",
    "    yn[v] = auth_pro_years_count[k]\n",
    "for k,v in pairwise_authors.items():\n",
    "    yn[n_auths+v] = auth_pro_years_count[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "clf5 = SGDRegressor('huber',alpha=1e-3,penalty='l2',tol=1e-6,max_iter=25,average=True,verbose=1,fit_intercept=FI)\n",
    "clf5.fit(Xauth,y/yn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwunk_apm_weights = clf5.coef_\n",
    "pwunk_apm_weights = (pwunk_apm_weights-pwunk_apm_weights.mean())/pwunk_apm_weights.std()\n",
    "_ = plt.hist(pwunk_apm_weights,100,log=True)\n",
    "print((clf5.intercept_-pwunk_apm_weights.mean())/pwunk_apm_weights.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#year_span2 = (auth_years[:,1] - auth_years[:,0]) + 1\n",
    "year_span2 = np.maximum(1,year_span2)\n",
    "\n",
    "eff_scores = total_scores/year_span2\n",
    "df_clfs2 = pd.DataFrame(np.vstack([total_scores,eff_scores,apm_weights,pw_apm_weights,pweff_apm_weights,pwunk_apm_weights]).T,columns=['Total','Per Year','APM','PW APM','PW Eff APM','PW APM Yearly'])\n",
    "#print(df_clfs2.corr('spearman').to_latex())\n",
    "corr = df_clfs2.corr('spearman')\n",
    "print(corr.mean(0))\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argsort(apm_weights)[::-1]\n",
    "for k in range(180):\n",
    "    idx = best_idx[k]\n",
    "    name = all_authors[idx]\n",
    "    if name + ' 0001' in name_idx:\n",
    "        print('.',end='')\n",
    "        #continue\n",
    "    print('{:30s}\\t{:.2f}\\t{:.4f}'.format(all_authors[idx],total_scores[idx],apm_weights[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argsort(pw_apm_weights)[::-1]\n",
    "for k in range(180):\n",
    "    idx = best_idx[k]\n",
    "    name = all_authors[idx]\n",
    "    if name + ' 0001' in name_idx:\n",
    "        print('.',end='')\n",
    "        #continue\n",
    "    print('{:30s}\\t{:.2f}\\t{:.2f}\\t{:.2f}'.format(name,total_scores[idx],pw_apm_weights[idx],apm_weights[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argsort(pweff_apm_weights)[::-1]\n",
    "for k in range(180):\n",
    "    idx = best_idx[k]\n",
    "    name = all_authors[idx]\n",
    "    if name + ' 0001' in name_idx:\n",
    "        print('.',end='')\n",
    "        #continue\n",
    "    print('{:30s}\\t{:.2f}\\t{:.2f}\\t{:.2f}'.format(name,total_scores[idx],pweff_apm_weights[idx],apm_weights[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argsort(pwunk_apm_weights)[::-1]\n",
    "for k in range(180):\n",
    "    idx = best_idx[k]\n",
    "    name = all_authors[idx]\n",
    "    if name + ' 0001' in name_idx:\n",
    "        print('.',end='')\n",
    "        #continue\n",
    "    print('{:30s}\\t{:.2f}\\t{:.2f}\\t{:.2f}'.format(name,total_scores[idx],pwunk_apm_weights[idx],apm_weights[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(pwunk_apm_weights[name_idx[name]],name) for name in uni_names if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(pweff_apm_weights[name_idx[name]],name) for name in uni_names if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(pw_apm_weights[name_idx[name]],name) for name in uni_names if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(apm_weights[name_idx[name]],name) for name in uni_names if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(total_scores[name_idx[name]],name) for name in uni_names if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curious_names = ['Xiaolong Wang 0004','Judy Hoffman','Paris Siminelakis','Roie Levin','Leonid Keselman',\n",
    "                 'Nicholas Rhinehart','Vincent Sitzmann','Siddharth Ancha','Xingyu Lin',\n",
    "                 'Humphrey Hu','Aditya Dhawale','Nick Gisolfi','Andrey Kurenkov',\n",
    "                 'David F. Fouhey','Chelsea Finn','Akshara Rai','Ankit Bhatia',\n",
    "                 'Lerrel Pinto','Graeme Best','Alexander Spitzer','Roberto Shu','Amir Abboud',\n",
    "                 'Justin Johnson','Kumar Shaurya Shankar','Ellen A. Cappo',\n",
    "                 'Amir Roshan Zamir','Dominik Peters','Jonathan T. Barron','Dorsa Sadigh','Derek Hoiem','Vaggos Chatziafratis',\n",
    "                 'Brian Okorn','David Held']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(pwunk_apm_weights[name_idx[name]],name) for name in curious_names if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(pweff_apm_weights[name_idx[name]],name) for name in curious_names if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(pw_apm_weights[name_idx[name]],name) for name in curious_names if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(apm_weights[name_idx[name]],name) for name in curious_names if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(total_scores[name_idx[name]],name) for name in curious_names if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_cand = ['Pulkit Agrawal',\n",
    " 'Joydeep Biswas',\n",
    " 'Katherine L. Bouman',\n",
    " 'David Braun',\n",
    " 'Naomi T. Fitter',\n",
    " 'David F. Fouhey',\n",
    " 'Saurabh Gupta 0001',\n",
    " 'Judy Hoffman',\n",
    " 'Hanbyul Joo',\n",
    " 'Changliu Liu',\n",
    " 'Petter Nilsson',\n",
    " \"Matthew O'Toole\",\n",
    " 'Alessandro Roncone',\n",
    " 'Alanson P. Sample',\n",
    " 'Manolis Savva',\n",
    " 'Adriana Schulz',\n",
    " 'Amy Tabb',\n",
    " 'Fatma Zeynep Temel',\n",
    " 'Long Wang 0007',\n",
    " 'Cathy Wu',\n",
    " 'Ling-Qi Yan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(pwunk_apm_weights[name_idx[name]],name) for name in prev_cand if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(pweff_apm_weights[name_idx[name]],name) for name in prev_cand if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(pw_apm_weights[name_idx[name]],name) for name in prev_cand if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(apm_weights[name_idx[name]],name) for name in prev_cand if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(total_scores[name_idx[name]],name) for name in prev_cand if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_cand = ['Yonatan Bisk',\n",
    "             'Angela Dai',\n",
    "             'Abe Davis',\n",
    "             'Tali Dekel',\n",
    "             'Jaime F. Fisac',\n",
    "             'Zakia Hammal',\n",
    "             'Josie Hughes',\n",
    "             'László A. Jeni',\n",
    "             'Angjoo Kanazawa',\n",
    "             'Deepak Pathak',\n",
    "             'Lerrel Pinto',\n",
    "             'Elaine Short',\n",
    "             'Wen Sun 0002',\n",
    "             'Jiajun Wu 0001',\n",
    "             #'Ji Zhang', # disambig\n",
    "             'Jun-Yan Zhu',\n",
    "             'Yuke Zhu'\n",
    "            ]\n",
    "[_ for _ in prev_cand if _ not in name_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(pwunk_apm_weights[name_idx[name]],name) for name in prev_cand if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(pweff_apm_weights[name_idx[name]],name) for name in prev_cand if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(pw_apm_weights[name_idx[name]],name) for name in prev_cand if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(apm_weights[name_idx[name]],name) for name in prev_cand if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_scores = sorted([(total_scores[name_idx[name]],name) for name in prev_cand if name in name_idx],reverse=True)\n",
    "uni_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_affil = pd.read_csv('faculty-affiliations.csv')\n",
    "\n",
    "year_span = (auth_years[:,1] - auth_years[:,0]) + 1\n",
    "faculty_lookup = {_[1]:_[2] for _ in faculty_affil.itertuples()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_paper_year = 2013\n",
    "min_pub_years = 2\n",
    "cand_total = total_scores * (auth_years[:,0] >= first_paper_year).astype(np.float)* (year_span2 >= min_pub_years).astype(np.float) \n",
    "\n",
    "cand_ri = ri_scores* (auth_years[:,0] >= first_paper_year).astype(np.float) * (year_span2 >= min_pub_years).astype(np.float) \n",
    "\n",
    "cand_total_ef = cand_total/year_span2\n",
    "cand_ri_ef = cand_ri/year_span2\n",
    "\n",
    "print('{}\\t{:30s}\\t{:s}\\t{:s}\\t{:s}\\t{:s}\\t{:s}\\t{:20s}'.format('Rank','Author',\n",
    "                                                      'RI','Total',\n",
    "                                                      'eRI','eTotal',\n",
    "                                                      'Since','Affiliation'))\n",
    "for num,idx in enumerate(np.argsort(cand_total_ef)[::-1][:50000]):\n",
    "    uni = faculty_lookup.get(all_authors[idx],'None')\n",
    "    if all_authors[idx] + ' 0001' in name_idx:\n",
    "        continue\n",
    "    print('{}\\t{:30s}\\t{:.1f}\\t{:.1f}\\t{:.1f}\\t{:.1f}\\t{:.0f}\\t{:20s}'.format(num+1,all_authors[idx],\n",
    "                                                      cand_ri[idx],cand_total[idx],\n",
    "                                                      cand_ri_ef[idx],cand_total_ef[idx],\n",
    "                                                      auth_years[idx,0],uni))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_paper_year = 2013\n",
    "min_pub_years = 2\n",
    "cand_apm_total = pw_apm_weights * (auth_years[:,0] >= first_paper_year).astype(np.float)* (year_span >= min_pub_years).astype(np.float) \n",
    "\n",
    "\n",
    "print('{}\\t{:30s}\\t{:s}\\t{:s}\\t{:s}\\t{:s}\\t{:20s}'.format('Rank','Author',\n",
    "                                                      'APM','Total','eTotal',\n",
    "                                                      'Since','Affiliation'))\n",
    "for num,idx in enumerate(np.argsort(cand_apm_total)[::-1][:50000]):\n",
    "    uni = faculty_lookup.get(all_authors[idx],'None')\n",
    "    if all_authors[idx] + ' 0001' in name_idx:\n",
    "        continue\n",
    "    print('{}\\t{:30s}\\t{:.1f}\\t{:.1f}\\t{:.1f}\\t{:.0f}\\t{:20s}'.format(num+1,all_authors[idx],\n",
    "                                                      cand_apm_total[idx],total_scores[idx],eff_scores[idx],\n",
    "                                                      auth_years[idx,0],uni))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_test = 'Alanson P. Sample'\n",
    "print(total_scores[name_idx[name_to_test]])\n",
    "for i,paper in enumerate(all_papers):\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    if not name_to_test in authors:\n",
    "        continue\n",
    "    n = len(authors)\n",
    "    ji = years_per_conf*conf_idx[venue] + (year-min_year)//YEAR_BLOCKS\n",
    "    per_person_credit = clf[ji]/n\n",
    "    authors_s = sorted(authors)\n",
    "    combos = list(itertools.combinations(authors_s, 2))\n",
    "    all_vals = [total_scores[name_idx[n]] for n in authors]\n",
    "    print('{:.2f}\\t{:d} {:d}\\t{:.1f}\\t{:.1f}'.format(per_person_credit,n,len(combos),min(all_vals),max(all_vals) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('apm'+str(FI),apm_weights)\n",
    "np.save('pwapm'+str(FI),pw_apm_weights)\n",
    "np.save('pweffapm'+str(FI),pweff_apm_weights)\n",
    "np.save('pwunkapm'+str(FI),pwunk_apm_weights)\n",
    "np.save('apm'+str(FI),apm_weights)\n",
    "np.save('total',total_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
