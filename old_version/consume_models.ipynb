{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fnmatch\n",
    "import zipfile\n",
    "import xmltodict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import csv\n",
    "import scipy.sparse\n",
    "Xauth = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    uni_faculty = faculty_affil[faculty_affil.affiliation == 'Carnegie Mellon University'] #Carnegie Mellon University\n",
    "    uni_names = np.array(uni_faculty.name)\n",
    "    csr_uni_names = list(uni_names)\n",
    "\n",
    "    cmu_uni = pd.read_csv('cmu_faculty.csv')\n",
    "    cmu_uni = cmu_uni.fillna('Other')\n",
    "    #print(list(cmu_uni.name))\n",
    "    uni_names = list(cmu_uni.name)\n",
    "    uni_labels = list(cmu_uni.dept)\n",
    "    uni_labels_unique = list(set(uni_labels))\n",
    "    t = [_ for _ in uni_names if _ not in csr_uni_names]\n",
    "    for _ in t:\n",
    "        print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.read_hdf('papers.h5','table')\n",
    "# munge the years\n",
    "min_year = papers.year.min()\n",
    "max_year = papers.year.max()\n",
    "span_years = max_year - min_year\n",
    "year_blocks = 8 #11\n",
    "offset_years = [(i-min_year)//year_blocks for i in range(min_year,max_year+1)]\n",
    "year_ind = max(offset_years)+1\n",
    "print(min_year,max_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = pickle.load(open('big_names.pkl','rb'))\n",
    "unique_confs = pickle.load(open('confs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_idx = pickle.load(open('conf_idx.pkl','rb'))\n",
    "name_idx = pickle.load(open('name_idx.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xauth = pickle.load(open('xauth.pkl','rb'))\n",
    "if True: #Xauth is None or (Xauth.shape[1] != year_ind*unique_confs.shape[0]):  \n",
    "    Xauth = scipy.sparse.dok_matrix((len(unique_names),year_ind*unique_confs.shape[0]))\n",
    "    xdict = {}\n",
    "    auth_years = np.ones((len(unique_names),2)) * np.array([3000,1000]) \n",
    "    for row in papers.itertuples():\n",
    "        paper_year = row[10]\n",
    "        #if row['year'] < 2005:\n",
    "        #    continue\n",
    "        #print(row)\n",
    "        #if row['conf'] == 'CoRR':\n",
    "        #    continue\n",
    "        conf = row[2]\n",
    "        n = row[4]\n",
    "        authors = row[3]\n",
    "        j = year_ind*conf_idx[conf] + (paper_year-min_year)//year_blocks\n",
    "        for a in authors:\n",
    "            i = name_idx[a]\n",
    "            xdict[(i,j)] = 1/n + xdict.get((i,j),0)\n",
    "            auth_years[i,0] = min(auth_years[i,0],paper_year)\n",
    "            auth_years[i,1] = max(auth_years[i,1],paper_year)\n",
    "    Xauth._update(xdict)\n",
    "    print(Xauth.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_affil = pd.read_csv('faculty-affiliations.csv')\n",
    "def csv2dict_str_str(fname):\n",
    "    with open(fname, mode='r') as infile:\n",
    "        rdr = csv.reader(infile)\n",
    "        d = {rows[0].strip(): rows[1].strip() for rows in rdr}\n",
    "    return d\n",
    "aliasdict = csv2dict_str_str('dblp-aliases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [pickle.load(open('clf_nsf_log.pkl','rb')),\n",
    "        pickle.load(open('clf_nsf.pkl','rb')),\n",
    "        pickle.load(open('clf_faculty_reg.pkl','rb')),\n",
    "        pickle.load(open('clf_faculty.pkl','rb')),\n",
    "        pickle.load(open('clf_uc.pkl','rb')),\n",
    "        \n",
    "       #pickle.load(open('clf_nsf_ind.pkl','rb')), #no\n",
    "        pickle.load(open('clf_nsf_ind_log.pkl','rb')), #no\n",
    "        #pickle.load(open('clf_nsf_ind_log_r1.pkl','rb')) #No?\n",
    "       ]\n",
    "clfs = [pickle.load(open('clf_nsf_log.pkl','rb')),\n",
    "        \n",
    "        pickle.load(open('clf_nsf.pkl','rb')),\n",
    "        pickle.load(open('clf_faculty_neg_pos_light_reg.pkl','rb')),\n",
    "        pickle.load(open('clf_faculty_neg_pos.pkl','rb')),\n",
    "        pickle.load(open('clf_uc.pkl','rb')),\n",
    "        \n",
    "        #pickle.load(open('clf_nsf_ind.pkl','rb')), #no\n",
    "       # pickle.load(open('clf_nsf_ind_log.pkl','rb')), #no\n",
    "       # pickle.load(open('clf_nsf_ind_log_r1.pkl','rb')) #No?\n",
    "       ]\n",
    "clfs = [np.squeeze(_) for _ in clfs]\n",
    "clp = 7\n",
    "clfs = [np.minimum(clp,np.maximum(-clp,(c-c.mean())/c.std())) for c in clfs]\n",
    "clfs = np.vstack(clfs)\n",
    "auth_years = pickle.load(open('auth_years.pkl','rb'))\n",
    "clf = np.mean(clfs,0)\n",
    "\n",
    "# remove arXiv\n",
    "print(clf.shape[0],conf_idx['CoRR'])\n",
    "non_arxiv = np.ones(clf.shape[0])\n",
    "for i in range(year_ind):\n",
    "    non_arxiv[year_ind*conf_idx['CoRR']+i] = 0\n",
    "clf = clf * non_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_ord = np.argsort(np.squeeze(clf))\n",
    "conf_choice = ['SIGGRAPH','HRI','ECCV (8)','ECCV (1)','Comput. Graph. Forum','Shape Modeling International','Symposium on Geometry Processing','Computer Aided Geometric Design','I. J. Robotics Res.','CVPR','International Journal of Computer Vision','Robotics: Science and Systems','ICRA','WACV','ICML','AISTATS','CoRR','SIGGRAPH Asia','ECCV','ICCV','ISER','Humanoids','3DV','IROS','CoRL','Canadian Conference on AI','ACCV','Graphics Interface','CRV','BMVC']\n",
    "ri_confs = np.zeros(len(unique_confs)*year_ind)\n",
    "ms = clf.mean()\n",
    "ss = clf.std()\n",
    "np.set_printoptions(precision=1)\n",
    "for i in range(len(unique_confs)*year_ind):\n",
    "    idx = conf_ord[-(i+1)]\n",
    "    conf_name = unique_confs[idx//year_ind]\n",
    "    conf_score = clf[idx]\n",
    "    if conf_name in conf_choice:\n",
    "        ri_confs[idx] = 1\n",
    "    if conf_name in conf_choice and (idx%year_ind)==5:\n",
    "    #if 'ICCV' in conf_name and (idx%year_ind)==4:\n",
    "        start_year = offset_years.index(idx%year_ind) + 1970\n",
    "        end_year = len(offset_years) - 1 - offset_years[::-1].index(idx%year_ind) + 1970\n",
    "        print_name =conf_name + '_' + str(start_year)[-2:] +'t' + str(end_year)[-2:]\n",
    "        vec = clfs[:,idx]\n",
    "        print('{:20s}\\t{:.1f}\\t\\t{:.1f}\\t{}'.format(print_name[:20],conf_score,(conf_score-ms)/ss,vec))\n",
    "ri_confs.shape,ri_confs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 50\n",
    "for i in range(top_k):\n",
    "    idx = conf_ord[-(i+1)]\n",
    "    conf_name = unique_confs[idx//year_ind]\n",
    "    conf_score = clf[idx]\n",
    "    start_year = offset_years.index(idx%year_ind) + 1970\n",
    "    end_year = len(offset_years) - 1 - offset_years[::-1].index(idx%year_ind) + 1970\n",
    "    print_name = conf_name[:10] + '_' + str(start_year)[-2:] +'t' + str(end_year)[-2:]\n",
    "    print('{:20s}\\t\\t\\t\\t{:.3f}\\t{:.2f}'.format(print_name,100*conf_score,(conf_score-ms)/ss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_ = plt.hist(clf,70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = Xauth.dot(clf)\n",
    "years_working = (1+auth_years[:,1]-auth_years[:,0])\n",
    "value_scores = scores\n",
    "norm_scores = (value_scores)/years_working\n",
    "ri_filter_mat = scipy.sparse.diags(ri_confs)\n",
    "ri_scores = Xauth.dot(ri_filter_mat).dot(clf)\n",
    "ri_norm_scores = ri_scores/years_working\n",
    "pub_num = Xauth.sum(1)\n",
    "total_scores = Xauth.dot(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "clf_gold = clf\n",
    "for clf in [clf_gold]:\n",
    "    plt.figure()\n",
    "    conf_choice2 = ['SIGGRAPH','AAAI','NIPS','CVPR','ICRA','ICML','ICCV','ECCV',\n",
    "                   'International Journal of Computer Vision','Robotics: Science and Systems']\n",
    "    #conf_choice2 = ['CVPR','ECCV','ICCV','International Journal of Computer Vision','Robotics: Science and Systems']\n",
    "    #conf_choice2 = ['SIGMOD Conference','VLDB','ICDT','KDD','ACM Trans. Inf. Syst.','PODS']\n",
    "    #conf_choice2 = ['ACL','EMNLP','EACL']\n",
    "\n",
    "    conf_choices = [conf_choice2, \n",
    "                    ['STOC','FOCS','SODA','EC','WINE','Electronic Colloquium on Computational Complexity (ECCC)'],\n",
    "                    ['UAI','AAAI','IJCAI','ICML','NIPS'],\n",
    "                    ['ECCV','ICCV','CVPR','BMVC','CRV','International Journal of Computer Vision','3DV','WACV','3DIMPVT'],\n",
    "                    ['ICRA','Robotics: Science and Systems','IROS','CoRL','HRI','ISER'],\n",
    "                    ['SIGGRAPH','SIGGRAPH Asia','ACM Trans. Graph.','Graphics Interface']\n",
    "                   ]\n",
    "    #conf_choices = [['Robotics: Science and Systems','IROS','ICRA','CoRL','WAFR','HRI','ISER']]\n",
    "    for conf_choice2 in conf_choices:\n",
    "        plt.figure()\n",
    "        #conf_choice2 = \n",
    "        conf_choice3 = []\n",
    "        vs = clf.std()\n",
    "        for conf in conf_choice2:\n",
    "            idx = conf_idx[conf]\n",
    "            s = clf_gold[year_ind*idx + year_ind - 2]#max(clf[span_years*idx:span_years*(idx+1)])\n",
    "            conf_choice3.append((s,conf))\n",
    "        plt.figure(figsize=(12,8))\n",
    "        for s,conf in sorted(conf_choice3,reverse=True):\n",
    "            idx = conf_idx[conf]\n",
    "            weights = [clf[year_ind*idx + y]/vs for y in offset_years]\n",
    "            _ = plt.plot(np.arange(min_year,max_year+1)[:-4],weights[:-4],label=conf,lw=5)\n",
    "        plt.grid()\n",
    "        plt.xlabel('year')\n",
    "        plt.ylabel('value')\n",
    "        plt.legend()\n",
    "        #plt.show()\n",
    "\n",
    "    #plt.show()\n",
    "clf = clf_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_cand = ['Pulkit Agrawal',\n",
    " 'Joydeep Biswas',\n",
    " 'Katherine L. Bouman',\n",
    " 'David Braun',\n",
    " 'Jia Deng',\n",
    " 'Naomi T. Fitter',\n",
    " 'David F. Fouhey',\n",
    " 'Saurabh Gupta',\n",
    " 'Judy Hoffman',\n",
    " 'Hanbyul Joo',\n",
    " 'Honglak Lee',\n",
    " 'Changliu Liu',\n",
    " 'Petter Nilsson',\n",
    " \"Matthew O'Toole\",\n",
    " 'Alessandro Roncone',\n",
    " 'Alanson P. Sample',\n",
    " 'Manolis Savva',\n",
    " 'Adriana Schulz',\n",
    " 'Amy Tabb',\n",
    " 'Fatma Zeynep Temel',\n",
    " 'Long Wang',\n",
    " 'Cathy Wu',\n",
    " 'Ling-Qi Yan']\n",
    "print('{:20s}\\t{:4s}\\t{:4s}\\t{:4s}\\t{}\\t{}'.format('name','rate','total','ri','years','pubs'))\n",
    "for ns, name in sorted([(value_scores[name_idx[ni]],ni) for ni in prev_cand],reverse=True):\n",
    "    ni = name_idx[name]\n",
    "    print('{:20s}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.0f}\\t{:.1f}'.format(name,norm_scores[ni],value_scores[ni],ri_scores[ni],years_working[ni],pub_num[ni,0]))\n",
    "print('')\n",
    "curious_names = ['Xiaolong Wang 0004','Judy Hoffman','Paris Siminelakis','Roie Levin','Leonid Keselman',\n",
    "                 'Nicholas Rhinehart','Vincent Sitzmann','Siddharth Ancha','Xingyu Lin',\n",
    "                 'Humphrey Hu',\n",
    "                 'David F. Fouhey','Chelsea Finn',\n",
    "                 'Lerrel Pinto',\n",
    "                 'Justin Johnson',\n",
    "                 'Amir Roshan Zamir','Dominik Peters','Jonathan T. Barron','Dorsa Sadigh','Derek Hoiem','Vaggos Chatziafratis',\n",
    "                 'Brian Okorn','David Held']\n",
    "print('{:20s}\\t{:4s}\\t{:4s}\\t{:4s}\\t{}\\t{}'.format('name','rate','total','ri','years','pubs'))\n",
    "for _,name in sorted([(value_scores[name_idx[_]],_) for _ in curious_names],reverse=True):\n",
    "    ni = name_idx[name]\n",
    "    print('{:20s}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{}\\t{:.1f}'.format(name,norm_scores[ni],value_scores[ni],ri_scores[ni],years_working[ni],pub_num[ni,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n best overall \\n')\n",
    "cmu_scores = []\n",
    "\n",
    "best_scores = np.argsort(value_scores)[::-1]\n",
    "#print(best_scores.shape,unique_names[best_scores[0]])\n",
    "fa_list = list(faculty_affil.name)\n",
    "fa_a_list = list(faculty_affil.affiliation)\n",
    "uni_names = [unique_names[i] for i in best_scores[:50000]]\n",
    "for name in set([aliasdict.get(n, n) for n in uni_names]):\n",
    "    if name in name_idx:\n",
    "        uni = 'unknown'\n",
    "        if name in fa_list:\n",
    "            uni = fa_a_list[fa_list.index(name)]\n",
    "        if name not in []:#['Jacob Walker','Justin Johnson','Pieter Abbeel','Martial Hebert','Jessica K. Hodgins','Abhinav Gupta','Christopher G. Atkeson','Tom M. Mitchell','Matthew T. Mason']:\n",
    "            if years_working[name_idx[name]] < 3:\n",
    "                continue\n",
    "            if years_working[name_idx[name]] > 8:\n",
    "                continue\n",
    "            if ri_scores[name_idx[name]] < 7.2:\n",
    "                continue\n",
    "            if auth_years[name_idx[name],1] < 2016:\n",
    "                continue\n",
    "        #if (np.array(X[name_idx[name],:].todense()) * ri_confs).sum() == 0:\n",
    "        #    continue\n",
    "        #print(name,auth_years[name_idx[name]])\n",
    "        rate_score = norm_scores[name_idx[name]]\n",
    "        ri_rate_score = ri_norm_scores[name_idx[name]]\n",
    "        ri_total_score = ri_scores[name_idx[name]]\n",
    "        total_score = value_scores[name_idx[name]]\n",
    "        cmu_scores.append((rate_score,ri_total_score,total_score,uni,name,auth_years[name_idx[name]],ri_rate_score))\n",
    "    else:\n",
    "        pass\n",
    "        #print(name)\n",
    "        ri_norm_scores\n",
    "print('{:22s}\\t{:15s}\\t{:5s}\\t{:3s}\\t{:4s}\\t{:4s}\\t{} {}'.format('name','uni','rate','RI-t','total','RI-r','start','end'))\n",
    "for vs,ris,s,u,p,yrs,rir in sorted(cmu_scores,reverse=True):\n",
    "    print('{:22s}\\t{:15s}\\t{:.1f}\\t{:.1f}\\t{:.1f}\\t{:.1f}\\t{} {}'.format(p[:22],u[:15],vs,ris,s,rir,int(yrs[0]),int(yrs[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_faculty = faculty_affil[faculty_affil.affiliation == 'Carnegie Mellon University'] #Carnegie Mellon University\n",
    "uni_names = np.array(uni_faculty.name)\n",
    "uni_names = list(uni_names)\n",
    "cmu_scores = []\n",
    "#uni_names = [unique_names[i] for i in (np.argsort(scores)[::-1])[:150]]\n",
    "for name in set([aliasdict.get(n, n) for n in uni_names]):\n",
    "    if name in name_idx:\n",
    "        #if ri_scores[name_idx[name]] < 2.5:\n",
    "        #    continue\n",
    "        score = scores[name_idx[name]]\n",
    "        cmu_scores.append((score,name))\n",
    "    else:\n",
    "        pass\n",
    "        #print(name)\n",
    "for s,p in sorted(cmu_scores,reverse=True):\n",
    "    print('{:30s}\\t\\t{:.3f}'.format(p,s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clfs = [pickle.load(open('clf_nsf_log.pkl','rb')),\n",
    "        \n",
    "        #pickle.load(open('clf_nsf.pkl','rb')),\n",
    "        #pickle.load(open('clf_faculty_neg_pos_light_reg.pkl','rb')),\n",
    "        #pickle.load(open('clf_faculty_neg_pos.pkl','rb')),\n",
    "        pickle.load(open('clf_uc.pkl','rb')),\n",
    "        pickle.load(open('clf_faculty_reg.pkl','rb')),\n",
    "        #pickle.load(open('clf_faculty.pkl','rb')),\n",
    "        #pickle.load(open('clf_nsf_ind.pkl','rb')), #no\n",
    "       # pickle.load(open('clf_nsf_ind_log.pkl','rb')), #no\n",
    "        #pickle.load(open('clf_nsf_ind_log_r1.pkl','rb')) #No?\n",
    "       ]\n",
    "\n",
    "clfs = [pickle.load(open('clf_nsf_log.pkl','rb')),\n",
    "        \n",
    "        pickle.load(open('clf_nsf.pkl','rb')),\n",
    "        pickle.load(open('clf_faculty_neg_pos_light_reg.pkl','rb')),\n",
    "        pickle.load(open('clf_faculty_neg_pos.pkl','rb')),\n",
    "        pickle.load(open('clf_uc.pkl','rb'))]\n",
    "clfs = [np.squeeze(_) for _ in clfs]\n",
    "for clp in [7]:\n",
    "    #clp = 25\n",
    "    clfsn = [np.minimum(clp,np.maximum(-clp,(c-c.mean())/c.std())) for c in clfs]\n",
    "    clfs = np.vstack(clfsn)\n",
    "    #auth_years = pickle.load(open('auth_years.pkl','rb'))\n",
    "    clf = np.mean(clfsn,0)\n",
    "    clf_gold = np.copy(clf)\n",
    "\n",
    "    #clfs_test = np.vstack([clfsn,clf_gold.reshape(1,-1)])\n",
    "    for clfn, clf in enumerate([clf_gold]):\n",
    "        total_scores = Xauth.dot(clf)\n",
    "        df_corr = pd.read_csv('correlation_cleaned.csv')\n",
    "        df_corr = df_corr.drop(columns=[_ for _ in df_corr.columns if 'Unnamed' in _])\n",
    "        df_corr = df_corr.drop(columns=['pms','n_papers'])\n",
    "        df_corr = df_corr.rename(columns={'totals': 'venue_score', 'csrp': 'csr_pubs','csrpn': 'csr_adj','gcite': 'influence'})\n",
    "        pd.set_option('precision', 2)\n",
    "        df_corr = df_corr[['name','papers', 'citations', 'h-index',\n",
    "               'i10','csr_pubs', 'csr_adj','venue_score','influence']]\n",
    "        df_corr = df_corr.dropna('index')\n",
    "        df_corr.index = df_corr.name\n",
    "\n",
    "        ts = total_scores.std()\n",
    "        for name in df_corr.name:\n",
    "            if name in name_idx:\n",
    "                idx = name_idx[name]\n",
    "                df_corr.loc[name,'venue_score'] = total_scores[idx]/ts\n",
    "        print(clp,clfn,df_corr.corr('spearman').loc['influence','venue_score'],df_corr.corr('kendall').loc['influence','venue_score'],df_corr.corr('spearman').loc['h-index','venue_score'])\n",
    "        #if clfn == clfs_test.shape[-1]:\n",
    "    df_corr.corr('spearman')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.corr('spearman').loc['venue_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_mat = pickle.load(open('top_conf_embed.pkl','rb'))\n",
    "ind_mat = np.array(pickle.load(open('top_conf_ind.pkl','rb'))).reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_mat.shape\n",
    "full_to_sub = {}\n",
    "j = 0\n",
    "for i,v in enumerate(ind_mat):\n",
    "    if v:\n",
    "        full_to_sub[i] = j\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_mat.shape,len(full_to_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xauth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_mat = np.repeat(ind_mat,year_ind).astype(np.float)\n",
    "rep_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmap = scipy.sparse.dok_matrix((Xauth.shape[1],vec_mat.shape[0]))\n",
    "xdict = {}\n",
    "print(xmap.shape)\n",
    "for i,v in enumerate(rep_mat):\n",
    "    if v:\n",
    "        xdict[(i,full_to_sub[i//year_ind])] = 1\n",
    "xmap._update(xdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_all = Xauth.dot(xmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_all_mag = mapped_all.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    uni_faculty = faculty_affil[faculty_affil.affiliation == 'Carnegie Mellon University'] #Carnegie Mellon University\n",
    "    uni_names = np.array(uni_faculty.name)\n",
    "    uni_names = list(uni_names)\n",
    "    pd.Series(uni_names).to_csv('cmu_faculty.csv')\n",
    "else:\n",
    "    cmu_uni = pd.read_csv('cmu_faculty.csv')\n",
    "    cmu_uni = cmu_uni.fillna('Other')\n",
    "    #print(list(cmu_uni.name))\n",
    "    uni_names = list(cmu_uni.name)\n",
    "    uni_labels = list(cmu_uni.dept)\n",
    "    uni_labels_unique = list(set(uni_labels))\n",
    "cmu_scores = []\n",
    "uni_colors = []\n",
    "#uni_names = [unique_names[i] for i in (np.argsort(scores)[::-1])[:150]]\n",
    "for name,d in set([(aliasdict.get(n, n),dept) for n,dept in zip(uni_names,uni_labels)]):\n",
    "    if name in name_idx:\n",
    "        #if ri_scores[name_idx[name]] < 2.5:\n",
    "        #    continue\n",
    "        loc = mapped_all[name_idx[name],:].dot(vec_mat)\n",
    "        loc /= max(1,mapped_all_mag[name_idx[name]])\n",
    "        cmu_scores.append((loc,name))\n",
    "        uni_colors.append( uni_labels_unique.index(d))\n",
    "        #print(name,d)\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = np.vstack([_[0] for _ in cmu_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = TSNE(2,20,init='random',n_iter=6500)\n",
    "ys = embedder.fit_transform(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors\n",
    "import matplotlib.patheffects as path_effects\n",
    "\n",
    "nc = (len(uni_labels_unique))\n",
    "cmap = plt.get_cmap('tab10')\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.scatter(ys[:,0],ys[:,1],s=120,lw=1,edgecolors='k',c=cmap(np.array(uni_colors)/nc-0.0))\n",
    "for i in range(ys.shape[0]):\n",
    "    rv = np.random.randn(2)\n",
    "    xr,yr = 0.2*(rv)#/np.linalg.norm(rv)\n",
    "    text= plt.text(ys[i,0]+xr,yr+ys[i,1],cmu_scores[i][1],size='14',color=cmap(uni_colors[i]/nc),\n",
    "             horizontalalignment='center',verticalalignment='center',alpha=0.9,weight='bold')\n",
    "    text.set_path_effects([path_effects.Stroke(linewidth=0.2, foreground='black'),\n",
    "               path_effects.Normal()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# very different stuff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "r1_scores = pickle.load(open('r1_scores.pkl','rb'))\n",
    "r1_confs = pickle.load(open('r1_confs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = TSNE(n_iter=10000,metric='cosine')\n",
    "ys = embed.fit_transform(vec_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = MiniBatchKMeans(26,n_init=100)\n",
    "km.fit(vec_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_small = np.zeros(len(full_to_sub))\n",
    "for old,new in full_to_sub.items():\n",
    "    clf_small[new] = clf[old*year_ind+5]\n",
    "    print(r1_confs[new],clf_small[new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "plt.figure(figsize=(50,25))\n",
    "cmap = plt.get_cmap('tab20c_r')\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(ys[:,0],ys[:,1],c=km.labels_/26,cmap='tab20c_r',s=0.15*r1_scores**2,lw=0.1,edgecolors='k')\n",
    "for i in range(26):\n",
    "    tmp = [(r1_scores[i],i) for i,v in enumerate(km.labels_ == i) if v ] \n",
    "    score_idx = sorted(tmp,reverse=True)\n",
    "    print(i)\n",
    "    k = 4\n",
    "    j = 0\n",
    "    for s,idx in reversed(score_idx[:k]):\n",
    "        rv = np.random.randn(2)\n",
    "        xr,yr = 3*(rv)#/np.linalg.norm(rv)\n",
    "        text = plt.text(xr+ys[idx,0],2*(j-(k-1)/2)/(k-1)+ys[idx,1],r1_confs[idx],size='18',color=np.array(cmap(i/25)),\n",
    "                 ha='center',va='center',alpha=0.9,weight='bold')\n",
    "        text.set_path_effects([path_effects.Stroke(linewidth=1, foreground='black'),\n",
    "                       path_effects.Normal()])\n",
    "        j+=1\n",
    "        print('\\t',r1_confs[idx])\n",
    "    #print()\n",
    "plt.title('Clusters and largest venues',size=48)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "cmap = plt.get_cmap('tab20c_r')#cmap=cmap,c=km.labels_/26 #cmap=cmap2,c=cmap_small\n",
    "cmap2 = plt.get_cmap('viridis')\n",
    "cmap_small = clf_small-clf_small.min()\n",
    "cmap_small=cmap_small/cmap_small.max()\n",
    "plt.scatter(ys[:,0],ys[:,1],cmap=cmap2,c=cmap_small,s=0.15*r1_scores**2,lw=0.1,edgecolors='k')\n",
    "for i in range(26):\n",
    "    tmp = [(clf_small[i],i) for i,v in enumerate(km.labels_ == i) if v ] \n",
    "    score_idx = sorted(tmp,reverse=True)\n",
    "    #print(i)\n",
    "    k = 3\n",
    "    j = 0\n",
    "    print(i)\n",
    "    for s,idx in reversed(score_idx[:k]):\n",
    "        rv = np.random.randn(2)\n",
    "        xr,yr = 2*(rv)#/np.linalg.norm(rv) #np.array(cmap(i/25))\n",
    "        #print(cmap_small[idx],idx)\n",
    "        text = plt.text(ys[idx,0]+xr,3*(j-(k-1)/2)/(k-1)+ys[idx,1],r1_confs[idx],size='20',color=np.array(cmap2(cmap_small[idx])),\n",
    "                 ha='center',va='center',alpha=0.9,weight='bold')\n",
    "        text.set_path_effects([path_effects.Stroke(linewidth=2,foreground='white'),# foreground= np.array(cmap(i/25))),\n",
    "                       path_effects.Normal()])\n",
    "        j+=1\n",
    "        print('\\t',r1_confs[idx],s)\n",
    "plt.title('Highest Quality',size=48)\n",
    "    #print('\\t',r1_confs[idx])\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "plt.figure(figsize=(25,25))\n",
    "cmap = plt.get_cmap('tab20c_r')#cmap=cmap,c=km.labels_/26 #cmap=cmap2,c=cmap_small\n",
    "cmap2 = plt.get_cmap('viridis')\n",
    "cmap_small = clf_small-clf_small.min()\n",
    "cmap_small=cmap_small/cmap_small.max()\n",
    "plt.scatter(ys[:,0],ys[:,1],cmap=cmap,c=km.labels_/26 ,s=0.15*r1_scores**2,lw=0.1,edgecolors='k')\n",
    "for i in range(26):\n",
    "    tmp = [(clf_small[i],i) for i,v in enumerate(km.labels_ == i) if v ] \n",
    "    score_idx = sorted(tmp,reverse=True)\n",
    "    #print(i)\n",
    "    k = 3\n",
    "    j = 0\n",
    "    #print(i)\n",
    "    for s,idx in reversed(score_idx[:k]):\n",
    "        rv = np.random.randn(2)\n",
    "        xr,yr = 3*(rv)#/np.linalg.norm(rv) #np.array(cmap(i/25))\n",
    "        #print(cmap_small[idx],idx)\n",
    "        text = plt.text(ys[idx,0]+xr,3*(j-(k-1)/2)/(k-1)+ys[idx,1],r1_confs[idx],size='20',color=np.array(cmap2(cmap_small[idx])),\n",
    "                 ha='center',va='center',alpha=0.9,weight='bold')\n",
    "        text.set_path_effects([path_effects.Stroke(linewidth=1, foreground='black'),\n",
    "                       path_effects.Normal()])\n",
    "        j+=1\n",
    "        #print('\\t',r1_confs[idx],s)\n",
    "    #print('\\t',r1_confs[idx])\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = pd.read_csv('../ranks.csv')\n",
    "top_k = 36\n",
    "subplot = int(round(np.sqrt(top_k)))\n",
    "min_v = ys.min(0)\n",
    "max_v = ys.max(0)\n",
    "plt.figure(figsize=(subplot*4,subplot*4))\n",
    "for i in range(top_k):\n",
    "    Uname = ranks.iloc[i,:].uni\n",
    "    uni_faculty = faculty_affil[faculty_affil.affiliation == Uname] \n",
    "    uni_names = np.array(uni_faculty.name)\n",
    "    uni_names = list(uni_names)\n",
    "    cmu_scores = []\n",
    "    for name in set([aliasdict.get(n, n) for n in uni_names]):\n",
    "        if name in name_idx:\n",
    "            loc = mapped_all[name_idx[name],:].dot(ys)\n",
    "            loc /= max(1,mapped_all_mag[name_idx[name]])\n",
    "            cmu_scores.append((loc))\n",
    "    cmu_scores = np.squeeze(np.array(cmu_scores))\n",
    "    plt.subplot(subplot,subplot,i+1)\n",
    "    plt.hexbin(cmu_scores[:,0],cmu_scores[:,1],gridsize=15,extent=(min_v[0],max_v[0],min_v[1],max_v[1]),vmin=0,vmax=4)\n",
    "    plt.title(Uname,color='k')\n",
    "    plt.xticks([],[])\n",
    "    plt.yticks([],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "good_names = prev_cand #+ curious_names\n",
    "plt.figure(figsize=(25,25))\n",
    "\n",
    "\n",
    "cmu_uni = pd.read_csv('cmu_faculty.csv')\n",
    "cmu_uni = cmu_uni.fillna('Other')\n",
    "#print(list(cmu_uni.name))\n",
    "uni_names = list(cmu_uni.name) + good_names\n",
    "uni_labels = list(cmu_uni.dept) + len(good_names)*['cand']\n",
    "uni_labels_unique = list(set(uni_labels)) + ['cand']\n",
    "cmu_scores = []\n",
    "cmu_full_dim = []\n",
    "cmu_names = []\n",
    "uni_colors = []\n",
    "cand_num = 0\n",
    "for name,d in [(aliasdict.get(n, n),dept) for n,dept in zip(uni_names,uni_labels)]:\n",
    "    if name in cmu_names:\n",
    "        continue\n",
    "    if name in name_idx:\n",
    "        #if ri_scores[name_idx[name]] < 2.5:\n",
    "        #    continue\n",
    "        loc = mapped_all[name_idx[name],:].dot(ys)\n",
    "        loc /= max(1,mapped_all_mag[name_idx[name]])\n",
    "        cmu_scores.append((loc))\n",
    "        loc = mapped_all[name_idx[name],:].dot(vec_mat)\n",
    "        loc /= max(1,mapped_all_mag[name_idx[name]])\n",
    "        cmu_full_dim.append((loc))\n",
    "        cmu_names.append(name)\n",
    "        uni_colors.append( uni_labels_unique.index(d))\n",
    "        if d == 'cand':\n",
    "            cand_num += 1\n",
    "    else:\n",
    "        pass\n",
    "cmu_scores = np.squeeze(np.array(cmu_scores))\n",
    "import matplotlib.colors\n",
    "nc = (len(uni_labels_unique))\n",
    "cmap = plt.get_cmap('tab10')\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.scatter(cmu_scores[:,0],cmu_scores[:,1],s=120,lw=1,edgecolors='k',c=cmap(np.array(uni_colors)/nc-0.0),alpha=0.5)\n",
    "for i in range(cmu_scores.shape[0]):\n",
    "    rv = np.random.randn(2)\n",
    "    xr,yr = 0.7*(rv)#/np.linalg.norm(rv)\n",
    "    plt.text(cmu_scores[i,0]+xr,yr+cmu_scores[i,1],cmu_names[i],size='16',color=cmap(uni_colors[i]/nc),\n",
    "             horizontalalignment='center',verticalalignment='center',alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_full_dim = np.squeeze(np.array(cmu_full_dim))\n",
    "faculty = cmu_full_dim[:-cand_num,:]\n",
    "cands = cmu_full_dim[-cand_num:,:]\n",
    "faculty.shape,cands.shape\n",
    "nullvec = np.linalg.norm(faculty,axis=1) == 0\n",
    "faculty[nullvec] = np.random.randn(nullvec.sum(),faculty.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "dist = cdist(faculty,cands,metric='cosine')\n",
    "min_dist=np.argmin(dist,0)\n",
    "top_k = 3\n",
    "for i,cand_name in enumerate(cmu_names[-cand_num:]):\n",
    "    nns= np.argsort(dist[:,i])\n",
    "    print('{:20s}'.format(cand_name),end='\\t')\n",
    "    for j in range(top_k):\n",
    "        name_dist = '{} ({:.1f})'.format(cmu_names[nns[j]][:25],100*dist[nns[j],i])\n",
    "        print('{:30s}'.format(name_dist),end=' ')\n",
    "    print('\\n',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.read_hdf('papers.h5','table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict\n",
    "def di():\n",
    "    return defaultdict(float)\n",
    " \n",
    "author_by_year = defaultdict(di)\n",
    "for row in papers.itertuples():\n",
    "    paper_year = row[10]\n",
    "    conf = row[2]\n",
    "    n = row[4]\n",
    "    authors = row[3]\n",
    "    for a in authors:\n",
    "        auth = aliasdict.get(a,a)\n",
    "        author_by_year[auth][paper_year] += clf[year_ind*conf_idx[conf] + offset_years[paper_year-1970]]/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_by_year = np.zeros(2019-1969)\n",
    "v_count = np.zeros(2019-1969)\n",
    "for auth, years in author_by_year.items():\n",
    "    yrs = years.keys()\n",
    "    start_year = min(yrs)\n",
    "    end_year = max(yrs)\n",
    "    span = end_year - start_year\n",
    "    if span < 85:\n",
    "        for y,v in years.items():\n",
    "            val_by_year[y-start_year] += v\n",
    "            v_count[y-start_year] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "plt.figure(figsize=(8,8))\n",
    "example_names = ['Martial Hebert','Christopher G. Atkeson','Takeo Kanade','Matthew T. Mason','Deva Ramanan','Abhinav Gupta'] #,'Pieter Abbeel'\n",
    "for example_name in example_names:\n",
    "    example_value = np.zeros(max_year+1-min_year)\n",
    "    years = author_by_year[example_name]\n",
    "    yrs = [_ for _ in years.keys() if _  > 0]\n",
    "    start_year = min(yrs)\n",
    "    end_year = max(yrs)\n",
    "    span = end_year - start_year\n",
    "    start_year,end_year,span\n",
    "    for y,v in years.items():\n",
    "        example_value[y-1970] += v\n",
    "            \n",
    "    plt.plot(np.arange(1970,2018),gaussian_filter1d(example_value[:-2], sigma=2),label=example_name )\n",
    "    #plt.plot(gaussian_filter1d(example_value[:span], sigma=2),label=example_name )\n",
    "\n",
    "#plt.plot((val_by_year/v_count),label='average author')\n",
    "plt.ylabel('annual value (3yr avg)')\n",
    "#plt.xlabel('year since first publication')\n",
    "plt.xlabel('working year')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_by_year)\n",
    "plt.title('author value by year')\n",
    "plt.xlabel('years since first publication')\n",
    "plt.ylabel('total annual value generated')\n",
    "plt.grid(True)\n",
    "plt.figure()\n",
    "plt.plot(v_count)\n",
    "plt.xlabel('years since first publication')\n",
    "plt.ylabel('number of authors')\n",
    "plt.grid(True)\n",
    "plt.figure()\n",
    "plt.plot(val_by_year/v_count)\n",
    "plt.title('author value by year')\n",
    "plt.xlabel('years since first publication')\n",
    "plt.ylabel('average annual value generated')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "for i in range(0,26,5):\n",
    "    plt.subplot(2,3,i//5+1)\n",
    "    val_by_year_surv = np.zeros(2019-1969)\n",
    "    v_count_surv = np.zeros(2019-1969)\n",
    "    for auth, years in author_by_year.items():\n",
    "        yrs = years.keys()\n",
    "        start_year = min(yrs)\n",
    "        end_year = max(yrs)\n",
    "        span = end_year - start_year\n",
    "        if span >= i:\n",
    "            #value_vec = np.array(list(years.values()))\n",
    "            #min_v = value_vec.min()\n",
    "            #total = (value_vec-min_v).sum()\n",
    "            for y,v in years.items():\n",
    "                val_by_year_surv[y-start_year] += v#(v-min_v)/total\n",
    "                v_count_surv[y-start_year] += 1\n",
    "    plt.plot(val_by_year_surv/v_count_surv)\n",
    "    plt.title('author value by year (career $\\geq$ {} yrs)'.format(i))\n",
    "    plt.xlabel('years since first publication')\n",
    "    plt.ylabel('annual value generated')\n",
    "    #sorted_vals = sorted(val_by_year_surv)\n",
    "    #plt.ylim(-250,1950)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusted Plus-Minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xapm = scipy.sparse.dok_matrix((papers.shape[0],len(unique_names)))\n",
    "xdict = {}\n",
    "y = np.zeros(papers.shape[0])\n",
    "for row in papers.itertuples():\n",
    "    paper_year = row[10]\n",
    "    conf = row[2]\n",
    "    n = row[4]\n",
    "    authors = row[3]\n",
    "    if conf == 'CoRR':\n",
    "        continue\n",
    "    y[row[0]] = clf[year_ind*conf_idx[conf] + offset_years[paper_year-min_year]]\n",
    "    for a in authors:\n",
    "        xdict[(row[0],name_idx[a])] = 1\n",
    "        #Xapm[row[0],name_idx[a]] = 1\n",
    "Xapm._update(xdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist((y-y.mean())/y.std(),50)\n",
    "y.std(),y.mean()\n",
    "plt.figure()\n",
    "_ = plt.hist(y,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "# huber is noise tolerant, squared is not, zeros weights conferences equally, otherwise using learned weightrs \n",
    "X = scipy.sparse.csr_matrix(Xapm)\n",
    "# good ones\n",
    "clf2 = SGDRegressor('huber',alpha=1e-3,penalty='l2',tol=1e-6,max_iter=1000,average=True,verbose=1) #,fit_intercept=False\n",
    "#clf2 = SGDRegressor('squared_loss',alpha=1e-3,penalty='l2',tol=1e-6,max_iter=1000,average=True,verbose=1)\n",
    "# high reg?\n",
    "#clf2 = SGDRegressor('huber',alpha=0,penalty='l2',tol=1e-6,max_iter=100,average=True,verbose=1)\n",
    "\n",
    "#clf2.fit(X,(y-y.mean())/y.std())\n",
    "clf2.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.argsort(clf2.coef_)\n",
    "k = 500\n",
    "rs = ri_scores.std()\n",
    "us = clf2.coef_.std()\n",
    "ts = value_scores.std()\n",
    "for i in range(k):\n",
    "    idx = scores[-(i+1)]\n",
    "    if ri_scores[idx]/rs < 12.0:\n",
    "        continue\n",
    "    print('{}\\t{:30s}\\t\\t\\t{:.1f}'.format(i+1,unique_names[idx][:20],clf2.coef_[idx]/us))\n",
    "_ = plt.hist(clf2.coef_/us,100)\n",
    "clf2.coef_[name_idx['Judea Pearl']]/us,value_scores[name_idx['Judea Pearl']]/ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.argsort(value_scores)\n",
    "ts = value_scores.std()\n",
    "k = 100\n",
    "for i in range(k):\n",
    "    idx = scores[-(i+1)]\n",
    "    if ri_scores[idx]/rs < 12.0:\n",
    "        continue\n",
    "    print('{}\\t{:30s}\\t\\t\\t{:.1f}'.format(i+1,unique_names[idx][:20],value_scores[idx]/ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_faculty = faculty_affil[faculty_affil.affiliation == 'Carnegie Mellon University'] #Carnegie Mellon University\n",
    "uni_names = np.array(uni_faculty.name)\n",
    "uni_names = list(uni_names) + ['Derek Hoiem','Nicholas Rhinehart','Jacob Walker','Lerrel Pinto','Brian Okorn','Leonid Keselman','Siddharth Ancha','Humphrey Hu']\n",
    "cmu_scores = []\n",
    "for name in uni_names:#['Martial Hebert','Abhinav Gupta','Derek Hoiem','David Held']:\n",
    "    if name in name_idx:\n",
    "        idx= name_idx[name]\n",
    "        cmu_scores.append((clf2.coef_[idx]/us,unique_names[idx]))\n",
    "for s,name in sorted(cmu_scores,reverse=True):\n",
    "    print('{:30s}\\t\\t\\t\\t{:.1f}'.format(name[:25],s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 250\n",
    "i = 0\n",
    "fa_list = list(faculty_affil.name)\n",
    "fa_a_list = list(faculty_affil.affiliation)\n",
    "rs = ri_scores.std()\n",
    "print('rank\\t{:23s}\\t{:23s} {:30s} {}\\t{}\\t{} {}'.format('name','uni (if prof)','CMU nn               (nn dist)','APM','RI s','start','end'))\n",
    "scores = np.argsort(clf2.coef_)\n",
    "for sidx in scores[::-1]:\n",
    "    uni = 'unknown'\n",
    "\n",
    "    if years_working[sidx] < 3:\n",
    "        continue\n",
    "    if years_working[sidx] > 8:\n",
    "        continue\n",
    "    if auth_years[sidx,1] < 2016:\n",
    "        continue\n",
    "    if ri_scores[sidx]/rs < 2.0:\n",
    "        continue\n",
    "    if unique_names[sidx] in fa_list:\n",
    "        uni = fa_a_list[fa_list.index(unique_names[sidx] )]\n",
    "    loc = mapped_all[sidx,:].dot(vec_mat)\n",
    "    loc /= max(1,mapped_all_mag[sidx])\n",
    "    dist = cdist(loc,faculty,metric='cosine')\n",
    "\n",
    "    min_dist=np.argmin(dist[0])\n",
    "    cmn, cms= cmu_names[min_dist][:22], dist[0,min_dist]\n",
    "    name_dist = '{:22s} ({:.1f})'.format(cmn,cms*100)\n",
    "\n",
    "    print('{}\\t{:23s}\\t{:23s} {:30s} {:.1f}\\t{:.1f}\\t{:.0f} {:.0f}'.format(i+1,unique_names[sidx][:20],uni[:20],name_dist,clf2.coef_[sidx]/us,ri_scores[sidx]/rs,auth_years[sidx,0],auth_years[sidx,1]))\n",
    "    i+=1\n",
    "\n",
    "    if i == k:\n",
    "         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "ts = total_scores.std()\n",
    "for name in curious_names:\n",
    "    sidx = name_idx[name]\n",
    "    uni = 'unknown'\n",
    "    if unique_names[sidx] in fa_list:\n",
    "        uni = fa_a_list[fa_list.index(unique_names[sidx] )]\n",
    "    i = (scores.shape[0] - np.where(scores == sidx)[0])[0]\n",
    "    z.append((i+1,unique_names[sidx][:20],uni[:20],clf2.coef_[sidx]/us,total_scores[sidx]/ts,ri_scores[sidx]/rs,auth_years[sidx,0],auth_years[sidx,1]))\n",
    "print('{}\\t{:30s}\\t{:25s}{}\\t{}\\t{}\\t{} {}'.format('rank','name','uni','APM','TS','RI-s','start','end'))\n",
    "for _ in sorted(z):\n",
    "    print('{}\\t{:30s}\\t{:25s}{:.1f}\\t{:.1f}\\t{:.1f}\\t{:.0f} {:.0f}'.format(*_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "for name in prev_cand:\n",
    "    sidx = name_idx[name]\n",
    "    uni = 'unknown'\n",
    "    if unique_names[sidx] in fa_list:\n",
    "        uni = fa_a_list[fa_list.index(unique_names[sidx] )]\n",
    "    i = (scores.shape[0] - np.where(scores == sidx)[0])[0]\n",
    "    z.append((i+1,unique_names[sidx][:20],uni[:20],clf2.coef_[sidx]/us,total_scores[sidx]/ts,ri_scores[sidx]/rs,auth_years[sidx,0],auth_years[sidx,1]))\n",
    "print('{}\\t{:30s}\\t{:25s}{}\\t{}\\t{}\\t{} {}'.format('rank','name','uni','APM','TS','RI-s','start','end'))\n",
    "for _ in sorted(z):\n",
    "    print('{}\\t{:30s}\\t{:25s}{:.1f}\\t{:.1f}\\t{:.1f}\\t{:.0f} {:.0f}'.format(*_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict\n",
    "def di():\n",
    "    return defaultdict(float)\n",
    " \n",
    "\n",
    "apm_by_year = np.zeros(2019-1969)\n",
    "apm_cnt_by_year = np.zeros(2019-1969)\n",
    "for idx in range(clf2.coef_.shape[0]):\n",
    "    start_year = auth_years[idx,0]\n",
    "    end_year = auth_years[idx,1]\n",
    "    span = int(end_year - start_year)\n",
    "    if span >= 0:\n",
    "        apm_by_year[span] += clf2.coef_[idx]/us\n",
    "        apm_cnt_by_year[span] += 1\n",
    "plt.plot(apm_by_year)\n",
    "plt.title('plus minus by wokring year')\n",
    "plt.xlabel('years since first publication')\n",
    "plt.ylabel('total apm')\n",
    "plt.grid(True)\n",
    "plt.figure()\n",
    "plt.plot(apm_cnt_by_year)\n",
    "plt.xlabel('years since first publication')\n",
    "plt.ylabel('number of authors')\n",
    "plt.grid(True)\n",
    "plt.figure()\n",
    "plt.plot(apm_by_year/apm_cnt_by_year)\n",
    "plt.title('author value by year')\n",
    "plt.xlabel('years since first publication')\n",
    "plt.ylabel('average apm')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using CLF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500\n",
    "i = 0\n",
    "scores = np.argsort(clf2.coef_) #norm_scores (rate), total_scores (total), clf2.coef_\n",
    "fa_list = list(faculty_affil.name)\n",
    "fa_a_list = list(faculty_affil.affiliation)\n",
    "rs = ri_scores.std()\n",
    "print('rank\\t{:20s}\\t{:20s} {}  {:27s} {}\\t{}\\t{} {}'.format('name','uni (if prof)','score','CMU nn            (nn dist)','APM','RI s','start','end'))\n",
    "\n",
    "for sidx in scores[::-1]:\n",
    "    uni = 'unknown'\n",
    "\n",
    "    if years_working[sidx] < 3:\n",
    "        continue\n",
    "    if years_working[sidx] > 9:\n",
    "        continue\n",
    "    if auth_years[sidx,1] < 2016:\n",
    "        continue\n",
    "    if ri_scores[sidx]/rs < 2.0:\n",
    "        continue\n",
    "    if unique_names[sidx] in fa_list:\n",
    "        uni = fa_a_list[fa_list.index(unique_names[sidx] )]\n",
    "    loc = mapped_all[sidx,:].dot(vec_mat)\n",
    "    loc /= max(1,mapped_all_mag[sidx])\n",
    "    dist = cdist(loc,faculty,metric='cosine')\n",
    "\n",
    "    min_dist=np.argmin(dist[0])\n",
    "    cmn, cms= cmu_names[min_dist][:20], dist[0,min_dist]\n",
    "    name_dist = '{:20s} ({:.1f})'.format(cmn,cms*100)\n",
    "\n",
    "    print('{}\\t{:20s}\\t{:20s} {:.2f} {:27s} {:.1f}\\t{:.1f}\\t{:.0f} {:.0f}'.format(i+1,unique_names[sidx][:20],uni[:20],total_scores[sidx]/rs,name_dist,clf2.coef_[sidx]/us,ri_scores[sidx]/rs,auth_years[sidx,0],auth_years[sidx,1]))\n",
    "    i+=1\n",
    "\n",
    "    if i == k:\n",
    "         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. Martial\n",
    "# 1 0 0 = (total/authors)/years\n",
    "# 0.5 0.5 = (total between authors/authors)/years\n",
    "# 4 papers, 4 authors\n",
    "# 1 0 0 0 = 1\n",
    "# 0.5 0.5 0 0 = 1\n",
    "if False:\n",
    "    import itertools\n",
    "    Xdict = {}\n",
    "    ydict = {}\n",
    "\n",
    "    for row in papers.itertuples():\n",
    "        paper_year = row[10]\n",
    "        conf = row[2]\n",
    "        n = row[4]\n",
    "        authors = row[3]\n",
    "        l=authors\n",
    "        cmb = [itertools.combinations(l,i+1) for i in range(len(l)) ]\n",
    "        for c in cmb:\n",
    "            for nms in c:\n",
    "                ln = len(nms)\n",
    "                idxs = tuple(sorted([name_idx[_] for _ in nms if _ in name_idx]))\n",
    "                Xdict[idxs] = 1\n",
    "                ydict[idxs] = ln*clf[year_ind*conf_idx[conf] + offset_years[paper_year-1970]]/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdict, ydict = None, None\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# university rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_fil = [1 if _ >=5 else 0 for _ in offset_years]\n",
    "recent_fil = len(conf_idx)*[1 if _ >=5 else 0 for _ in range(year_ind)]\n",
    "clf_fil = clf * np.array(recent_fil)\n",
    "rec_scores = Xauth.dot(ri_filter_mat).dot(clf_fil)\n",
    "unis = faculty_affil.affiliation.unique()\n",
    "total_score = Xauth.dot(clf_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fd():\n",
    "    return defaultdict(float)\n",
    "uni_fac_scores = defaultdict(fd)\n",
    "uni_ts_scores = defaultdict(fd)\n",
    "uni_rs_scores = defaultdict(fd)\n",
    "\n",
    "for row in faculty_affil.itertuples():\n",
    "    auth = aliasdict.get(row[1],row[1])\n",
    "    uni = row[2]\n",
    "    if auth not in uni_fac_scores[row[2]] and auth in name_idx:\n",
    "        uni_fac_scores[row[2]][auth] = clf2.coef_[name_idx[auth]]\n",
    "        uni_ts_scores[row[2]][auth] = total_scores[name_idx[auth]]\n",
    "        uni_rs_scores[row[2]][auth] = rec_scores[name_idx[auth]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_fac_scores['Carnegie Mellon University'].values()\n",
    "from scipy.stats import trim_mean ,trimboth\n",
    "uni_pm = {k: trimboth(list(v.values()),0.0).sum() for k,v in uni_fac_scores.items()}\n",
    "uni_ts = {k: trimboth(list(v.values()),0.0).sum() for k,v in uni_ts_scores.items()}\n",
    "uni_rs = {k: trimboth(list(v.values()),0.0).sum() for k,v in uni_rs_scores.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_pm_scores = sorted([(v,k) for k,v in uni_pm.items()],reverse=True)\n",
    "uni_v_scores = sorted([(v,k) for k,v in uni_ts.items()],reverse=True)\n",
    "uni_r_scores = sorted([(v,k) for k,v in uni_rs.items()],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_pm_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_v_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_r_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analytics (authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg(M,alpha=0.85,tol=1e-6,max_iter=1,verbose=False):\n",
    "    N = M.shape[0]\n",
    "    nodelist = np.arange(N)\n",
    "    S = scipy.array(M.sum(axis=1)).flatten()\n",
    "    S[S != 0] = 1.0 / S[S != 0]\n",
    "    Q = scipy.sparse.spdiags(S.T, 0, *M.shape, format='csr')\n",
    "    M = Q * M\n",
    "\n",
    "    # initial vector\n",
    "    x = scipy.repeat(1.0 / N, N)\n",
    "\n",
    "    # Personalization vector\n",
    "    p = scipy.repeat(1.0 / N, N)\n",
    "\n",
    "    # Dangling nodes\n",
    "    dangling_weights = p\n",
    "    is_dangling = scipy.where(S == 0)[0]\n",
    "\n",
    "    # power iteration: make up to max_iter iterations\n",
    "    for _ in range(max_iter):\n",
    "        xlast = x\n",
    "        x = alpha * (x * M + sum(x[is_dangling]) * dangling_weights) + \\\n",
    "            (1 - alpha) * p\n",
    "        # check convergence, l1 norm\n",
    "        err = scipy.absolute(x - xlast).sum()\n",
    "        if verbose:\n",
    "            print(_,err)\n",
    "        if err < N * tol:\n",
    "            return x\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauth_auth = scipy.sparse.dok_matrix((len(unique_names),len(unique_names)))\n",
    "for row in papers.itertuples():\n",
    "    paper_year = row[10]\n",
    "    conf = row[2]\n",
    "    n = row[4]\n",
    "    authors = row[3]\n",
    "    if clf[conf_idx[conf]] > 0:\n",
    "        for a in authors:\n",
    "            auth = aliasdict.get(a,a)\n",
    "            for a2 in authors:\n",
    "                auth2 = aliasdict.get(a2,a2)\n",
    "                if auth in name_idx and auth2 in name_idx:\n",
    "                    gauth_auth[name_idx[auth],name_idx[auth2]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import igraph as ig\n",
    "    sources, targets = gauth_auth.nonzero()\n",
    "    weights = gauth_auth[sources, targets]\n",
    "    weights = np.array(weights) #Need to convert Scipy's matrix format into a form appropriate for igraph\n",
    "    #g = ig.Graph(zip(sources, targets), directed=True, edge_attrs={'weight': weights})\n",
    "    weights.shape\n",
    "    pr2 =  ig.pagerank(g,niter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauth_auth = scipy.sparse.csr_matrix(gauth_auth)\n",
    "pr = pg(gauth_auth,max_iter=100,verbose=True,tol=1e-12)\n",
    "print(gauth_auth.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_s = np.argsort(pr)[::-1]\n",
    "top_k = 100\n",
    "i = 0\n",
    "j = 0 \n",
    "rs = ri_scores.std()\n",
    "while i < top_k:\n",
    "    j += 1\n",
    "    idx = pr_s[j]\n",
    "    if(ri_scores[idx]/rs < 10.0):\n",
    "        continue\n",
    "    print(unique_names[idx],pr[idx],ri_scores[idx]/rs)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis (confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_confs = defaultdict(set)\n",
    "for row in papers.itertuples():\n",
    "    paper_year = row[10]\n",
    "    conf = row[2]\n",
    "    n = row[4]\n",
    "    authors = row[3]\n",
    "    #if clf[conf_idx[conf]] > 0:\n",
    "    for a in authors:\n",
    "        auth = aliasdict.get(a,a)\n",
    "        auth_confs[auth].add(conf_idx[conf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_confs = {k: list(v) for k,v in auth_confs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "auth_confs_iter = {k: itertools.combinations_with_replacement(v,2) for k,v in auth_confs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "dconf = dict()\n",
    "\n",
    "gconf_conf = scipy.sparse.dok_matrix((len(conf_idx),len(conf_idx)))\n",
    "dconf = {}\n",
    "for k,v in auth_confs_iter.items():\n",
    "    for i,j in v:\n",
    "        tmp = 1 + dconf.get((i,j),0)\n",
    "        dconf[(i,j)] = tmp\n",
    "        if i != j:\n",
    "            dconf[(j,i)] = tmp\n",
    "\n",
    "gconf_conf._update(dconf)\n",
    "    #n = len(v)\n",
    "    #for i in range(n):\n",
    "    #    new_row = scipy.sparse.dok_matrix((1,len(conf_idx)))\n",
    "    #    for j in range(i,n):\n",
    "    #        new_row[0,v[j]] = 1\n",
    "    #    new_row = scipy.sparse.csr_matrix(new_row)\n",
    "    #    gconf_conf[v[i]] += new_row\n",
    "     #        i1 = v[i]\n",
    "    #        i2 = v[j]\n",
    "    #        gconf_conf[i1,i2] += 1\n",
    "            #gconf_conf[i2,i1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gconf_conf.setdiag(gconf_conf.diagonal()/2)\n",
    "#gconf_conf =  gconf_conf + gconf_conf.T - scipy.sparse.diags(gconf_conf.diagonal(),format='dok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .diagonal() and .setdiag()\n",
    "gconf_conf = scipy.sparse.csr_matrix(gconf_conf)\n",
    "prc = pg(gconf_conf,max_iter=100,verbose=True,tol=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_s = np.argsort(prc)[::-1]\n",
    "top_k = 100\n",
    "i = 0\n",
    "while i < top_k:\n",
    "    idx = prc_s[i]\n",
    "    print(unique_confs[idx],prc[idx])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_ = plt.hist(np.log(prc),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prcs = np.log(prc)\n",
    "prcs = (prcs - prcs.mean())/prcs.std()\n",
    "scores = []\n",
    "for conf in conf_choice: #+ ['STOC','FOCS','SODA']:\n",
    "    idx = conf_idx[conf]\n",
    "    scores.append((prcs[idx],conf))\n",
    "for s,n in sorted(scores,reverse=True):\n",
    "    print('{:30}\\t{:.1f}'.format(n[:25],s))\n",
    "_ = plt.hist(prcs,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
