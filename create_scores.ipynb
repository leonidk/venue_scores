{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for statistics\n",
    "\n",
    "\n",
    "#### 3 types of authorship model scores (full, 1/n, 1/position)\n",
    "* Mean Scores across years (x)\n",
    "* Max Scores across year & Year of Max ( )\n",
    "* Average value in last 7 years  ( )\n",
    "* Most productive co-author (x)\n",
    "\n",
    "#### Remaining Analysis from 1/position\n",
    "* \"RI\" Conf sub-score (X)\n",
    "* \"Top Graphics\",\"Top Vision\", \"Top Robotics\", \"Top ML\", \"Other\" sub-scores (x)\n",
    "* Average number of authors (x)\n",
    "* Average Author Position (x)\n",
    "* Average & Median \"quality\" of collab ( )\n",
    "* Current Affiliation (x)\n",
    "* Total number of collab ( )\n",
    "* Top 3 collabs (x)\n",
    "* Top 3 conferences from generated value ( )\n",
    "* Career length (x)\n",
    "* Number of collabs w/ more than 4 papers\n",
    "\n",
    "#### Advanced Stats from 1/n\n",
    "* 5 unlabeled variants of plus-minus (w/ intercept) (x)\n",
    "* 5 unlabeled variants of plius-minus (w/o intercept) (x)\n",
    "\n",
    "#### NSF Data\n",
    "* Total number of grants (x)\n",
    "* Total grant money (x)\n",
    "* fractional grant money (x)\n",
    "* grant money of collabs ( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import csv\n",
    "import scipy.sparse\n",
    "Xauth = None\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the update to work despite the broken scipy documentation\n",
    "try:\n",
    "    a = scipy.sparse.dok_matrix((10,10))\n",
    "    a.update({(0,0):1.0})\n",
    "    scipy.sparse.dok_matrix.my_update = scipy.sparse.dok_matrix.update\n",
    "except:\n",
    "    a = scipy.sparse.dok_matrix((10,10))\n",
    "    a._update({(0,0):1.0})\n",
    "    scipy.sparse.dok_matrix.my_update = scipy.sparse.dok_matrix._update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('useful_venue_list.pkl.gz','rb') as fp:\n",
    "    all_venues = pickle.load(fp)\n",
    "with gzip.open('useful_authors_list.pkl.gz','rb') as fp:\n",
    "    all_authors = pickle.load(fp)\n",
    "with gzip.open('useful_papers.pkl.gz','rb') as fp:\n",
    "    all_papers = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_year = all_papers[0][6]\n",
    "max_year = all_papers[-1][6]\n",
    "span_years = max_year - min_year + 1\n",
    "print(min_year,max_year,span_years)\n",
    "conf_idx = {v:i for i,v in enumerate(all_venues)}\n",
    "name_idx = {v:i for i,v in enumerate(all_authors)}\n",
    "n_confs = len(all_venues)\n",
    "n_auths = len(all_authors)\n",
    "n_papers = len(all_papers)\n",
    "print(n_confs,n_auths,n_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreV = {}\n",
    "clf =  np.load('clf_gold.pkl.npy')\n",
    "years_per_conf = clf.shape[0]//n_confs\n",
    "YEAR_BLOCKS = span_years//years_per_conf\n",
    "clf[2323]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for FI in [False,True]:\n",
    "    scoreV['_apm' + str(FI)] = np.load('apm'+str(FI) + '.npy')\n",
    "    scoreV['pw_apm' + str(FI)] = np.load('pwapm'+str(FI) + '.npy')\n",
    "    scoreV['pweff_apm' + str(FI)] = np.load('pweffapm'+str(FI) + '.npy')\n",
    "    scoreV['pwunk_apm' + str(FI)] = np.load('pwunkapm'+str(FI) + '.npy')\n",
    "    print(scoreV['pwunk_apm' + str(FI)].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    try:\n",
    "        import gzip\n",
    "        import pickle\n",
    "        with gzip.open('scoresV2.pkl.gz','rb') as fp:\n",
    "            scoreV = pickle.load(fp)\n",
    "    except:\n",
    "        print('failed!')\n",
    "    PROCESS_DATA = len(scoreV) < 13\n",
    "    print(scoreV['pwunk_apm' + str(FI)].shape)\n",
    "    USE_LIMITS = False\n",
    "else:\n",
    "    PROCESS_DATA = True\n",
    "    USE_LIMITS = False\n",
    "    last_years = np.load('last_years.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k,_.shape) for k,_ in scoreV.items() if _.shape[0] == 2468621]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_uni = pd.read_csv('other_ranks/cmu_faculty.csv')\n",
    "cmu_uni = cmu_uni.fillna('Other')\n",
    "cmu_uni = cmu_uni[cmu_uni.dept == 'RI']\n",
    "uni_names = set(list(cmu_uni.name))\n",
    "\n",
    "print(len(uni_names))\n",
    "conf_counts = {}\n",
    "conf_counts_value = {}\n",
    "\n",
    "#interesting_set = uni_names\n",
    "\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    if year < 2004:\n",
    "        continue\n",
    "    n = len(authors)\n",
    "    for a in authors:\n",
    "        if a in uni_names:\n",
    "            conf_counts[venue] = 1/n + conf_counts.get(venue,0)\n",
    "            conf_counts_value[venue] = clf[years_per_conf*(conf_idx[venue]) + (year-min_year)//YEAR_BLOCKS]/n + conf_counts_value.get(venue,0)\n",
    "conf_counts_value = {k: v/conf_counts[k] for k,v in conf_counts_value.items()}\n",
    "ri_fav_confs = [(conf_counts[_[1]]*conf_counts_value[_[1]],_[1],conf_counts[_[1]],conf_counts_value[_[1]]) for _ in sorted([(v,k) for k,v in conf_counts.items() if v > 0],reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri_confs = [_[1] for _ in sorted(ri_fav_confs,reverse=True) if _[-2] >= 1.25]\n",
    "#confs_to_filter =['ICRA','IROS','Robotics: Science and Systems']\n",
    "ri_confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_DATA:\n",
    "    auth_years = np.ones((n_auths,2)) * np.array([3000,1000]) \n",
    "    for paper in all_papers:\n",
    "        tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "        for a in authors:\n",
    "            i = name_idx[a]\n",
    "            auth_years[i,0] = min(auth_years[i,0],year)\n",
    "            auth_years[i,1] = max(auth_years[i,1],year)\n",
    "    working_years = (auth_years[:,1] - auth_years[:,0]+1)\n",
    "    scoreV['working_years'] = working_years\n",
    "    scoreV['auth_years'] = auth_years\n",
    "    scoreV['last_years'] = last_years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_DATA:\n",
    "    valid_ns = set()\n",
    "    for paper in all_papers:\n",
    "        tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "        n = len(authors)\n",
    "        valid_ns.add(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LIMITS:\n",
    "    for i in range(max(valid_ns)):\n",
    "        valid_ns.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_types = {\n",
    "        'RI': ri_confs,\n",
    "        'ML':['NIPS','ICML','AAAI','AISTATS','IJCAI','UAI','CoRL','ICLR'],\n",
    "        'CV':['CVPR','ICCV','ECCV','IEEE Trans. Pattern Anal. Mach. Intell.','FGR','Int. J. Comput. Vis.','WACV','BMVC','ACCV'],\n",
    "        'ROB':['HRI','Int. J. Robotics Res.','Robotics: Science and Systems','Humanoids','WAFR','IROS','ICRA','FSR','ISER','ISRR','AAMAS','IEEE Robotics Autom. Lett.','IEEE Trans. Robotics and Automation'],\n",
    "        'GR':['ACM Trans. Graph.','Comput. Graph. Forum','SIGGRAPH','SIGGRAPH Asia','Symposium on Computer Animation'],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_types = ['full','1/n','1/i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_DATA:\n",
    "    confTypeN = len(conf_types)+1\n",
    "    YearConf = scipy.sparse.lil_matrix((n_confs*years_per_conf,years_per_conf*confTypeN))\n",
    "    for i in range(years_per_conf):\n",
    "        year_filter = np.zeros_like(clf).reshape((-1,years_per_conf))\n",
    "        year_filter[:,i] = 1\n",
    "        YearConf[:,i*confTypeN] = (clf * year_filter.reshape(clf.shape))[:,np.newaxis]\n",
    "        j = 1\n",
    "        for f_type, f_confs in conf_types.items():\n",
    "            year_filter = np.zeros_like(clf).reshape((-1,years_per_conf))\n",
    "            for conf in f_confs:\n",
    "                year_filter[conf_idx[conf],i] = 1\n",
    "            YearConf[:,i*confTypeN+j] = (clf * year_filter.reshape(clf.shape))[:,np.newaxis]\n",
    "            j+=1\n",
    "    YearConf = scipy.sparse.csr_matrix(YearConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import gc\n",
    "if PROCESS_DATA:\n",
    "    for amt in am_types:\n",
    "\n",
    "        per_author_val = {}\n",
    "\n",
    "        if amt == 'full':\n",
    "            for n in valid_ns:\n",
    "                author_scores = np.ones(n)\n",
    "                per_author_val[n] = author_scores\n",
    "        elif amt == '1/n':\n",
    "            for n in valid_ns:\n",
    "                author_scores = (np.ones(n))\n",
    "                per_author_val[n] = author_scores/author_scores.sum()\n",
    "        elif amt == '1/i':\n",
    "            for n in valid_ns:\n",
    "                author_scores = 1/(np.arange(n)+1)\n",
    "                per_author_val[n] = author_scores/author_scores.sum()\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "        count_vecs = {}\n",
    "        paper_vecs = []\n",
    "        for paper in all_papers:\n",
    "                \n",
    "            tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "            n = len(authors)\n",
    "            j = years_per_conf*conf_idx[venue] + (year-min_year)//YEAR_BLOCKS\n",
    "\n",
    "            author_scores = per_author_val[n]\n",
    "            if USE_LIMITS:\n",
    "                tmpp = []\n",
    "                tmpapers = authors[:-1] if n >= 2 else authors\n",
    "                tmpscores = author_scores[:-1] if n >= 2 else author_scores\n",
    "\n",
    "                for a,v in zip(tmpapers,tmpscores):\n",
    "                    idx = name_idx[a]\n",
    "                    if year > last_years[idx]:\n",
    "                        continue\n",
    "                    tmpp.append((idx,j,v))\n",
    "                paper_vecs.append(tmpp)\n",
    "            else:\n",
    "                paper_vecs.append([(name_idx[a],j,v) for a,v in zip(authors,author_scores)])\n",
    "\n",
    "        Xauth = scipy.sparse.dok_matrix((n_auths,years_per_conf*n_confs))\n",
    "        xdict = {}\n",
    "\n",
    "        for paper_vec in paper_vecs:\n",
    "            for i,j,v in paper_vec:\n",
    "                xdict[(i,j)] = v + xdict.get((i,j),0)\n",
    "\n",
    "        Xauth.my_update(xdict)\n",
    "\n",
    "        Xauth = scipy.sparse.csr_matrix(Xauth)\n",
    "\n",
    "\n",
    "\n",
    "        scoreV[amt] = Xauth @ YearConf\n",
    "\n",
    "\n",
    "        paper_vec = []\n",
    "        xdict = {}\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "for am in am_types:\n",
    "    #scores = np.array(scoreV[am]).reshape((n_auths,years_per_conf,-1)).astype(np.float32)\n",
    "    scores = np.array(scoreV[am].todense()).reshape((n_auths,years_per_conf,-1)).astype(np.float32)\n",
    "    scores = np.transpose(scores,(0,2,1))\n",
    "    smooth_kernel = scipy.ndimage.gaussian_filter1d(np.identity(years_per_conf,np.float32),1)\n",
    "    scores = scores @ smooth_kernel\n",
    "    scoreV[am] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sTypes = ['Full'] + [k for k,v in conf_types.items()]\n",
    "scores.dtype,scores.nbytes,gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_years = scoreV['auth_years']\n",
    "working_years = scoreV['working_years']\n",
    "\n",
    "total_scores = scoreV['1/i'][:,sTypes.index('Full')].sum(1)\n",
    "ri_scores = scoreV['1/i'][:,sTypes.index('RI')].sum(1)\n",
    "ri_eff_scores = ri_scores/working_years#,np.maximum(auth_years[:,1]-2000,1))\n",
    "\n",
    "ri_scores_max = scoreV['1/i'][:,sTypes.index('RI')].max(1)\n",
    "ri_scores_max_yr = np.argmax(scoreV['1/n'][:,sTypes.index('RI')],axis=1)*YEAR_BLOCKS + min_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreV['1/i_total_1970'] = scoreV['1/i'][:,sTypes.index('Full')].sum(1)\n",
    "scoreV['1/i_RI_1970'] = scoreV['1/i'][:,sTypes.index('RI')].sum(1)\n",
    "for sub in ['ROB','CV','GR','ML']:\n",
    "    den = scoreV['1/i_{}_1970'.format(sub)] = scoreV['1/i'][:,sTypes.index(sub)].sum(1)\n",
    "scoreV['full_total_1970'] = scoreV['full'][:,sTypes.index('Full')].sum(1)\n",
    "scoreV['1/n_total_1970'] = scoreV['1/n'][:,sTypes.index('Full')].sum(1)\n",
    "\n",
    "\n",
    "scoreV['1/n_max'] = scoreV['1/n'][:,sTypes.index('Full')].max(1)\n",
    "scoreV['1/n_max_yr'] = np.argmax(scoreV['1/n'][:,sTypes.index('Full')],axis=1)*YEAR_BLOCKS+min_year\n",
    "\n",
    "scoreV['1/i_max'] = scoreV['1/i'][:,sTypes.index('Full')].max(1)\n",
    "scoreV['1/i_max_yr'] = np.argmax(scoreV['1/i'][:,sTypes.index('Full')],axis=1)*YEAR_BLOCKS+min_year\n",
    "\n",
    "scoreV['full_max'] = scoreV['full'][:,sTypes.index('Full')].max(1)\n",
    "scoreV['full_max_yr'] = np.argmax(scoreV['full'][:,sTypes.index('Full')],axis=1)*YEAR_BLOCKS+min_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in am_types:\n",
    "    del scoreV[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in scoreV.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_auth_pos = np.zeros(n_auths)\n",
    "avg_auth_cnt = np.zeros(n_auths)\n",
    "auth_been_last = np.zeros(n_auths)\n",
    "auth_first_last_year = 3000*np.ones(n_auths)\n",
    "\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    if n  > 1:\n",
    "        for i,a in enumerate(authors):\n",
    "            idx = name_idx[a]\n",
    "            pos = i/(n-1)\n",
    "            auth_been_last[idx] += int(pos == 1)\n",
    "            avg_auth_pos[idx] += pos\n",
    "            avg_auth_cnt[idx] += 1\n",
    "        auth_first_last_year[name_idx[authors[-1]]] = min(year,auth_first_last_year[name_idx[authors[-1]]])\n",
    "np.save('last_years',auth_first_last_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_years = np.load('last_years.npy')\n",
    "scoreV['last_years'] = last_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_DATA:\n",
    "    import gzip\n",
    "    import pickle\n",
    "    with gzip.open('scoresV2.pkl.gz','wb') as fp:\n",
    "        pickle.dump(scoreV,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(scoreV),PROCESS_DATA)#,years_per_conf"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
