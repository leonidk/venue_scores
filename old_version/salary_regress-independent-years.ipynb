{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fnmatch\n",
    "import zipfile\n",
    "import xmltodict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import csv\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salary data\n",
    "dt = {'Employee Name': str,\n",
    "'Job Title': str,\n",
    "'Base Pay': float,\n",
    "'Overtime Pay': float,\n",
    "'Other Pay': float,\n",
    "'Benefits': float,\n",
    "'Total Pay': float,\n",
    "'Total Pay & Benefits': float,\n",
    "'Year': float,\n",
    "'Notes': str,\n",
    "'Agency': str,\n",
    "'Status': str}\n",
    "na_values = [ 'Aggregate','#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NA', 'NULL', 'NaN', 'n/a', 'nan', 'null']\n",
    "dfs = [pd.read_csv('university-of-california-{}.csv'.format(i),dtype=dt,na_values=na_values) for i in range(2015,2018)]\n",
    "Xauth = None\n",
    "[_.shape for _ in dfs],sum([_.shape[0] for _ in dfs])\n",
    "dfs = [_[_['Job Title'].str.contains('PROF')] for _ in dfs]\n",
    "dfs = [_[['Employee Name','Total Pay & Benefits']] for _ in dfs]\n",
    "dfs = [_.reset_index(drop=True) for _ in dfs]\n",
    "[_.shape for _ in dfs],sum([_.shape[0] for _ in dfs])\n",
    "from collections import defaultdict\n",
    "ca_pay = defaultdict(int)\n",
    "for df in dfs:\n",
    "    df = df.fillna(0)\n",
    "    for row in df.itertuples():\n",
    "        ca_pay[row[1]] = max(ca_pay[row[1]],row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_confs = pickle.load(open('r1_confs.pkl','rb'))\n",
    "r1_confs_dict = {_:1 for _ in r1_confs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the big paper thing\n",
    "papers = pd.read_hdf('papers.h5','table')\n",
    "unique_names = pickle.load(open('big_names.pkl','rb'))\n",
    "unique_confs = pickle.load(open('confs.pkl','rb'))\n",
    "\n",
    "faculty_affil = pd.read_csv('faculty-affiliations.csv')\n",
    "ranks = pd.read_csv('ranks.csv')\n",
    "def csv2dict_str_str(fname):\n",
    "    with open(fname, mode='r') as infile:\n",
    "        rdr = csv.reader(infile)\n",
    "        d = {rows[0].strip(): rows[1].strip() for rows in rdr}\n",
    "    return d\n",
    "aliasdict = csv2dict_str_str('dblp-aliases.csv')\n",
    "conf_idx = pickle.load(open('conf_idx.pkl','rb'))\n",
    "name_idx = pickle.load(open('name_idx.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areadict = {\n",
    "    'icse' : ['ICSE', 'ICSE (1)'],\n",
    "    'fse'  : ['SIGSOFT FSE', 'ESEC/SIGSOFT FSE'],\n",
    "    'usenixatc' : ['USENIX Annual Technical Conference', 'USENIX Annual Technical Conference, General Track'], # next tier\n",
    "    'imc': ['IMC', 'Internet Measurement Conference'],\n",
    "    'sigmetrics': ['SIGMETRICS', 'SIGMETRICS/Performance', 'POMACS'],\n",
    "    'mobicom' : ['MobiCom', 'MOBICOM'],\n",
    "    'rtas' : ['RTAS', 'IEEE Real-Time and Embedded Technology and Applications Symposium'],\n",
    "    'ccs': ['CCS', 'ACM Conference on Computer and Communications Security'],\n",
    "    'oakland' : ['IEEE Symposium on Security and Privacy'],\n",
    "    'usenixsec' : ['USENIX Security Symposium', 'USENIX Security'],\n",
    "    'pets' : ['PoPETs', 'Privacy Enhancing Technologies'],\n",
    "    'cav': ['CAV', 'CAV (1)', 'CAV (2)'],\n",
    "    'lics' : ['LICS', 'CSL-LICS'],\n",
    "    'nips': ['NIPS', 'NeurIPS'],\n",
    "    'icml': ['ICML', 'ICML (1)', 'ICML (2)', 'ICML (3)'],\n",
    "    'aaai': ['AAAI', 'AAAI/IAAI'],\n",
    "    'ubicomp' : ['UbiComp', 'Ubicomp', 'IMWUT', 'Pervasive'],\n",
    "    'emnlp': ['EMNLP', 'EMNLP-CoNLL', 'HLT/EMNLP'],\n",
    "    'acl' : ['ACL', 'ACL (1)', 'ACL (2)', 'ACL/IJCNLP', 'COLING-ACL'],\n",
    "    'naacl' : ['NAACL', 'HLT-NAACL', 'NAACL-HLT'],\n",
    "    'cvpr': ['CVPR', 'CVPR (1)', 'CVPR (2)'],\n",
    "    'eccv': ['ECCV', 'ECCV (1)', 'ECCV (2)', 'ECCV (3)', 'ECCV (4)', 'ECCV (5)', 'ECCV (6)', 'ECCV (7)', 'ECCV (8)', 'ECCV (9)', 'ECCV (10)', 'ECCV (11)', 'ECCV (12)', 'ECCV (13)', 'ECCV (14)', 'ECCV (15)', 'ECCV (16)'],\n",
    "    'icra': ['ICRA', 'ICRA (1)', 'ICRA (2)'],\n",
    "    'rss': ['Robotics: Science and Systems'],\n",
    "    'crypto': ['CRYPTO', 'CRYPTO (1)', 'CRYPTO (2)', 'CRYPTO (3)'],\n",
    "    'eurocrypt': ['EUROCRYPT', 'EUROCRYPT (1)', 'EUROCRYPT (2)', 'EUROCRYPT (3)'],\n",
    "}\n",
    "inverse_area_dict = {}\n",
    "for k,v in areadict.items():\n",
    "    n = len(v)\n",
    "    for i in range(1,n):\n",
    "        inverse_area_dict[v[i]] = v[0]\n",
    "for k,v in inverse_area_dict.items():\n",
    "    if k in conf_idx and v in conf_idx:\n",
    "        conf_idx[k] = conf_idx[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys = {}\n",
    "for k,v in name_idx.items():\n",
    "    ns = k.split(' ')\n",
    "    n_s = ns[0] + ' ' + ns[-1]\n",
    "\n",
    "    if not (ns[0] + ns[-1]).isalpha():\n",
    "        continue\n",
    "    if n_s not in name_idx:\n",
    "        new_keys[n_s] = v\n",
    "name_idx.update(new_keys)\n",
    "\n",
    "# munge the years\n",
    "min_year = papers.year.min()\n",
    "max_year = papers.year.max()\n",
    "span_years = max_year - min_year +1\n",
    "\n",
    "print(span_years,min_year,max_year,len(name_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "from unidecode import unidecode\n",
    "\n",
    "for name in list(name_idx.keys()):\n",
    "    dn = unidecode(ftfy.fix_encoding(name))\n",
    "    name_idx[dn] = name_idx[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys = list(ca_pay.keys())\n",
    "ca_pay_prof = {}\n",
    "for name in keys:\n",
    "    name_s = name.split(' ')\n",
    "    if name in name_idx:\n",
    "        n = name\n",
    "        ca_pay_prof[name] = (name_idx[n] ,ca_pay[name])\n",
    "    elif name_s[0] + name_s[-1] in name_idx:\n",
    "        n = name_s[0] + name_s[-1]\n",
    "        ca_pay_prof[name] = (name_idx[n] ,ca_pay[name])\n",
    "print(len(ca_pay_prof))\n",
    "ca_pay_prof['Kristofer Pister'],ca_pay_prof['Pieter Abbeel'],ca_pay_prof['Sergey Levine'],ca_pay_prof['Jitendra Malik'],len(ca_pay_prof)\n",
    "ca_prof_n = len(ca_pay_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict\n",
    "# create or load some author data\n",
    "def dd():\n",
    "    return defaultdict(list)\n",
    "if True:\n",
    "    papers_per_year = {} \n",
    "    author_papers = defaultdict(dd)\n",
    "\n",
    "    for row in papers.itertuples():\n",
    "        paper_year = row[10]\n",
    "        conf = row[2]\n",
    "        n = row[4]\n",
    "        authors = row[3]\n",
    "        yc = papers_per_year.get(conf,np.zeros(2020-1970))\n",
    "        yc[paper_year-1970] += 1\n",
    "        papers_per_year[conf] = yc\n",
    "        for a in authors:\n",
    "            split_name = a.split(' ')\n",
    "            if not split_name[-1].isalpha() and len(split_name) > 2:\n",
    "                first_last = split_name[0] +' ' + split_name[-2]\n",
    "            else: \n",
    "                first_last = split_name[0] +' ' + split_name[-1]\n",
    "                author_papers[first_last][paper_year].append((conf,n))\n",
    "    import pickle\n",
    "    with open('nsf_auth.pkl','wb') as fp:\n",
    "        pickle.dump(author_papers,fp)\n",
    "else:\n",
    "    pass\n",
    "    with open('nsf_auth.pkl','rb') as fp:\n",
    "        author_papers = pickle.load(fp)\n",
    "    with open('papers_per_year.pkl','rb') as fp:\n",
    "        papers_per_year = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddn():\n",
    "    return defaultdict(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "sigma = 2\n",
    "weights = []\n",
    "for i in range(span_years):\n",
    "    a = np.array([scipy.stats.norm.pdf( (j-i)/sigma) for j in range(span_years)])\n",
    "    a[a < 0.05] = 0\n",
    "    weights.append(a/np.linalg.norm(a))\n",
    "_ = plt.plot(np.arange(span_years)+min_year,weights[2000-min_year])\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#pairs_of_years = itertools.product(range(span_years),range(span_years))\n",
    "\n",
    "wdict = {}\n",
    "for i,j,k in itertools.product(range(unique_confs.shape[0]),range(span_years),range(span_years)):\n",
    "    wdict[i*span_years+j,i*span_years+k] = weights[j][k]\n",
    "wsa = scipy.sparse.dok_matrix((span_years*unique_confs.shape[0],span_years*unique_confs.shape[0]))\n",
    "wsa._update(wdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create design mattrix\n",
    "X = scipy.sparse.dok_matrix((ca_prof_n,span_years*unique_confs.shape[0]))\n",
    "xdict = {}\n",
    "y = np.zeros(ca_prof_n,dtype=np.float32)\n",
    "y_paper = np.zeros(ca_prof_n,dtype=np.float32)\n",
    "\n",
    "for idx,d in enumerate(ca_pay_prof.items()):\n",
    "    k,v = d\n",
    "    a = unique_names[v[0]]\n",
    "    y[idx] = v[1]\n",
    "    sum_paper = 0 \n",
    "    for year_a,conf_list in author_papers[a].items():\n",
    "        for paper in conf_list:\n",
    "            j = span_years*conf_idx[paper[0]] + year_a-min_year\n",
    "            xdict[(idx,j)] = 1/paper[1]\n",
    "            sum_paper += 1\n",
    "    y_paper[idx] = sum_paper\n",
    "X._update(xdict)\n",
    "y_orig = np.copy(y)\n",
    "print(X.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scipy.sparse.csr_matrix(X)\n",
    "wsa = scipy.sparse.csr_matrix(wsa)\n",
    "X = X @ wsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonarxiv = np.ones(span_years*len(unique_confs))\n",
    "nonarxiv[span_years*conf_idx['CoRR']:span_years*(conf_idx['CoRR']+1)] = 0\n",
    "r1_confs = np.ones(span_years*len(unique_confs))\n",
    "\n",
    "for k,v in r1_confs_dict.items():\n",
    "    pass\n",
    "    #r1_confs[span_years*conf_idx[k]:span_years*(conf_idx[k]+1)] = 1\n",
    "\n",
    "skipped_conf = scipy.sparse.diags(nonarxiv * r1_confs)\n",
    "skipped_data_vec =  np.ones_like(y) * (y > 50000)# * (y > 120000) * (y < 600000)\n",
    "skipped_data_vec = skipped_data_vec.astype(np.float)\n",
    "skipped_data = scipy.sparse.diags(skipped_data_vec)\n",
    "\n",
    "y_reg = np.copy(y)\n",
    "y_reg[skipped_data_vec == 0] = y[skipped_data_vec != 0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_orig = np.copy(y)\n",
    "print(y_reg.min(),y_reg.max())\n",
    "print((y_reg > 0).sum())\n",
    "if True: # do log\n",
    "    y_reg = np.copy(np.log(1+y_reg))\n",
    "else:\n",
    "    y_reg = np.copy(y_reg)\n",
    "\n",
    "from matplotlib.pyplot import figure,hist\n",
    "hist((y_reg-y_reg.mean())/y.std(),100)\n",
    "figure()\n",
    "_ = hist(y_reg,100)\n",
    "print(y_reg.min(),y_reg.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X = scipy.sparse.csr_matrix(X)\n",
    "clf = SGDRegressor('huber',tol=1e-9,max_iter=250,verbose=0,penalty='l2',alpha=1e-3,epsilon=0.01,average=True)\n",
    "#clf = SGDRegressor('huber',tol=1e-9,max_iter=100,verbose=1,penalty='l1',alpha=1e-7)\n",
    "\n",
    "clf.fit(skipped_data @X@ skipped_conf ,(y-y.mean())/y.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_ord = np.argsort(np.squeeze(clf.coef_))\n",
    "conf_choice = ['SIGGRAPH','HRI','ECCV (8)','ECCV (1)','Comput. Graph. Forum','Shape Modeling International','Symposium on Geometry Processing','Computer Aided Geometric Design','I. J. Robotics Res.','CVPR','International Journal of Computer Vision','Robotics: Science and Systems','ICRA','WACV','ICML','AISTATS','CoRR','SIGGRAPH Asia','ECCV','ICCV','ISER','Humanoids','3DV','IROS','CoRL','Canadian Conference on AI','ACCV','Graphics Interface','CRV','BMVC']\n",
    "ri_confs = np.zeros(len(unique_confs)*span_years)\n",
    "print(clf.intercept_)\n",
    "ms = clf.coef_.mean()\n",
    "ss = clf.coef_.std()\n",
    "seen = {}\n",
    "for i in range(len(unique_confs)*span_years):\n",
    "    idx = conf_ord[-(i+1)]\n",
    "    conf_name = unique_confs[idx//span_years]\n",
    "    conf_score = clf.coef_[idx]\n",
    "    if conf_name in conf_choice:\n",
    "        ri_confs[idx] = 1\n",
    "    if conf_name in conf_choice and conf_name not in seen:\n",
    "        print('{:20s}{}\\t{:.1f}'.format(conf_name[:20],str(min_year + (idx % span_years)),(conf_score-ms)/ss))\n",
    "        seen[conf_name] =1\n",
    "ri_confs.shape,ri_confs.sum(),X.shape\n",
    "\n",
    "conf_choice2 = ['SIGGRAPH','BMVC','AAAI','NIPS','CVPR','ICRA','ICML','ICCV','ECCV','IROS',\n",
    "               'International Journal of Computer Vision','Robotics: Science and Systems']\n",
    "conf_choice3 = []\n",
    "vs = clf.coef_.std()\n",
    "for conf in conf_choice2:\n",
    "    idx = conf_idx[conf]\n",
    "    s = max(clf.coef_[span_years*idx:span_years*(idx+1)])\n",
    "    conf_choice3.append((s,conf))\n",
    "plt.figure(figsize=(12,8))\n",
    "for s,conf in sorted(conf_choice3,reverse=True):\n",
    "    idx = conf_idx[conf]\n",
    "    _ = plt.plot(np.arange(min_year,max_year+1),(clf.coef_[span_years*idx:span_years*(idx+1)]/vs),label=conf)\n",
    "plt.grid()\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('value')\n",
    "plt.legend()\n",
    "plt.savefig('salary-fixed-log-names.pdf')\n",
    "pickle.dump(clf.coef_,open('salary-fixed-log-names.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 50\n",
    "i = -1\n",
    "j = 0\n",
    "seen = {}\n",
    "while j < top_k:\n",
    "    i += 1\n",
    "    idx = conf_ord[-(i+1)]\n",
    "    conf_name = unique_confs[idx//span_years]\n",
    "    if conf_name in seen:\n",
    "        continue\n",
    "    j+=1\n",
    "    conf_score = clf.coef_[idx]\n",
    "    seen[conf_name] = 1\n",
    "    print('{:20s}\\t{}\\t\\t{:.3f}\\t{:.2f}'.format(conf_name[:18],min_year + (idx % span_years),100*conf_score,(conf_score-ms)/ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = hist(clf.coef_,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Xauth is None or (Xauth.shape[1] != span_years*unique_confs.shape[0]):  \n",
    "    Xauth = scipy.sparse.dok_matrix((len(unique_names),span_years*unique_confs.shape[0]))\n",
    "    xdict = {}\n",
    "    auth_years = np.ones((len(unique_names),2)) * np.array([3000,1000]) \n",
    "    for row in papers.itertuples():\n",
    "        paper_year = row[10]\n",
    "        #if row['year'] < 2005:\n",
    "        #    continue\n",
    "        #print(row)\n",
    "        #if row['conf'] == 'CoRR':\n",
    "        #    continue\n",
    "        conf = row[2]\n",
    "        n = row[4]\n",
    "        authors = row[3]\n",
    "        j = span_years*conf_idx[conf] + (paper_year-min_year)\n",
    "        for a in authors:\n",
    "            i = name_idx[a]\n",
    "            xdict[(i,j)] = 1/n + xdict.get((i,j),0)\n",
    "            auth_years[i,0] = min(auth_years[i,0],paper_year)\n",
    "            auth_years[i,1] = max(auth_years[i,1],paper_year)\n",
    "    Xauth._update(xdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = clf.predict(Xauth) - np.squeeze(clf.intercept_)\n",
    "years_working = (1+auth_years[:,1]-auth_years[:,0])\n",
    "value_scores = scores\n",
    "norm_scores = (value_scores)/years_working\n",
    "ri_filter_mat = scipy.sparse.diags(ri_confs)\n",
    "ri_scores = clf.predict(Xauth.dot(ri_filter_mat))-np.squeeze(clf.intercept_)\n",
    "ri_norm_scores = ri_scores/years_working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_cand = ['Pulkit Agrawal',\n",
    " 'Joydeep Biswas',\n",
    " 'Katherine L. Bouman',\n",
    " 'David Braun',\n",
    " 'Jia Deng',\n",
    " 'Naomi T. Fitter',\n",
    " 'David F. Fouhey',\n",
    " 'Saurabh Gupta',\n",
    " 'Judy Hoffman',\n",
    " 'Hanbyul Joo',\n",
    " 'Honglak Lee',\n",
    " 'Changliu Liu',\n",
    " 'Petter Nilsson',\n",
    " \"Matthew O'Toole\",\n",
    " 'Alessandro Roncone',\n",
    " 'Alanson P. Sample',\n",
    " 'Manolis Savva',\n",
    " 'Adriana Schulz',\n",
    " 'Amy Tabb',\n",
    " 'Fatma Zeynep Temel',\n",
    " 'Long Wang',\n",
    " 'Cathy Wu',\n",
    " 'Ling-Qi Yan']\n",
    "print('{:20s}\\t{:4s}\\t{:4s}\\t{:4s}\\t{}'.format('name','rate','total','ri','years'))\n",
    "for ns, name in sorted([(value_scores[name_idx[ni]],ni) for ni in prev_cand],reverse=True):\n",
    "    ni = name_idx[name]\n",
    "    print('{:20s}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.0f}'.format(name,100*norm_scores[ni],100*value_scores[ni],100*ri_scores[ni],years_working[ni]))\n",
    "print('')\n",
    "curious_names = ['Xiaolong Wang 0004','Judy Hoffman','Paris Siminelakis',\n",
    "                 'Nicholas Rhinehart',\n",
    "                 'Humphrey Hu',\n",
    "                 'David F. Fouhey',\n",
    "                 'Lerrel Pinto',\n",
    "                 'Justin Johnson',\n",
    "                 'Amir Roshan Zamir',\n",
    "                 'Brian Okorn','David Held']\n",
    "print('{:20s}\\t{:4s}\\t{:4s}\\t{:4s}\\t{}'.format('name','rate','total','ri','years'))\n",
    "for _,name in sorted([(value_scores[name_idx[_]],_) for _ in curious_names],reverse=True):\n",
    "    ni = name_idx[name]\n",
    "    print('{:20s}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.0f}'.format(name,100*norm_scores[ni],100*value_scores[ni],100*ri_scores[ni],years_working[ni]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n best overall \\n')\n",
    "cmu_scores = []\n",
    "\n",
    "best_scores = np.argsort(value_scores)[::-1]\n",
    "#print(best_scores.shape,unique_names[best_scores[0]])\n",
    "fa_list = list(faculty_affil.name)\n",
    "fa_a_list = list(faculty_affil.affiliation)\n",
    "uni_names = [unique_names[i] for i in best_scores[:38000]]\n",
    "for name in set([aliasdict.get(n, n) for n in uni_names]):\n",
    "    if name in name_idx:\n",
    "        uni = 'unknown'\n",
    "        if name in fa_list:\n",
    "            uni = fa_a_list[fa_list.index(name)]\n",
    "        if name not in []:#['Jacob Walker','Justin Johnson','Pieter Abbeel','Martial Hebert','Jessica K. Hodgins','Abhinav Gupta','Christopher G. Atkeson','Tom M. Mitchell','Matthew T. Mason']:\n",
    "            if years_working[name_idx[name]] < 3:\n",
    "                continue\n",
    "            if years_working[name_idx[name]] > 8:\n",
    "                continue\n",
    "            if ri_scores[name_idx[name]] < 0.008:\n",
    "                continue\n",
    "            if auth_years[name_idx[name],1] < 2017:\n",
    "                continue\n",
    "        #if (np.array(X[name_idx[name],:].todense()) * ri_confs).sum() == 0:\n",
    "        #    continue\n",
    "        #print(name,auth_years[name_idx[name]])\n",
    "        score = norm_scores[name_idx[name]]\n",
    "        ri_vscore = ri_norm_scores[name_idx[name]]\n",
    "        vscore = value_scores[name_idx[name]]\n",
    "        cmu_scores.append((vscore,ri_scores[name_idx[name]],score,uni,name,auth_years[name_idx[name]],ri_vscore))\n",
    "    else:\n",
    "        pass\n",
    "        #print(name)\n",
    "        ri_norm_scores\n",
    "print('{:22s}\\t{:15s}\\t{:5s}\\t{:3s}\\t{:4s}\\t{:4s}\\t{} {}'.format('name','uni','rate','RI-t','total','RI-r','start','end'))\n",
    "for vs,ris,s,u,p,yrs,rir in sorted(cmu_scores,reverse=True):\n",
    "    print('{:22s}\\t{:15s}\\t{:.3f}\\t{:.1f}\\t{:.2f}\\t{:.2f}\\t{} {}'.format(p[:22],u[:15],s*100,ris*100,vs*100,rir*100,int(yrs[0]),int(yrs[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_faculty = faculty_affil[faculty_affil.affiliation == 'Carnegie Mellon University'] #Carnegie Mellon University\n",
    "uni_names = np.array(uni_faculty.name)\n",
    "uni_names = list(uni_names) + ['Nicholas Rhinehart','Jacob Walker','Lerrel Pinto','Brian Okorn','Leonid Keselman','Siddharth Ancha','Humphrey Hu']\n",
    "cmu_scores = []\n",
    "#uni_names = [unique_names[i] for i in (np.argsort(scores)[::-1])[:150]]\n",
    "for name in set([aliasdict.get(n, n) for n in uni_names]):\n",
    "    if name in name_idx:\n",
    "        #if ri_scores[name_idx[name]] < 2.5:\n",
    "        #    continue\n",
    "        score = scores[name_idx[name]]\n",
    "        cmu_scores.append((score,name))\n",
    "    else:\n",
    "        pass\n",
    "        #print(name)\n",
    "for s,p in sorted(cmu_scores,reverse=True):\n",
    "    print('{:30s}\\t\\t{:.3f}'.format(p,s*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf.coef_,open('salary_indep.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Xauth,open('xauth.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
