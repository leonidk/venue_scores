{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from fuzzywuzzy import process\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csr = pd.read_csv('../faculty-affiliations.csv')\n",
    "csr_unis = list(df_csr.affiliation.unique())\n",
    "csr_names = list(df_csr.name.unique())\n",
    "csr_names_dict = {k:1 for k in csr_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2dict_str_str(fname):\n",
    "    with open(fname, mode='r') as infile:\n",
    "        rdr = csv.reader(infile)\n",
    "        d = {rows[0].strip(): rows[1].strip() for rows in rdr}\n",
    "    return d\n",
    "aliasdict = csv2dict_str_str('../dblp-aliases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profs = pd.read_excel('all_professors.xlsx',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_map = pd.read_csv('map_from_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profs_merged = pd.merge(df_profs,name_map,left_on='UniversityID',right_on='ID')\n",
    "df_profs_merged.school = df_profs_merged.school.str.strip()\n",
    "scr_unis = df_profs_merged.school.unique()\n",
    "new_uni = [n for n in scr_unis if n not in csr_unis]\n",
    "old_uni = [n for n in scr_unis if n  in csr_unis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(csr_unis),len(scr_unis),len(new_uni),len(old_uni),new_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uni in new_uni:\n",
    "    print(uni)#,'\\t',process.extractOne(uni,csr_unis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "row_alts = []\n",
    "csr_names = csr_names\n",
    "uni = 'Carnegie Mellon University'\n",
    "csr_names = list(df_csr[df_csr.affiliation == uni].name)\n",
    "print(len(csr_names),df_profs_merged[df_profs_merged.school == uni].shape[0])\n",
    "for row in df_profs_merged[df_profs_merged.school == uni].itertuples():\n",
    "    n = ' '.join([row[3],row[2]])\n",
    "    n = aliasdict.get(n,n)\n",
    "    matcher = fuzz.SequenceMatcher(None, fuzz._process_and_sort(n,False))\n",
    "    n3s = []\n",
    "    for n2 in csr_names:\n",
    "        matcher.set_seq2(fuzz._process_and_sort(n2,False))\n",
    "\n",
    "        n3s.append(matcher.ratio())\n",
    "    v=np.argmax(n3s)\n",
    "    print(fuzz._process_and_sort(n,False),csr_names[v],n3s[v])\n",
    "    #three_alts = process.extractBests(n,csr_names,limit=3)\n",
    "    #row_alts.append(three_alts)\n",
    "    #print(n,'\\t\\t\\t\\t\\t',three_alts[0],n in aliasdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "with gzip.open('../useful_authors_list.pkl.gz','rb') as fp:\n",
    "    all_authors = pickle.load(fp)\n",
    "with gzip.open('../useful_papers.pkl.gz','rb') as fp:\n",
    "    all_papers = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts = {}\n",
    "for paper in all_papers:\n",
    "    for a in paper[2]:\n",
    "        author_counts[a] = 1 + author_counts.get(a,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viable_authors = [a for a,c in author_counts.items() if c >= 10 ]\n",
    "len(viable_authors),len(all_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchable_names = [fuzz._process_and_sort(n,False) for n in viable_authors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_names = []\n",
    "for row in df_profs_merged.itertuples():\n",
    "    n = ' '.join([row[3],row[2]])\n",
    "    n = aliasdict.get(n,n)\n",
    "    matcher = fuzz.SequenceMatcher(None, fuzz._process_and_sort(n,False))\n",
    "    n3s = []\n",
    "    for n2 in matchable_names:\n",
    "        matcher.set_seq2(n2)\n",
    "        n3s.append(matcher.ratio())\n",
    "    v=np.argmax(n3s)\n",
    "    res = (n,viable_authors[v],n3s[v])\n",
    "    matched_names.append(res)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profs_merged['First Name']= df_profs_merged['First Name'].fillna('')\n",
    "df_profs_merged['Last Name']= df_profs_merged['Last Name'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_author_min = [unidecode.unidecode(n).lower().strip().replace('.','').replace('(','').replace(')','').replace('-','') for n in viable_authors]\n",
    "xcl_author_min = [unidecode.unidecode(' '.join([row[3],row[2]])).lower().strip().replace('.','').replace('(','').replace(')','').replace('-','') for row in df_profs_merged.itertuples()]\n",
    "xcl_author_max = [' '.join([row[3],row[2]]) for row in df_profs_merged.itertuples()]\n",
    "\n",
    "all_author_min_set = {k:i for i,k in enumerate(all_author_min)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcl_nomid = {}\n",
    "xcl_nomidl = []\n",
    "for n in xcl_author_min:\n",
    "    ns = n.split(' ')\n",
    "    if len(ns) > 2:\n",
    "        n = ns[0] + ' ' + ns[-1]\n",
    "    xcl_nomid[n] = 1 + xcl_nomid.get(n,0)\n",
    "    xcl_nomidl.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nomid = {}\n",
    "for i,n in enumerate(all_author_min):\n",
    "    ns = n.split(' ')\n",
    "    if len(ns) > 2:\n",
    "        nn = ns[0] + ' ' + ns[-1]\n",
    "    else:\n",
    "        nn = n\n",
    "    on = all_nomid.get(nn,[])\n",
    "    if on is None:\n",
    "        on = []\n",
    "    all_nomid[nn] =  on + [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_names = []\n",
    "exists = []\n",
    "for idx, name in enumerate(xcl_nomidl):\n",
    "    new_name = df_profs_merged.loc[idx,'First Name'] + ' ' + df_profs_merged.loc[idx,'Last Name']\n",
    "\n",
    "    if df_profs_merged.loc[idx,'Full Name'].lower().strip() in all_author_min_set:\n",
    "        new_name = viable_authors[all_author_min.index(df_profs_merged.loc[idx,'Full Name'].lower().strip())]\n",
    "        exists.append(1)\n",
    "    elif xcl_nomid[name] == 1 and name in all_nomid and len(all_nomid[name]) == 1:\n",
    "        new_name = viable_authors[all_nomid[name][0]]\n",
    "        exists.append(1)\n",
    "    elif xcl_nomid[name] == 1 and xcl_author_min[idx] in all_author_min_set:\n",
    "        new_name = viable_authors[all_author_min.index(xcl_author_min[idx])]\n",
    "        exists.append(1)\n",
    "    else:\n",
    "        #if name in all_nomid:\n",
    "        #    counts = [(viable_authors[idx],author_counts[viable_authors[idx]]) for idx in all_nomid[name]]\n",
    "        #    res = process.extractWithoutOrder(xcl_author_max[idx],[viable_authors[idx] for idx in all_nomid[name]])\n",
    "        #    #print(xcl_author_max[idx],counts,list(res))\n",
    "        #else:\n",
    "        #    full_name = xcl_author_max[idx]\n",
    "        #    matcher = fuzz.SequenceMatcher(None, fuzz._process_and_sort(full_name,False))\n",
    "        #    n3s = []\n",
    "        #    for n2 in matchable_names:\n",
    "        #        matcher.set_seq2(n2)\n",
    "        #        n3s.append(matcher.ratio())\n",
    "        #    v=np.argmax(n3s)\n",
    "        #    res = (full_name,viable_authors[v],n3s[v])\n",
    "        #    #print(res)        \n",
    "        exists.append(0)\n",
    "\n",
    "    merged_names.append(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(exists),len(exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'yang richard yang' in all_author_min_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_num2 = []\n",
    "matched_names2 = []\n",
    "for n,i in zip(merged_names,exists):\n",
    "    print(n,i)\n",
    "    n2 = n\n",
    "    if i == 0:\n",
    "        matcher = fuzz.SequenceMatcher(None, fuzz._process_and_sort(n,False))\n",
    "        n3s = []\n",
    "        for n2 in matchable_names:\n",
    "            matcher.set_seq2(n2)\n",
    "            n3s.append(matcher.ratio())\n",
    "            #atcher.\n",
    "        v=np.argmax(n3s)\n",
    "        res = (n,viable_authors[v],n3s[v])\n",
    "        print(res)\n",
    "        n2 = res[-2]\n",
    "        matched_num2.append(res[-1])\n",
    "    else:\n",
    "        matched_num2.append(0)\n",
    "    matched_names2.append(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profs_merged[(df_profs_merged.fuzzyscore >= 0.96) & (df_profs_merged.fuzzyscore < 1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profs_merged['dblpname'] = merged_names\n",
    "df_profs_merged['dblpexists'] = exists\n",
    "df_profs_merged['fuzzyscore'] = matched_num2\n",
    "df_profs_merged['fuzzyname'] = matched_names2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftfy.fix_text(str(df_profs_merged.loc[50,'Full Name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profs_merged.to_csv('../faculty_affil_scholar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uni in new_uni:\n",
    "    for row in df_profs_merged[(df_profs_merged.school == uni) & (df_profs_merged.dblpexists == 1)].itertuples():\n",
    "        print(row[-2],row[-3],'NOSCHOLARPAGE' if str(row[4]) == 'nan' else row[4][row[4].index('AAAAJ')-7:row[4].index('AAAAJ')+5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_names =[aliasdict.get(a,a) for a in list( df_csr.name)]\n",
    "matchable_names = [fuzz._process_and_sort(n,False) for n in old_names]\n",
    "\n",
    "for uni in old_uni:\n",
    "    for row in df_profs_merged[(df_profs_merged.school == uni) & (df_profs_merged.dblpexists == 1)].itertuples():\n",
    "            n = aliasdict.get(row[-2],row[-2])\n",
    "            try:\n",
    "                if n not in old_names:\n",
    "                   # matcher = fuzz.SequenceMatcher(None, fuzz._process_and_sort(n,False))\n",
    "                    #n3s = []\n",
    "                    #for n2 in matchable_names:\n",
    "                    #    matcher.set_seq2(n2)\n",
    "                   #    n3s.append(matcher.quick_ratio())\n",
    "                        #atcher.\n",
    "                    #v=np.argmax(n3s)\n",
    "                    #res = (n,all_authors[v],n3s[v])\n",
    "                    res = process.extractOne(n,old_names)\n",
    "\n",
    "                    if(res[-1] < 95):\n",
    "                        print(n,res)\n",
    "                        print(n,row[-3],'NOSCHOLARPAGE' if str(row[4]) == 'nan' else row[4][row[4].index('AAAAJ')-7:row[4].index('AAAAJ')+5] )\n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profs_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, urllib\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import urllib\n",
    "import requests\n",
    "\n",
    "f = open(\"more2.csv\", \"at\")\n",
    "\n",
    "for uni in old_uni:\n",
    "    for row in df_profs_merged[(df_profs_merged.school == uni) & (df_profs_merged.dblpexists == 1)].itertuples():\n",
    "            n = aliasdict.get(row[-2],row[-2])\n",
    "            try:\n",
    "                if n not in old_names:\n",
    "                    res = process.extractOne(n,old_names)\n",
    "\n",
    "                    if(res[-1] < 95):\n",
    "                        query = (row[-2] + ' ' + row[-3]).replace(' ','+')\n",
    "                        results = requests.get(\"http://www.bing.com/search\", \n",
    "                                      params={'q': query, 'first': 0}, \n",
    "                                      headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 Chrome/65.0.3325.162 Safari/537.36'})\n",
    "                        soup = BeautifulSoup(results.text)\n",
    "\n",
    "\n",
    "                        res1 = soup.findAll('li', {\"class\" : \"b_algo\" })\n",
    "                        res2 = [li.find('a') for li in res1]\n",
    "                        res3 = [link.get('href') for link in res2]#[0]\n",
    "                        url = res3[0]\n",
    "                        res4 = [link for link in res3 if 'edu' in link]\n",
    "                        if len(res4) > 0:\n",
    "                            url = res4[0]\n",
    "\n",
    "                        new_entry = [row[-2],row[-3],url,'NOSCHOLARPAGE' if str(row[4]) == 'nan' else row[4][row[4].index('AAAAJ')-7:row[4].index('AAAAJ')+5]]\n",
    "                        print(','.join(new_entry))\n",
    "                        f.write(','.join(new_entry))\n",
    "                        f.write('\\n')\n",
    "                        time.sleep(1)\n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except:\n",
    "                pass\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, urllib\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import urllib\n",
    "f = open(\"more.csv\", \"at\")\n",
    "\n",
    "\n",
    "for uni in new_uni:\n",
    "    for row in df_profs_merged[(df_profs_merged.school == uni) & (df_profs_merged.dblpexists == 1)].itertuples():\n",
    "        #query = (row[-2] + ' ' + row[-3]).replace(\" \", \"%20\")\n",
    "        import requests\n",
    "        query = (row[-2] + ' ' + row[-3]).replace(' ','+')\n",
    "        results = requests.get(\"http://www.bing.com/search\", \n",
    "                      params={'q': query, 'first': 0}, \n",
    "                      headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 Chrome/65.0.3325.162 Safari/537.36'})\n",
    "        soup = BeautifulSoup(results.text)\n",
    "\n",
    "\n",
    "        res1 = soup.findAll('li', {\"class\" : \"b_algo\" })\n",
    "        res2 = [li.find('a') for li in res1]\n",
    "        res3 = [link.get('href') for link in res2]#[0]\n",
    "        url = res3[0]\n",
    "        res4 = [link for link in res3 if 'edu' in link]\n",
    "        if len(res4) > 0:\n",
    "            url = res4[0]\n",
    "            \n",
    "        new_entry = [row[-2],row[-3],url,'NOSCHOLARPAGE' if str(row[4]) == 'nan' else row[4][row[4].index('AAAAJ')-7:row[4].index('AAAAJ')+5]]\n",
    "        print(','.join(new_entry))\n",
    "        f.write(','.join(new_entry))\n",
    "        time.sleep(1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"more.csv\", \"r\").read()\n",
    "def grouplen(sequence, chunk_size):\n",
    "    return list(zip(*[iter(sequence)] * chunk_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = open('evenmore.csv','wt')\n",
    "f2.write('\\n'.join([','.join(a) for a in grouplen(f.split(','),4)]))\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouplen(f.split(','),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('li', {\"class\" : \"b_algo\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
