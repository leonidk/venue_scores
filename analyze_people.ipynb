{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import csv\n",
    "import scipy.sparse\n",
    "Xauth = None\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the update to work despite the broken scipy documentation\n",
    "try:\n",
    "    a = scipy.sparse.dok_matrix((10,10))\n",
    "    a.update({(0,0):1.0})\n",
    "    scipy.sparse.dok_matrix.my_update = scipy.sparse.dok_matrix.update\n",
    "except:\n",
    "    a = scipy.sparse.dok_matrix((10,10))\n",
    "    a._update({(0,0):1.0})\n",
    "    scipy.sparse.dok_matrix.my_update = scipy.sparse.dok_matrix._update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('useful_venue_list.pkl.gz','rb') as fp:\n",
    "    all_venues = pickle.load(fp)\n",
    "with gzip.open('useful_authors_list.pkl.gz','rb') as fp:\n",
    "    all_authors = pickle.load(fp)\n",
    "with gzip.open('useful_papers.pkl.gz','rb') as fp:\n",
    "    all_papers = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_idx = {v:i for i,v in enumerate(all_venues)}\n",
    "name_idx = {v:i for i,v in enumerate(all_authors)}\n",
    "n_confs = len(all_venues)\n",
    "n_auths = len(all_authors)\n",
    "print(n_confs,n_auths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_affil = pd.read_csv('faculty-affiliations.csv')\n",
    "ranks = pd.read_csv('other_ranks/ranks.csv')\n",
    "def csv2dict_str_str(fname):\n",
    "    with open(fname, mode='r') as infile:\n",
    "        rdr = csv.reader(infile)\n",
    "        d = {rows[0].strip(): rows[1].strip() for rows in rdr}\n",
    "    return d\n",
    "aliasdict = csv2dict_str_str('dblp-aliases-expanded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year = all_papers[0][6]\n",
    "max_year = all_papers[-1][6]\n",
    "span_years = max_year - min_year + 1\n",
    "print(min_year,max_year,span_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curious_names = [\n",
    "    'Aditya Dhawale',\n",
    "    'Tesca Fitzgerald',\n",
    "    'Adam W. Harley',\n",
    "    \"Xiaolong Wang 0004\",\n",
    "    \"Judy Hoffman\",\n",
    "    \"Paris Siminelakis\",\n",
    "    \"Roie Levin\",\n",
    "    \"Leonid Keselman\",\n",
    "    \"Rick Goldstein\",\n",
    "    \"Nicholas Rhinehart\",\n",
    "    \"Vincent Sitzmann\",\n",
    "    \"Siddharth Ancha\",\n",
    "    \"Xingyu Lin\",\n",
    "    \"Humphrey Hu\",\n",
    "    \"David F. Fouhey\",\n",
    "    \"Chelsea Finn\",\n",
    "    \"Dinesh Jayaraman\",\n",
    "    \"Wen Sun 0002\",\n",
    "    \"Lerrel Pinto\",\n",
    "    \"Justin Johnson 0001\",\n",
    "    \"Amir Zamir\",\n",
    "    \"Dominik Peters\",\n",
    "    \"Jonathan T. Barron\",\n",
    "    \"Dorsa Sadigh\",\n",
    "    \"Derek Hoiem\",\n",
    "    \"Vaggos Chatziafratis\",\n",
    "    \"Brian Okorn\",\n",
    "    \"David Held\"\n",
    "]\n",
    "\n",
    "curious_names2 = ['Pulkit Agrawal',\n",
    " 'Joydeep Biswas',\n",
    " 'Katherine L. Bouman',\n",
    " 'David Braun',\n",
    " 'Jia Deng',\n",
    " 'Naomi T. Fitter',\n",
    " 'David F. Fouhey',\n",
    " 'Saurabh Gupta',\n",
    " 'Judy Hoffman',\n",
    " 'Hanbyul Joo',\n",
    " 'Honglak Lee',\n",
    " 'Changliu Liu',\n",
    " 'Petter Nilsson',\n",
    " \"Matthew O'Toole\",\n",
    " 'Alessandro Roncone',\n",
    " 'Alanson P. Sample',\n",
    " 'Manolis Savva',\n",
    " 'Adriana Schulz',\n",
    " 'Amy Tabb',\n",
    " 'Fatma Zeynep Temel',\n",
    " 'Long Wang',\n",
    " 'Ling-Qi Yan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri_names = list(set([aliasdict.get(row[1],row[1]) for row in pd.read_csv('other_ranks/cmu_faculty.csv').itertuples() if row[2] == 'RI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreV = None\n",
    "try:\n",
    "    import gzip\n",
    "    import pickle\n",
    "    with gzip.open('scoresV2.pkl.gz','rb') as fp:\n",
    "        scoreV = pickle.load(fp)\n",
    "except:\n",
    "    print('failed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_types = {\n",
    "        'RI': [_[1] for _ in sorted(ri_fav_confs,reverse=True) if _[-2] >= 1.25],\n",
    "        'ML':['NIPS','ICML','AAAI','AISTATS','IJCAI','UAI','CoRL','ICLR'],\n",
    "        'CV':['CVPR','ICCV','ECCV','IEEE Trans. Pattern Anal. Mach. Intell.','FGR','Int. J. Comput. Vis.','WACV','BMVC'],\n",
    "        'ROB':['HRI','Int. J. Robotics Res.','Robotics: Science and Systems','Humanoids','WAFR','IROS','ICRA','FSR','ISER','ISRR','AAMAS','IEEE Robotics Autom. Lett.','IEEE Trans. Robotics and Automation'],\n",
    "        'GR':['ACM Trans. Graph.','Comput. Graph. Forum','SIGGRAPH','SIGGRAPH Asia','Symposium on Computer Animation','AISTATS','IJCAI','UAI'],\n",
    "             }\n",
    "sTypes = ['Full'] + [k for k,v in conf_types.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in ['ROB','CV','GR','ML']:\n",
    "    print(scoreV['1/i'][:,sTypes.index(sub)].sum(1).shape)\n",
    "print(scoreV['1/i'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_paper_year = 2005\n",
    "last_paper_year = 2019\n",
    "min_pub_years = 2\n",
    "avg_auth_pos2 = avg_auth_pos[idx]/avg_auth_cnt[idx]\n",
    "condition = (auth_years[:,0] >= first_paper_year).astype(np.float)\n",
    "condition = (((auth_been_last < 2) )& (auth_years[:,0] >= first_paper_year) & (auth_years[:,1] >= last_paper_year) ) .astype(np.float)\n",
    "\n",
    "cand_total = total_scores * condition* (year_span >= min_pub_years).astype(np.float) \n",
    "\n",
    "cand_ri = ri_scores* condition* (year_span >= min_pub_years).astype(np.float) \n",
    "\n",
    "cand_total_ef = cand_total/year_span\n",
    "cand_ri_ef = cand_ri/year_span\n",
    "cols_l = ['Rank','Author','RI','Total','eRI','eTotal','Since','Until','Last','Affiliation']\n",
    "print('{}\\t{:30s}\\t{:s}\\t{:s}\\t{:s}\\t{:s}\\t{:s}\\t{:s}\\t{:s}\\t{:20s}'.format(*cols_l))\n",
    "export_list = []\n",
    "for num,idx in enumerate(np.argsort(cand_ri)[::-1][:50000]):\n",
    "    uni = faculty_lookup.get(all_authors[idx],'None')\n",
    "    data_l = [num+1,all_authors[idx],\n",
    "              cand_ri[idx],cand_total[idx],\n",
    "              cand_ri_ef[idx],cand_total_ef[idx],\n",
    "              auth_years[idx,0],auth_years[idx,1],auth_been_last[idx],uni]\n",
    "    export_list.append(data_l)\n",
    "\n",
    "    if num < 100:\n",
    "        print('{}\\t{:30s}\\t{:.1f}\\t{:.1f}\\t{:.1f}\\t{:.1f}\\t{:.0f}\\t{:.0f}\\t{:.0f}\\t{:20s}'.format(*data_l))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(export_list,columns=cols_l).iloc[:50000].to_csv('pot_export.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_papers = np.zeros(years_per_conf*n_confs)\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    yr = (year-min_year)//YEAR_BLOCKS\n",
    "    j = years_per_conf*conf_idx[venue] + yr\n",
    "    count_of_papers[j] += 1\n",
    "# safe divide\n",
    "count_of_papers = np.maximum(1,count_of_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_papers2 = count_of_papers.reshape((-1,years_per_conf))\n",
    "masked_count = (count_of_papers2 * (count_of_papers2 > 1))\n",
    "how_many_valid_years = (masked_count > 1).sum(1)\n",
    "conf_sizes = masked_count.sum(1)/how_many_valid_years\n",
    "valid_confs_for_vis = (np.isfinite(conf_sizes)) & (conf_sizes > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_confs_for_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_scores = clf_gold.reshape((-1,years_per_conf))[:,-3:].copy()\n",
    "ranks = np.nan_to_num(np.true_divide(tmp_scores.sum(1),(tmp_scores!=0).sum(1)))\n",
    "#ranks = clf_gold.reshape((-1,years_per_conf))[:,-3:].max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_names = {'Theory of Computing, Graduate Surveys' : 'Theory of Computing, GS',\n",
    "               'IEEE Symposium on Security and Privacy':'IEEE SSP',\n",
    "              'IEEE Conference on Computational Complexity': 'IEEE CCC',\n",
    "              'USENIX Annual Technical Conference': 'USENIX ATC',\n",
    "              'Journal of Machine Learning Research': 'JMLR',\n",
    "              'Computer Communication Review' : 'SIGCOMM CCR'}\n",
    "things = []\n",
    "for idx in np.argsort(ranks)[::-1]:#[:500]:\n",
    "    name = short_names[all_venues[idx]] if all_venues[idx] in short_names else all_venues[idx]\n",
    "    paper_count = conf_sizes[idx]\n",
    "    if not valid_confs_for_vis[idx]:\n",
    "        continue\n",
    "    if ranks[idx] < 0.1:\n",
    "        continue\n",
    "    things.append((name,ranks[idx],conf_sizes[idx]))\n",
    "    print('{:30s}\\t{:.1f}\\t{:.1f}'.format(name,ranks[idx],conf_sizes[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(things,columns=['Name','Score','Size']).set_index('Name').to_csv('confs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks[conf_idx['STOC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs cluster_new to be run. sorry\n",
    "labels = np.load('labels.npy')\n",
    "mask = np.load('mask.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs cluster_new to be run. sorry\n",
    "cluster_ranks = clf_gold.reshape((-1,years_per_conf))[mask,:]\n",
    "small_names = np.array([all_venues[i] for i in range(n_confs) if mask[i]])\n",
    "alias_dict = conf_fix#csv2dict_str_str('name_alias.csv')\n",
    "group_names = csv2dict_str_str('group_names.csv')\n",
    "group_names = {k:v for k,v in group_names.items() if v != ''}\n",
    "valid_confs_for_small_vis = valid_confs_for_vis[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = labels.max() + 1\n",
    "small_conf_counts = np.zeros(NUM_CLUSTERS)\n",
    "\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cluster_names = small_names[labels == i]\n",
    "    for conf in cluster_names:\n",
    "        #clf_gold[years_per_conf*conf_idx[conf]:years_per_conf*(1+conf_idx[conf])].max() *\n",
    "        small_conf_counts[i] +=  count_of_papers[years_per_conf*conf_idx[conf]:years_per_conf*(1+conf_idx[conf])].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ROWS = int(np.ceil(NUM_CLUSTERS/4))\n",
    "plt.figure(figsize=(4*ROWS,32))\n",
    "names_to_save = []\n",
    "\n",
    "\n",
    "\n",
    "for cnti, i in enumerate(np.argsort(small_conf_counts)[::-1]):\n",
    "    plt.subplot(ROWS,4,cnti+1)\n",
    "    cluster_names = small_names[labels == i]\n",
    "    cluster_filtered = cluster_ranks[labels == i] * valid_confs_for_small_vis[labels == i].reshape((-1,1))\n",
    "    \n",
    "    cluster_scores = cluster_filtered[:,-10:].max(1)\n",
    "    valid_idxs = np.argsort(cluster_filtered[:,-4:-1].mean(1))[::-1]\n",
    "    has_name = False\n",
    "    for idx in valid_idxs[:6]:\n",
    "        name_for_now = cluster_names[idx]\n",
    "        if not has_name and name_for_now in group_names:\n",
    "            plt.title(group_names[name_for_now])\n",
    "            has_name = True\n",
    "            #print(group_names[name_for_now],cnti,small_conf_counts[i])\n",
    "        name_for_now = alias_dict[name_for_now] if name_for_now in alias_dict else name_for_now\n",
    "        line = [np.nan if _ <= 0 else _ for _ in cluster_filtered[idx]]\n",
    "        s = [_ for _ in cluster_filtered[idx] if _ >=0][-1]\n",
    "        plt.plot(min_year+np.arange(years_per_conf)*YEAR_BLOCKS,line,label='{:15s} ({:.1f})'.format(name_for_now[:15],s))\n",
    "        names_to_save.append(cluster_names[idx])\n",
    "        #print(name_for_now,i)\n",
    "    plt.legend(loc=3,fancybox=True, framealpha=0.8, borderpad=1,frameon=True,prop={'stretch':0,'family': 'monospace'})\n",
    "    plt.tight_layout()\n",
    "    #plt.ylim(0,13)\n",
    "    plt.ylim(bottom=0.05)\n",
    "    plt.xlim(1970,2020)\n",
    "plt.savefig('big_plot7.pdf',facecolor='w',edgecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in [_ for _ in names_to_save if _ not in conf_fix and len(_) > 8]:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
