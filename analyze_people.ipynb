{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import csv\n",
    "import scipy.sparse\n",
    "Xauth = None\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the update to work despite the broken scipy documentation\n",
    "try:\n",
    "    a = scipy.sparse.dok_matrix((10,10))\n",
    "    a.update({(0,0):1.0})\n",
    "    scipy.sparse.dok_matrix.my_update = scipy.sparse.dok_matrix.update\n",
    "except:\n",
    "    a = scipy.sparse.dok_matrix((10,10))\n",
    "    a._update({(0,0):1.0})\n",
    "    scipy.sparse.dok_matrix.my_update = scipy.sparse.dok_matrix._update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('useful_venue_list.pkl.gz','rb') as fp:\n",
    "    all_venues = pickle.load(fp)\n",
    "with gzip.open('useful_authors_list.pkl.gz','rb') as fp:\n",
    "    all_authors = pickle.load(fp)\n",
    "with gzip.open('useful_papers.pkl.gz','rb') as fp:\n",
    "    all_papers = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_idx = {v:i for i,v in enumerate(all_venues)}\n",
    "name_idx = {v:i for i,v in enumerate(all_authors)}\n",
    "n_confs = len(all_venues)\n",
    "n_auths = len(all_authors)\n",
    "print(n_confs,n_auths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_affil = pd.read_csv('faculty-affiliations.csv')\n",
    "ranks = pd.read_csv('other_ranks/ranks.csv')\n",
    "def csv2dict_str_str(fname):\n",
    "    with open(fname, mode='r') as infile:\n",
    "        rdr = csv.reader(infile)\n",
    "        d = {rows[0].strip(): rows[1].strip() for rows in rdr}\n",
    "    return d\n",
    "aliasdict = csv2dict_str_str('dblp-aliases-expanded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year = all_papers[0][6]\n",
    "max_year = all_papers[-1][6]\n",
    "span_years = max_year - min_year + 1\n",
    "print(min_year,max_year,span_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreV = None\n",
    "try:\n",
    "    import gzip\n",
    "    import pickle\n",
    "    with gzip.open('scoresV2.pkl.gz','rb') as fp:\n",
    "        scoreV = pickle.load(fp)\n",
    "except:\n",
    "    print('failed!')\n",
    "    \n",
    "clf =  np.load('clf_gold.pkl.npy')\n",
    "years_per_conf = clf.shape[0]//n_confs\n",
    "YEAR_BLOCKS = span_years//years_per_conf\n",
    "pr_full = pickle.load(open('new_pagerank_people.pkl','rb'))\n",
    "faculty_lookup = {_[1]:_[2] for _ in faculty_affil.itertuples()}\n",
    "auth_years = scoreV['auth_years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in scoreV:\n",
    "    if 'apm' in k:\n",
    "        s =  scoreV[k] \n",
    "        scoreV[k] = (s-s.mean())/s.std() #1-(np.argsort(s)/(len(s)-1))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curious_names = [\n",
    "    'Aditya Dhawale',\n",
    "    'Tesca Fitzgerald',\n",
    "    'Adam W. Harley',\n",
    "    \"Xiaolong Wang 0004\",\n",
    "    \"Judy Hoffman\",\n",
    "    \"Paris Siminelakis\",\n",
    "    \"Roie Levin\",\n",
    "    \"Leonid Keselman\",\n",
    "    \"Rick Goldstein\",\n",
    "    \"Nicholas Rhinehart\",\n",
    "    \"Vincent Sitzmann\",\n",
    "    \"Siddharth Ancha\",\n",
    "    \"Xingyu Lin\",\n",
    "    \"Humphrey Hu\",\n",
    "    \"David F. Fouhey\",\n",
    "    \"Chelsea Finn\",\n",
    "    \"Dinesh Jayaraman\",\n",
    "    \"Wen Sun 0002\",\n",
    "    \"Lerrel Pinto\",\n",
    "    \"Justin Johnson 0001\",\n",
    "    \"Amir Zamir\",\n",
    "    \"Dominik Peters\",\n",
    "    \"Jonathan T. Barron\",\n",
    "    \"Dorsa Sadigh\",\n",
    "    \"Derek Hoiem\",\n",
    "    \"Vaggos Chatziafratis\",\n",
    "    \"Brian Okorn\",\n",
    "    \"David Held\"\n",
    "]\n",
    "\n",
    "curious_names2 = ['Pulkit Agrawal',\n",
    " 'Joydeep Biswas',\n",
    " 'Katherine L. Bouman',\n",
    " 'David Braun',\n",
    " 'Jia Deng',\n",
    " 'Naomi T. Fitter',\n",
    " 'David F. Fouhey',\n",
    " 'Saurabh Gupta',\n",
    " 'Judy Hoffman',\n",
    " 'Hanbyul Joo',\n",
    " 'Honglak Lee',\n",
    " 'Changliu Liu',\n",
    " 'Petter Nilsson',\n",
    " \"Matthew O'Toole\",\n",
    " 'Alessandro Roncone',\n",
    " 'Alanson P. Sample',\n",
    " 'Manolis Savva',\n",
    " 'Adriana Schulz',\n",
    " 'Amy Tabb',\n",
    " 'Fatma Zeynep Temel',\n",
    " 'Long Wang',\n",
    " 'Ling-Qi Yan']\n",
    "\n",
    "curious_names = ['Xiaolong Wang 0004','Judy Hoffman','Paris Siminelakis','Roie Levin','Leonid Keselman',\n",
    "                 'Nicholas Rhinehart','Vincent Sitzmann','Siddharth Ancha','Xingyu Lin',\n",
    "                 'Humphrey Hu','Aditya Dhawale','Nick Gisolfi','Andrey Kurenkov',\n",
    "                 'David F. Fouhey','Chelsea Finn','Akshara Rai','Ankit Bhatia','Adam W. Harley',\n",
    "                 'Lerrel Pinto','Graeme Best','Alexander Spitzer','Roberto Shu','Amir Abboud',\n",
    "                 'Justin Johnson 0001','Kumar Shaurya Shankar','Ellen A. Cappo',\n",
    "                 'Amir Zamir','Dominik Peters','Jonathan T. Barron','Dorsa Sadigh','Derek Hoiem','Vaggos Chatziafratis',\n",
    "                 'Brian Okorn','David Held','Jonathon Luiten','Mark Sheinin','Abhinav Gupta 0001','Nadine Chang',\n",
    "                'Allison Del Giorno']\n",
    "\n",
    "curious_names = ['Antonio Torralba 0001','Wojciech Matusik','Daniela Rus',\n",
    "                 'Mina Konakovic-Lukovic','Vivienne Sze','Sara Beery','Yufeng Chen 0003','Leonid Keselman',\n",
    "                 'Adam W. Harley','Pulkit Agrawal','Regina Barzilay','Vincent Sitzmann',\n",
    "                'John V. Guttag','Polina Golland','Stefanie Jegelka','Phillip Isola','Leslie Pack Kaelbling',\n",
    "                 'Dina Katabi','John J. Leonard','Tomás Lozano-Pérez','Stefanie Müller 0001','Nicholas Roy',\n",
    "                'Julie A. Shah','Justin Solomon 0001','Russ Tedrake','Brian C. Williams','Song Han 0003']\n",
    "\n",
    "interesting_set = set(curious_names)\n",
    "\n",
    "ri_names = list(set([aliasdict.get(row[1],row[1]) for row in pd.read_csv('other_ranks/cmu_faculty.csv').itertuples() if row[2] == 'RI']))\n",
    "#interesting_set = set(ri_names + ['Leonid Keselman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "author_results = defaultdict(dict)\n",
    "for name in interesting_set:\n",
    "    idx = name_idx[name]\n",
    "    author_results[name]['Affiliation'] = faculty_lookup.get(name,'Unknown')\n",
    "    author_results[name]['Years'] = scoreV['working_years'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_years = (auth_years[:,1] - auth_years[:,0]+1)\n",
    "\n",
    "v = scoreV['1/i_total_1970']/(np.maximum(2,working_years.astype(np.float32)))\n",
    "ratio_v = np.maximum(1e-3,scoreV['1/i_RI_1970'])/np.maximum(1e-3,scoreV['1/i_total_1970'])\n",
    "v *= np.sqrt(ratio_v)\n",
    "v *= 1/np.log(np.maximum(2,working_years.astype(np.float32)))\n",
    "tv = np.zeros_like(scoreV['1/i_total_1970'])\n",
    "for n in ['_apmFalse', '_apmTrue',  'pw_apmFalse', 'pw_apmTrue', 'pweff_apmFalse', 'pweff_apmTrue', 'pwunk_apmFalse','pwunk_apmTrue']:\n",
    "    tv +=  scoreV[n]\n",
    "v *= np.log(np.maximum(tv,10))\n",
    "v *= np.log(scoreV['1/i_max_yr']-1965)\n",
    "v = np.nan_to_num(v)\n",
    "meta_metric = v\n",
    "def softmin(x,temp=1):\n",
    "    x = np.array(x)\n",
    "    v = -x\n",
    "    v = v - v.min()\n",
    "    v = np.exp(v/temp)\n",
    "    return (v/v.sum() * x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs_of_interest = ['ICML','CoRL','L4DC','NeurIPS','ECCV','ICLR','MLSys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentence_embeddings = model.encode(['Wow'],show_progress_bar=False)\n",
    "model_dim = sentence_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coauthors  = defaultdict(lambda: defaultdict(int))\n",
    "coauthors_frac  = defaultdict(lambda: defaultdict(float))\n",
    "coauthors_num  = defaultdict(list)\n",
    "author_pos  = defaultdict(list)\n",
    "conf_paper_frac  =  defaultdict(lambda: defaultdict(int))\n",
    "paper_num  = defaultdict(int)\n",
    "paper_frac  = defaultdict(float)\n",
    "conf_interest_cnt = defaultdict(float)\n",
    "count_vecs = {}\n",
    "sent_set = defaultdict(list)\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    n = len(authors)\n",
    "    parse = False\n",
    "    for a in authors:\n",
    "        if a in interesting_set:\n",
    "            parse = True\n",
    "            break\n",
    "    if parse:\n",
    "\n",
    "        if n not in count_vecs:\n",
    "            author_scores = 1/(np.arange(n)+1) \n",
    "            author_score_sum = author_scores.sum()\n",
    "            count_vecs[n] = author_scores / author_score_sum\n",
    "\n",
    "            \n",
    "        for i,a in enumerate(authors):\n",
    "            if a in interesting_set:\n",
    "                \n",
    "                conf_paper_frac[a][venue] += 1/n\n",
    "                coauthors_num[a].append(n)\n",
    "                \n",
    "                sent_set['They wrote a research paper with the title: ' + title].append((a,count_vecs[n][i]))\n",
    "                \n",
    "                if venue in confs_of_interest:\n",
    "                    conf_interest_cnt[a] += 1/n\n",
    "                author_pos[a].append((i+1)/n if n > 1 else 0)\n",
    "                paper_num[a] += 1\n",
    "                paper_frac[a] += 1/n\n",
    "                for a2 in authors:\n",
    "                    if a2 == a:\n",
    "                        continue\n",
    "                    coauthors[a][a2] += 1\n",
    "                    coauthors_frac[a][a2] += 1/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tset = list(sent_set.keys())\n",
    "sentence_embeddings = model.encode(sent_tset,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_emb = defaultdict(lambda: np.zeros(model_dim))\n",
    "auth_emb_cnt = defaultdict(int)\n",
    "for sent,emb in zip(sent_tset,sentence_embeddings):\n",
    "    for a,emb_w in sent_set[sent]:\n",
    "        auth_emb[a] += emb_w*emb\n",
    "        auth_emb_cnt[a] +=emb_w  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_emb_nrm = {k:auth_emb[k]/auth_emb_cnt[k] for k in auth_emb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_emb_k = sorted(auth_emb_nrm.keys())\n",
    "auth_emb_v = np.array([auth_emb_nrm[k] for k in auth_emb_k])\n",
    "\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "#import matplotlib.pyplot as plt\n",
    "auth_emb_X = squareform(pdist(auth_emb_v,'cosine'))\n",
    "target_name = 'Leonid Keselman'\n",
    "target_dist = {k:v for k,v in zip(auth_emb_k,auth_emb_X[auth_emb_k.index(target_name)])}\n",
    "sorted([(v,k) for k,v in zip(auth_emb_k,auth_emb_X[auth_emb_k.index(target_name)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in interesting_set:\n",
    "    idx = name_idx[name]\n",
    "    author_results[name]['MetaMetric'] = meta_metric[idx]\n",
    "    author_results[name]['Score (1/pos)'] = scoreV['1/i_total_1970'][idx]\n",
    "    \n",
    "    author_results[name]['RIScore'] = scoreV['1/i_RI_1970'][idx]\n",
    "    author_results[name]['RI %'] = scoreV['1/i_RI_1970'][idx]/scoreV['1/i_total_1970'][idx]\n",
    "    author_results[name]['ConfI'] = conf_interest_cnt[name]\n",
    "    author_results[name]['TDist'] = target_dist[name]\n",
    "    sum_v = 0\n",
    "    for sub in ['ROB','CV','GR','ML']:\n",
    "        den = scoreV['1/i_{}_1970'.format(sub)][idx]\n",
    "        den = den if den != 0.0 else 0\n",
    "        author_results[name][sub + ' %'] = den/max(1e-9,scoreV['1/i_total_1970'][idx])\n",
    "        sum_v += den\n",
    "    author_results[name]['Other %'] = max(0,scoreV['1/i_total_1970'][idx] - sum_v)/scoreV['1/i_total_1970'][idx]\n",
    "    \n",
    "    author_results[name]['From'] = scoreV['auth_years'][idx][0]\n",
    "    author_results[name]['Until'] = scoreV['auth_years'][idx][1]\n",
    "\n",
    "    author_results[name]['YearlyScore (1/pos)'] = scoreV['1/i_total_1970'][idx]/scoreV['working_years'][idx]\n",
    "    author_results[name]['YearlyRIScore'] = scoreV['1/i_RI_1970'][idx]/scoreV['working_years'][idx]\n",
    "\n",
    "new_set = set()\n",
    "\n",
    "for name in interesting_set:\n",
    "    idx = name_idx[name]\n",
    "\n",
    "    author_results[name]['avgCoauthor'] = np.array(coauthors_num[name]).mean()\n",
    "    colabs = sorted([(v,k) for k,v in coauthors_frac[name].items()],reverse=True)\n",
    "    fam_colab = sorted([(v*scoreV['1/i_total_1970'][name_idx[k]],k) for k,v in coauthors_frac[name].items()],reverse=True)\n",
    "    \n",
    "    freq_colabs = sorted([(v,k) for k,v in coauthors[name].items() if v >= 4],reverse=True)\n",
    "    if len(colabs) > 0:\n",
    "        author_results[name]['mostCoauthorName'] = unidecode(colabs[0][1])\n",
    "        author_results[name]['mostCoauthorTimes'] = colabs[0][0]\n",
    "        new_set.add(colabs[0][1])\n",
    "    else:\n",
    "        author_results[name]['mostCoauthorName'] = ''\n",
    "        author_results[name]['mostCoauthorTimes'] = 0\n",
    "\n",
    "    if len(fam_colab) > 0:\n",
    "        author_results[name]['famCoauthorName'] = unidecode(fam_colab[0][1])\n",
    "        new_set.add(fam_colab[0][1])\n",
    "    else:\n",
    "        author_results[name]['famCoauthorName'] = ''\n",
    "\n",
    "    author_results[name]['authorPosition%'] = np.array(author_pos[name]).mean()\n",
    "    author_results[name]['totalCoauth'] = len(colabs)\n",
    "    author_results[name]['freqCoauth (> 3 papers)'] = len(freq_colabs)\n",
    "    author_results[name]['famCoauthFrac'] = sum([_[0] for _ in fam_colab])\n",
    "    author_results[name]['totalCoauthFrac'] = sum([_[0] for _ in colabs])\n",
    "    \n",
    "    author_results[name]['mostPaperConf'] = sorted([(v,k) for k,v in conf_paper_frac[name].items()],reverse=True)[0][1]\n",
    "    author_results[name]['venuesPublishedIn'] = len(conf_paper_frac[name].items())\n",
    "    author_results[name]['pageRank'] = pr_full[idx]\n",
    "    #author_results[name]['pageRankRI'] = pr_ri[idx]\n",
    "    \n",
    "    \n",
    "    author_results[name]['numPapers'] = paper_num[name]\n",
    "    author_results[name]['numPapersFrac'] = paper_frac[name]\n",
    "\n",
    "\n",
    "    author_results[name]['YearlyScore (1/n)'] = scoreV['1/n_total_1970'][idx]/scoreV['working_years'][idx]\n",
    "    author_results[name]['YearlyScore (Full)'] = scoreV['full_total_1970'][idx]/scoreV['working_years'][idx]\n",
    "    author_results[name]['Score (1/n)'] = scoreV['1/n_total_1970'][idx]\n",
    "    author_results[name]['Score (Full)'] = scoreV['full_total_1970'][idx]\n",
    "    \n",
    "    author_results[name]['MaxScore'] = scoreV['1/i_max'][idx]\n",
    "    author_results[name]['MaxScore (1/n)'] = scoreV['1/n_max'][idx]\n",
    "    author_results[name]['MaxScore (Full)'] = scoreV['full_max'][idx]\n",
    "    \n",
    "    author_results[name]['BestYear'] = scoreV['1/i_max_yr'][idx]\n",
    "    author_results[name]['BestYear (1/n)'] = scoreV['1/n_max_yr'][idx]\n",
    "    author_results[name]['BestYear (Full)'] = scoreV['full_max_yr'][idx]\n",
    "    \n",
    "    for i,n in enumerate(['_apmFalse', '_apmTrue', 'n_apmFalse','n_apmFalse', 'pw_apmFalse', 'pw_apmTrue', 'pweff_apmFalse', 'pweff_apmTrue', 'pwunk_apmFalse','pwunk_apmTrue']):\n",
    "        author_results[name]['Adv'+str(i+1)] =  scoreV[n][idx]\n",
    "results_list = []\n",
    "for k,v in author_results.items():\n",
    "    v['Name'] = k\n",
    "    results_list.append(v)\n",
    "def_order = list(author_results[list(author_results.keys())[0]].keys())\n",
    "df_results = pd.DataFrame(results_list)[def_order].set_index('Name')\n",
    "adv_set = df_results[['Adv2', 'Adv3', 'Adv4', 'Adv6']]\n",
    "adv_a = []\n",
    "for row in adv_set.itertuples():\n",
    "    adv_a.append(softmin(row[1:],1))\n",
    "df_results['MinA'] = adv_a\n",
    "df_results['AvgA'] = adv_set.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "clf_pred = sm.RLM(df_results['Score (1/pos)'],df_results[['totalCoauth']]).fit()\n",
    "df_results['mdlAvg'] = clf_pred.predict() - df_results['Score (1/pos)']\n",
    "clf_pred.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[[_ for _ in df_results.columns if 'Adv!' not in _]].sort_values('MinA',0,False).style.background_gradient()#.to_excel('out.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "conf_count = defaultdict(lambda: np.zeros(years_per_conf))\n",
    "for paper in all_papers:\n",
    "    tag,title, authors, venue, pages, startPage,year,volume,number,url,publtype,eb_toofew,eb_skip = paper\n",
    "    conf_count[venue][(year-min_year)//YEAR_BLOCKS] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_block = np.nan_to_num(clf.reshape((-1,years_per_conf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_score = np.max(conf_block[:,-3:],axis=1)\n",
    "prior_score = np.min(conf_block[:,-5:-3],axis=1)\n",
    "didx = np.argsort(curr_score-prior_score)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted([(v[:-2].max(),k) for k,v in conf_count.items()],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pidx = 0\n",
    "for i in range(150):\n",
    "    idx = didx[i]\n",
    "    cnt = conf_count[all_venues[idx]][-2:].max()\n",
    "    if cnt > 10 and prior_score[idx] != -1:\n",
    "        print(pidx+1,int(cnt),all_venues[idx],'\\t'.join(['{:.1f}'.format(_) for _ in [curr_score[idx]-prior_score[idx],curr_score[idx],prior_score[idx]]]),sep='\\t')\n",
    "        pidx+=1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
